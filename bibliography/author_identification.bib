
@article{stamatatos_survey_2009,
	title = {A survey of modern authorship attribution methods},
	volume = {60},
	issn = {1532-2890},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.21001},
	doi = {10.1002/asi.21001},
	abstract = {Authorship attribution supported by statistical or computational methods has a long history starting from the 19th century and is marked by the seminal study of Mosteller and Wallace (1964) on the authorship of the disputed “Federalist Papers.” During the last decade, this scientific field has been developed substantially, taking advantage of research advances in areas such as machine learning, information retrieval, and natural language processing. The plethora of available electronic texts (e.g., e-mail messages, online forum messages, blogs, source code, etc.) indicates a wide variety of applications of this technology, provided it is able to handle short and noisy text from multiple candidate authors. In this article, a survey of recent advances of the automated approaches to attributing authorship is presented, examining their characteristics for both text representation and text classification. The focus of this survey is on computational requirements and settings rather than on linguistic or literary issues. We also discuss evaluation methodologies and criteria for authorship attribution studies and list open questions that will attract future work in this area.},
	language = {en},
	number = {3},
	urldate = {2025-03-08},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Stamatatos, Efstathios},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.21001},
	pages = {538--556},
	file = {Full Text PDF:/Users/klara/Zotero/storage/ZYKIXQAS/Stamatatos - 2009 - A survey of modern authorship attribution methods.pdf:application/pdf;Snapshot:/Users/klara/Zotero/storage/Q5JSF69B/asi.html:text/html},
}

@article{stein_intrinsic_2011,
	title = {Intrinsic plagiarism analysis},
	volume = {45},
	copyright = {http://www.springer.com/tdm},
	issn = {1574-020X, 1574-0218},
	url = {http://link.springer.com/10.1007/s10579-010-9115-y},
	doi = {10.1007/s10579-010-9115-y},
	abstract = {Research in automatic text plagiarism detection focuses on algorithms that compare suspicious documents against a collection of reference documents. Recent approaches perform well in identifying copied or modiﬁed foreign sections, but they assume a closed world where a reference collection is given. This article investigates the question whether plagiarism can be detected by a computer program if no reference can be provided, e.g., if the foreign sections stem from a book that is not available in digital form. We call this problem class intrinsic plagiarism analysis; it is closely related to the problem of authorship veriﬁcation.},
	language = {en},
	number = {1},
	urldate = {2025-03-08},
	journal = {Language Resources and Evaluation},
	author = {Stein, Benno and Lipka, Nedim and Prettenhofer, Peter},
	month = mar,
	year = {2011},
	pages = {63--82},
	file = {Stein et al. - 2011 - Intrinsic plagiarism analysis.pdf:/Users/klara/Zotero/storage/4Z53GB8R/Stein et al. - 2011 - Intrinsic plagiarism analysis.pdf:application/pdf},
}

@article{ayele_overview_2024,
	title = {Overview of {PAN} 2024: {Multi}-{Author} {Writing} {Style} {Analysis}, {Multilingual} {Text} {Detoxification}, {Oppositional} {Thinking} {Analysis}, and {Generative} {AI} {Authorship} {Verification}},
	abstract = {The goal of the PAN lab is to advance the state of the art in text forensics and stylometry through an objective evaluation of new and established methods on new benchmark datasets. In 2024, we organized four shared tasks: (1) multi-author writing style analysis, which we continue from 2023; (2) multilingual text detoxification, a new task that aims to re-formulate text in a non-toxic way for multiple languages; (3) oppositional thinking analysis, a new task that aims to discriminate critical thinking from conspiracy narratives and identify their core actors; and (4) generative AI authorship verification, which formulates the detection of AI-generated text as an authorship problem. PAN 2024 concluded as one of our most successful editions with 74 notebook papers by 147 participating teams.},
	language = {en},
	author = {Ayele, Abinew Ali and Babakov, Nikolay and Bevendorff, Janek and Casals, Xavier Bonet and Chulvi, Berta and Dementieva, Daryna and Elnagar, Ashaf and Freitag, Dayne and Fröbe, Maik and Korenčić, Damir and Mayerl, Maximilian and Moskovskiy, Daniil and Mukherjee, Animesh and Panchenko, Alexander and Potthast, Martin and Rangel, Francisco and Rizwan, Naquee and Rosso, Paolo and Schneider, Florian and Smirnova, Alisa and Stamatatos, Efstathios and Stakovskii, Elisei and Stein, Benno and Taulé, Mariona and Ustalov, Dmitry and Wang, Xintong and Wiegmann, Matti and Yimam, Seid Muhie and Zangerle, Eva},
	year = {2024},
	file = {Ayele et al. - Overview of PAN 2024 Multi-Author Writing Style A.pdf:/Users/klara/Zotero/storage/QBR9KC9B/Ayele et al. - Overview of PAN 2024 Multi-Author Writing Style A.pdf:application/pdf},
}

@article{zangerle_overview_2024,
	title = {Overview of the {Multi}-{Author} {Writing} {Style} {Analysis} {Task} at {PAN} 2024},
	abstract = {Analyzing the writing style of individual authors in texts in which several authors are involved is a fundamental task in attributing authorship and detecting plagiarism, as it makes it possible to identify the points at which authorship changes. This year’s multi-author writing style analysis task focuses on identifying all instances of paragraph-level writing style changes within a given text. We provide datasets with three different degrees of topical homogeneity to investigate how different degrees of topic consistency affect the detection of writing style changes. This paper gives an overview of the task, its definition and the data used, the approaches proposed by the participants, and the results obtained.},
	language = {en},
	author = {Zangerle, Eva and Mayerl, Maximilian and Potthast, Martin and Stein, Benno},
	year = {2024},
	file = {Zangerle et al. - Overview of the Multi-Author Writing Style Analysi.pdf:/Users/klara/Zotero/storage/XDW2LQF7/Zangerle et al. - Overview of the Multi-Author Writing Style Analysi.pdf:application/pdf},
}

@inproceedings{emmery_adversarial_2021,
	address = {Online},
	title = {Adversarial {Stylometry} in the {Wild}: {Transferable} {Lexical} {Substitution} {Attacks} on {Author} {Profiling}},
	shorttitle = {Adversarial {Stylometry} in the {Wild}},
	url = {https://aclanthology.org/2021.eacl-main.203},
	doi = {10.18653/v1/2021.eacl-main.203},
	abstract = {Written language contains stylistic cues that can be exploited to automatically infer a variety of potentially sensitive author information. Adversarial stylometry intends to attack such models by rewriting an author’s text. Our research proposes several components to facilitate deployment of these adversarial attacks in the wild, where neither data nor target models are accessible. We introduce a transformerbased extension of a lexical replacement attack, and show it achieves high transferability when trained on a weakly labeled corpus—decreasing target model performance below chance. While not completely inconspicuous, our more successful attacks also prove notably less detectable by humans. Our framework therefore provides a promising direction for future privacy-preserving adversarial attacks.},
	language = {en},
	urldate = {2025-04-10},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Emmery, Chris and Kádár, Ákos and Chrupała, Grzegorz},
	year = {2021},
	pages = {2388--2402},
	file = {Emmery et al. - 2021 - Adversarial Stylometry in the Wild Transferable L.pdf:/Users/klara/Zotero/storage/4JTXSK7D/Emmery et al. - 2021 - Adversarial Stylometry in the Wild Transferable L.pdf:application/pdf},
}

@inproceedings{bevendorff_generalizing_2019,
	address = {Minneapolis, Minnesota},
	title = {Generalizing {Unmasking} for {Short} {Texts}},
	url = {http://aclweb.org/anthology/N19-1068},
	doi = {10.18653/v1/N19-1068},
	abstract = {Authorship veriﬁcation is the problem of inferring whether two texts were written by the same author. For this task, unmasking is one of the most robust approaches as of today with the major shortcoming of only being applicable to book-length texts. In this paper, we present a generalized unmasking approach which allows for authorship veriﬁcation of texts as short as four printed pages with very high precision at an adjustable recall tradeoff. Our generalized approach therefore reduces the required material by orders of magnitude, making unmasking applicable to authorship cases of more practical proportions. The new approach is on par with other state-ofthe-art techniques that are optimized for texts of this length: it achieves accuracies of 75–80 \%, while also allowing for easy adjustment to forensic scenarios that require higher levels of conﬁdence in the classiﬁcation.},
	language = {en},
	urldate = {2025-04-14},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {Bevendorff, Janek and Stein, Benno and Hagen, Matthias and Potthast, Martin},
	year = {2019},
	pages = {654--659},
	file = {Bevendorff et al. - 2019 - Generalizing Unmasking for Short Texts.pdf:/Users/klara/Zotero/storage/GYIDVVE6/Bevendorff et al. - 2019 - Generalizing Unmasking for Short Texts.pdf:application/pdf},
}

@inproceedings{bevendorff_bias_2019,
	address = {Florence, Italy},
	title = {Bias {Analysis} and {Mitigation} in the {Evaluation} of {Authorship} {Verification}},
	url = {https://www.aclweb.org/anthology/P19-1634},
	doi = {10.18653/v1/P19-1634},
	abstract = {The PAN series of shared tasks is well known for its continuous and high quality research in the ﬁeld of digital text forensics. Among others, PAN contributions include original corpora, tailored benchmarks, and standardized experimentation platforms. In this paper we review, theoretically and practically, the authorship veriﬁcation task and conclude that the underlying experiment design cannot guarantee pushing forward the state of the art—in fact, it allows for top benchmarking with a surprisingly straightforward approach. In this regard, we present a “Basic and Fairly Flawed” (BAFF) authorship veriﬁer that is on a par with the best approaches submitted so far, and that illustrates sources of bias that should be eliminated. We pinpoint these sources in the evaluation chain and present a reﬁned authorship corpus as effective countermeasure.},
	language = {en},
	urldate = {2025-04-14},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bevendorff, Janek and Hagen, Matthias and Stein, Benno and Potthast, Martin},
	year = {2019},
	pages = {6301--6306},
	file = {Bevendorff et al. - 2019 - Bias Analysis and Mitigation in the Evaluation of .pdf:/Users/klara/Zotero/storage/HIJXCAYF/Bevendorff et al. - 2019 - Bias Analysis and Mitigation in the Evaluation of .pdf:application/pdf},
}

@article{bevendorff_overview_2024,
	title = {Overview of the “{Voight}-{Kampff}” {Generative} {AI} {Authorship} {Verification} {Task} at {PAN} and {ELOQUENT}{\textasciitilde}2024},
	abstract = {The “Voight-Kampff” Generative AI Authorship Verification task aims to determine whether a text was generated by an AI or written by a human. As in its fictional inspiration,1 the Voight-Kampff task structures AI detection as a builder-breaker challenge: The builders, participants in the PAN lab, submit software to detect AI-written text and the breakers, participants in the ELOQUENT lab, submit AI-written text with the goal of fooling the builders. We formulate the task in a way that is reminiscent of a traditional authorship verification problem, where given a pair of texts, their human or machine authorship is to be inferred. For this first task installment, we further restrict the problem so that each pair is guaranteed to contain one human and one machine text. Hence the task description reads: Given two texts, one authored by a human, one by a machine: pick out the human.},
	language = {en},
	author = {Bevendorff, Janek and Wiegmann, Matti and Karlgren, Jussi and Dürlich, Luise and Gogoulou, Evangelia and Talman, Aarne and Stamatatos, Efstathios and Potthast, Martin and Stein, Benno},
	year = {2024},
	file = {Bevendorff et al. - Overview of the “Voight-Kampff” Generative AI Auth.pdf:/Users/klara/Zotero/storage/6L3LMH8A/Bevendorff et al. - Overview of the “Voight-Kampff” Generative AI Auth.pdf:application/pdf},
}

@article{bevendorff_divergence_based_2020,
	title = {On divergence-based author obfuscation: {An} attack on the state of the art in statistical authorship verification},
	volume = {62},
	issn = {2196-7032, 1611-2776},
	shorttitle = {On divergence-based author obfuscation},
	url = {https://www.degruyter.com/document/doi/10.1515/itit-2019-0046/html},
	doi = {10.1515/itit-2019-0046},
	abstract = {Authorship veriﬁcation is the task of determining whether two texts were written by the same author based on a writing style analysis. Author obfuscation is the adversarial task of preventing a successful veriﬁcation by altering a text’s style so that it does not resemble that of its original author anymore. This paper introduces new algorithms for both tasks and reports on a comprehensive evaluation to ascertain the merits of the state of the art in authorship veriﬁcation to withstand obfuscation. After introducing a new generalization of the well-known unmasking algorithm for short texts, thus completing our collection of state-of-the-art algorithms for veriﬁcation, we introduce an approach that (1) models writing style diﬀerence as the Jensen-Shannon distance between the character n-gram distributions of texts, and (2) manipulates an author’s writing style in a sophisticated manner using heuristic search. For obfuscation, we explore the huge space of textual variants in order to ﬁnd a paraphrased version of the to-be-obfuscated text that has a suﬃciently high Jensen-Shannon distance at minimal costs in terms of text quality loss. We analyze, quantify, and illustrate the rationale of this approach, deﬁne paraphrasing operators, derive text length-invariant thresholds for termination, and develop an eﬀective obfuscation framework. Our authorship obfuscation approach defeats the presented state-of-the-art veriﬁcation approaches, while keeping text changes at a minimum. As a ﬁnal contribution, we discuss and experimentally evaluate a reverse obfuscation attack against our obfuscation approach as well as possible remedies.},
	language = {en},
	number = {2},
	urldate = {2025-04-14},
	journal = {it - Information Technology},
	author = {Bevendorff, Janek and Wenzel, Tobias and Potthast, Martin and Hagen, Matthias and Stein, Benno},
	month = apr,
	year = {2020},
	pages = {99--115},
	file = {Bevendorff et al. - 2020 - On divergence-based author obfuscation An attack .pdf:/Users/klara/Zotero/storage/TCAVY4H2/Bevendorff et al. - 2020 - On divergence-based author obfuscation An attack .pdf:application/pdf},
}

@inproceedings{bevendorff_smauc_2023,
	address = {Santa Fe, NM, USA},
	title = {{SMAuC} - {The} {Scientific} {Multi}-{Authorship} {Corpus}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350399318},
	url = {https://ieeexplore.ieee.org/document/10266094/},
	doi = {10.1109/JCDL57899.2023.00013},
	abstract = {The rapidly growing volume of scientific publications offers an interesting challenge for research on methods for analyzing the authorship of documents with one or more authors. However, most existing datasets lack scientific documents or the necessary metadata for constructing new experiments and test cases. We introduce SMAuC, a comprehensive, metadata-rich corpus tailored to scientific authorship analysis. Comprising over 3 million publications across various disciplines from over 5 million authors, SMAuC is the largest openly accessible corpus for this purpose. It encompasses scientific texts from humanities and natural sciences, accompanied by extensive, curated metadata, including unambiguous author IDs. SMAuC aims to significantly advance the domain of authorship analysis in scientific texts.},
	language = {en},
	urldate = {2025-04-14},
	booktitle = {2023 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} ({JCDL})},
	publisher = {IEEE},
	author = {Bevendorff, Janek and Sauer, Philipp and Gienapp, Lukas and Kircheis, Wolfgang and Körner, Erik and Stein, Benno and Potthast, Martin},
	month = jun,
	year = {2023},
	pages = {25--29},
	file = {Bevendorff et al. - 2023 - SMAuC - The Scientific Multi-Authorship Corpus.pdf:/Users/klara/Zotero/storage/WSEDD5NK/Bevendorff et al. - 2023 - SMAuC - The Scientific Multi-Authorship Corpus.pdf:application/pdf},
}

@misc{tyo_state_2022,
	title = {On the {State} of the {Art} in {Authorship} {Attribution} and {Authorship} {Verification}},
	url = {http://arxiv.org/abs/2209.06869},
	doi = {10.48550/arXiv.2209.06869},
	abstract = {Statistics While older methods focused on small sets of summary statistics, more modern methods are able to combine all of these into a single model. Weerasinghe et al. (2021) provide the best example of this, calculating a plethora of hand-crafted features and Ngrams for each document (distribution of word lengths, hapax-legomena, Maas’ a2, Herdan’s Vm, and more). The authors take the difference between these large feature vectors for two texts and then train a logistic regression classiﬁer to predict if the texts were written by the same author or not. Despite its simplicity, this method performs well.},
	language = {en},
	urldate = {2025-04-14},
	publisher = {arXiv},
	author = {Tyo, Jacob and Dhingra, Bhuwan and Lipton, Zachary C.},
	month = oct,
	year = {2022},
	note = {arXiv:2209.06869 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Tyo et al. - 2022 - On the State of the Art in Authorship Attribution .pdf:/Users/klara/Zotero/storage/TK6RXFEV/Tyo et al. - 2022 - On the State of the Art in Authorship Attribution .pdf:application/pdf},
}

@article{koppel_authorship_2011,
	title = {Authorship attribution in the wild},
	volume = {45},
	issn = {1574-0218},
	url = {https://doi.org/10.1007/s10579-009-9111-2},
	doi = {10.1007/s10579-009-9111-2},
	abstract = {Most previous work on authorship attribution has focused on the case in which we need to attribute an anonymous document to one of a small set of candidate authors. In this paper, we consider authorship attribution as found in the wild: the set of known candidates is extremely large (possibly many thousands) and might not even include the actual author. Moreover, the known texts and the anonymous texts might be of limited length. We show that even in these difficult cases, we can use similarity-based methods along with multiple randomized feature sets to achieve high precision. Moreover, we show the precise relationship between attribution precision and four parameters: the size of the candidate set, the quantity of known-text by the candidates, the length of the anonymous text and a certain robustness score associated with a attribution.},
	language = {en},
	number = {1},
	urldate = {2025-04-14},
	journal = {Language Resources and Evaluation},
	author = {Koppel, Moshe and Schler, Jonathan and Argamon, Shlomo},
	month = mar,
	year = {2011},
	keywords = {Authorship attribution, Open candidate set, Randomized feature set},
	pages = {83--94},
	file = {Full Text PDF:/Users/klara/Zotero/storage/68IC9WZN/Koppel et al. - 2011 - Authorship attribution in the wild.pdf:application/pdf},
}

@inproceedings{chen_web_2008,
  title={Web page genre classification},
  author={Chen, Guangyu and Choi, Ben},
  booktitle={Proceedings of the 2008 ACM symposium on Applied computing},
  pages={2353--2357},
  year={2008}
}

@inproceedings{altakrori_topic_2021,
	address = {Punta Cana, Dominican Republic},
	title = {The {Topic} {Confusion} {Task}: {A} {Novel} {Evaluation} {Scenario} for {Authorship} {Attribution}},
	shorttitle = {The {Topic} {Confusion} {Task}},
	url = {https://aclanthology.org/2021.findings-emnlp.359},
	doi = {10.18653/v1/2021.findings-emnlp.359},
	language = {en},
	urldate = {2025-04-29},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Altakrori, Malik and Cheung, Jackie Chi Kit and Fung, Benjamin C. M.},
	year = {2021},
	pages = {4242--4256},
	file = {Altakrori et al. - 2021 - The Topic Confusion Task A Novel Evaluation Scena.pdf:/Users/klara/Zotero/storage/F47RGGGF/Altakrori et al. - 2021 - The Topic Confusion Task A Novel Evaluation Scena.pdf:application/pdf},
}

@inproceedings{barlas_cross_domain_2020,
	address = {Cham},
	title = {Cross-{Domain} {Authorship} {Attribution} {Using} {Pre}-trained {Language} {Models}},
	isbn = {978-3-030-49161-1},
	doi = {10.1007/978-3-030-49161-1_22},
	abstract = {Authorship attribution attempts to identify the authors behind texts and has important applications mainly in cyber-security, digital humanities and social media analytics. An especially challenging but very realistic scenario is cross-domain attribution where texts of known authorship (training set) differ from texts of disputed authorship (test set) in topic or genre. In this paper, we modify a successful authorship verification approach based on a multi-headed neural network language model and combine it with pre-trained language models. Based on experiments on a controlled corpus covering several text genres where topic and genre is specifically controlled, we demonstrate that the proposed approach achieves very promising results. We also demonstrate the crucial effect of the normalization corpus in cross-domain attribution.},
	language = {en},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Barlas, Georgios and Stamatatos, Efstathios},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	keywords = {Authorship Attribution, Neural network language models, Pre-trained language models},
	pages = {255--266},
	file = {Full Text PDF:/Users/klara/Zotero/storage/74N4C2HQ/Barlas und Stamatatos - 2020 - Cross-Domain Authorship Attribution Using Pre-trai.pdf:application/pdf},
}

@misc{bischoff_importance_2020,
	title = {The {Importance} of {Suppressing} {Domain} {Style} in {Authorship} {Analysis}},
	url = {http://arxiv.org/abs/2005.14714},
	doi = {10.48550/arXiv.2005.14714},
	abstract = {The prerequisite of many approaches to authorship analysis is a representation of writing style. But despite decades of research, it still remains unclear to what extent commonly used and widely accepted representations like character trigram frequencies actually represent an author’s writing style, in contrast to more domain-speciﬁc style components or even topic. We address this shortcoming for the ﬁrst time in a novel experimental setup of ﬁxed authors but swapped domains between training and testing. With this setup, we reveal that approaches using character trigram features are highly susceptible to favor domain information when applied without attention to domains, suffering drops of up to 55.4 percentage points in classiﬁcation accuracy under domain swapping. We further propose a new remedy based on domain-adversarial learning and compare it to ones from the literature based on heuristic rules. Both can work well, reducing accuracy losses under domain swapping to 3.6\% and 3.9\%, respectively.},
	language = {en},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Bischoff, Sebastian and Deckers, Niklas and Schliebs, Marcel and Thies, Ben and Hagen, Matthias and Stamatatos, Efstathios and Stein, Benno and Potthast, Martin},
	month = may,
	year = {2020},
	note = {arXiv:2005.14714 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Bischoff et al. - 2020 - The Importance of Suppressing Domain Style in Auth.pdf:/Users/klara/Zotero/storage/4Y9WQG8U/Bischoff et al. - 2020 - The Importance of Suppressing Domain Style in Auth.pdf:application/pdf},
}

@misc{boenninghoff_o2d2_2021,
	title = {{O2D2}: {Out}-{Of}-{Distribution} {Detector} to {Capture} {Undecidable} {Trials} in {Authorship} {Verification}},
	shorttitle = {{O2D2}},
	url = {http://arxiv.org/abs/2106.15825},
	doi = {10.48550/arXiv.2106.15825},
	abstract = {The PAN 2021 authorship verification (AV) challenge is part of a three-year strategy, moving from a cross-topic/closed-set AV task to a cross-topic/open-set AV task over a collection of fanfiction texts. In this work, we present a novel hybrid neural-probabilistic framework that is designed to tackle the challenges of the 2021 task. Our system is based on our 2020 winning submission, with updates to significantly reduce sensitivities to topical variations and to further improve the system’s calibration by means of an uncertainty adaptation layer. Our framework additionally includes an out-of-distribution detector (O2D2) for defining non-responses. Our proposed system outperformed all other systems that participated in the PAN 2021 AV task.},
	language = {en},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Boenninghoff, Benedikt and Nickel, Robert M. and Kolossa, Dorothea},
	month = jul,
	year = {2021},
	note = {arXiv:2106.15825 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: PAN@CLEF 2021},
	file = {Boenninghoff et al. - 2021 - O2D2 Out-Of-Distribution Detector to Capture Unde.pdf:/Users/klara/Zotero/storage/P3ZISRKE/Boenninghoff et al. - 2021 - O2D2 Out-Of-Distribution Detector to Capture Unde.pdf:application/pdf},
}

@article{elmanarelbouanani_authorship_2014,
	title = {Authorship {Analysis} {Studies}: {A} {Survey}},
	volume = {86},
	issn = {09758887},
	shorttitle = {Authorship {Analysis} {Studies}},
	url = {http://research.ijcaonline.org/volume86/number12/pxc3893384.pdf},
	doi = {10.5120/15038-3384},
	abstract = {The objective in this paper is to provide a review of the different studies done on authorship analysis. Focus is on outlining the Stylometric features that allow distinguishing between authors and on listing the diverse techniques used to classify an author’s texts.},
	language = {en},
	number = {12},
	urldate = {2025-04-29},
	journal = {International Journal of Computer Applications},
	author = {Elmanarelbouanani, Sara and Kassou, Ismail},
	month = jan,
	year = {2014},
	pages = {22--29},
	file = {Elmanarelbouanani und Kassou - 2014 - Authorship Analysis Studies A Survey.pdf:/Users/klara/Zotero/storage/ESJTHQEQ/Elmanarelbouanani und Kassou - 2014 - Authorship Analysis Studies A Survey.pdf:application/pdf},
}

@article{embarcadero_ruiz_graph_based_2022,
	title = {Graph-{Based} {Siamese} {Network} for {Authorship} {Verification}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/10/2/277},
	doi = {10.3390/math10020277},
	abstract = {In this work, we propose a novel approach to solve the authorship identification task on a cross-topic and open-set scenario. Authorship verification is the task of determining whether or not two texts were written by the same author. We model the documents in a graph representation and then a graph neural network extracts relevant features from these graph representations. We present three strategies to represent the texts as graphs based on the co-occurrence of the POS labels of words. We propose a Siamese Network architecture composed of graph convolutional networks along with pooling and classification layers. We present different variants of the architecture and discuss the performance of each one. To evaluate our approach we used a collection of fanfiction texts provided by the PAN@CLEF 2021 shared task in two settings: a “small” corpus and a “large” corpus. Our graph-based approach achieved average scores (AUC ROC, F1, Brier score, F0.5u, and C@1) between 90\% and 92.83\% when training on the “small” and “large” corpus, respectively. Our model obtain results comparable to those of the state of the art in this task and greater than traditional baselines.},
	language = {en},
	number = {2},
	urldate = {2025-04-29},
	journal = {Mathematics},
	author = {Embarcadero-Ruiz, Daniel and Gómez-Adorno, Helena and Embarcadero-Ruiz, Alberto and Sierra, Gerardo},
	month = jan,
	year = {2022},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {authorship verification, graph neural networks, POS tags, Siamese network, text graphs},
	pages = {277},
	file = {Full Text PDF:/Users/klara/Zotero/storage/CRCVHJ26/Embarcadero-Ruiz et al. - 2022 - Graph-Based Siamese Network for Authorship Verific.pdf:application/pdf},
}

@article{futrzynski_pairwise_2021,
	title = {Author classification as pre-training for pairwise authorship verification},
	abstract = {In this paper, we propose to use a standard BERT model for the PAN 2021 Authorship Verification task where two texts must be determined to either have the same or different authors. The model is chiefly trained to classify short sequences of text as belonging to one of three thousand authors selected from the large training dataset. Additional tasks are also used simultaneously during training in order to capitalize on the information available, namely, a masked language model task, a fandom classification task, and an author-fandom separation task. To perform Authorship Verification, an embedding is extracted from the trained BERT model. In order to reduce the computational cost, only a short sample of text is processed by BERT, but the same text is sampled a hundred times at random locations, and the embeddings from each sample are reduced to a single representation using the median. The representations from two texts are compared by cosine similarity, which is rescaled empirically so that most of the ambiguous pairs lie on the 0.5 threshold. Evaluated on authors and topics absent from the training dataset, this model achieved F1=0.832 and AUC=0.798.},
	language = {en},
	date = {2021},
	author = {Futrzynski, Romain},
	file = {Futrzynski - Notebook for PAN at CLEF 2021.pdf:/Users/klara/Zotero/storage/42WVQ3RZ/Futrzynski - Notebook for PAN at CLEF 2021.pdf:application/pdf},
}

@article{jafariakinabad_self_supervised_2022,
	title = {A {Self}-{Supervised} {Representation} {Learning} of {Sentence} {Structure} for {Authorship} {Attribution}},
	volume = {16},
	issn = {1556-4681, 1556-472X},
	url = {https://dl.acm.org/doi/10.1145/3491203},
	doi = {10.1145/3491203},
	abstract = {The syntactic structure of sentences in a document substantially informs about its authorial writing style. Sentence representation learning has been widely explored in recent years and it has been shown that it improves the generalization of different downstream tasks across many domains. Even though utilizing probing methods in several studies suggests that these learned contextual representations implicitly encode some amount of syntax, explicit syntactic information further improves the performance of deep neural models in the domain of authorship attribution. These observations have motivated us to investigate the explicit representation learning of syntactic structure of sentences. In this article, we propose a self-supervised framework for learning structural representations of sentences. The self-supervised network contains two components; a lexical sub-network and a syntactic sub-network which take the sequence of words and their corresponding structural labels as the input, respectively. Due to the
              n
              -to-1 mapping of words to their structural labels, each word will be embedded into a vector representation which mainly carries structural information. We evaluate the learned structural representations of sentences using different probing tasks, and subsequently utilize them in the authorship attribution task. Our experimental results indicate that the structural embeddings significantly improve the classification tasks when concatenated with the existing pre-trained word embeddings.},
	language = {en},
	number = {4},
	urldate = {2025-04-29},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Jafariakinabad, Fereshteh and Hua, Kien A.},
	month = aug,
	year = {2022},
	pages = {1--16},
	file = {Jafariakinabad und Hua - 2022 - A Self-Supervised Representation Learning of Sente.pdf:/Users/klara/Zotero/storage/C9FV3LWV/Jafariakinabad und Hua - 2022 - A Self-Supervised Representation Learning of Sente.pdf:application/pdf},
}

@article{neal_surveying_2018,
	title = {Surveying {Stylometry} {Techniques} and {Applications}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3132039},
	doi = {10.1145/3132039},
	abstract = {The analysis of authorial style, termed stylometry, assumes that style is quantifiably measurable for evaluation of distinctive qualities. Stylometry research has yielded several methods and tools over the past 200 years to handle a variety of challenging cases. This survey reviews several articles within five prominent subtasks: authorship attribution, authorship verification, authorship profiling, stylochronometry, and adversarial stylometry. Discussions on datasets, features, experimental techniques, and recent approaches are provided. Further, a current research challenge lies in the inability of authorship analysis techniques to scale to a large number of authors with few text samples. Here, we perform an extensive performance analysis on a corpus of 1,000 authors to investigate authorship attribution, verification, and clustering using 14 algorithms from the literature. Finally, several remaining research challenges are discussed, along with descriptions of various open-source and commercial software that may be useful for stylometry subtasks.},
	language = {en},
	number = {6},
	urldate = {2025-04-29},
	journal = {ACM Computing Surveys},
	author = {Neal, Tempestt and Sundararajan, Kalaivani and Fatima, Aneez and Yan, Yiming and Xiang, Yingfei and Woodard, Damon},
	month = nov,
	year = {2018},
	pages = {1--36},
	file = {Neal et al. - 2018 - Surveying Stylometry Techniques and Applications.pdf:/Users/klara/Zotero/storage/GGXMQHIB/Neal et al. - 2018 - Surveying Stylometry Techniques and Applications.pdf:application/pdf},
}

@article{ordonez_will_2020,
	title = {Will {Longformers} {PAN} {Out} for {Authorship} {Veriﬁcation}?},
	abstract = {Authorship veriﬁcation, the task of identifying if two text excerpts are from the same author, is an important part of evaluating the veracity and authenticity of writings and is one of the challenges for this year’s PAN @ CLEF 2020 event. In this paper, we describe our PAN authorship veriﬁcation submission system, a neural network that learns useful features for authorship veriﬁcation from fanﬁction texts and their corresponding fandoms. Our system uses the Longformer, a variant of state-of-the-art transformer models, that is pre-trained on large amounts of text. This model combines global self-attention and local self-attention to enable efﬁcient processing of long text inputs (like the fanﬁction data used for PAN @ CLEF 2020), and we augment the pre-trained Longformer model with additional fully-connected layers and ﬁne-tune it to learn features that are useful for author veriﬁcation. Finally, our model incorporates fandom information via the use of a multi-task loss function that optimizes for both authorship veriﬁcation and topic correspondence, allowing it to learn useful fandom features for author veriﬁcation indirectly. On a held-out subset of the PAN-provided “large training” set, our Longformer-based system attained a 0.963 overall veriﬁcation score, outperforming the PAN text compression baseline by 32.8\% relative. However, on the ofﬁcial PAN test set, our system attained a 0.685 overall score, underperforming the PAN text compression baseline by 7.6\% relative.},
	language = {en},
	year = {2020},
	author = {Ordoñez, Juanita and Soto, Rafael Rivera and Chen, Barry Y},
	file = {Ordoñez et al. - Will Longformers PAN Out for Authorship Veriﬁcatio.pdf:/Users/klara/Zotero/storage/7PUBPE5T/Ordoñez et al. - Will Longformers PAN Out for Authorship Veriﬁcatio.pdf:application/pdf},
}

@inproceedings{rivera_soto_learning_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Learning {Universal} {Authorship} {Representations}},
	url = {https://aclanthology.org/2021.emnlp-main.70},
	doi = {10.18653/v1/2021.emnlp-main.70},
	abstract = {Determining whether two documents were composed by the same author, also known as authorship veriﬁcation, has traditionally been tackled using statistical methods. Recently, authorship representations learned using neural networks have been found to outperform alternatives, particularly in large-scale settings involving hundreds of thousands of authors. But do such representations learned in a particular domain transfer to other domains? Or are these representations inherently entangled with domain-speciﬁc features? To study these questions, we conduct the ﬁrst large-scale study of cross-domain transfer for authorship veriﬁcation considering zero-shot transfers involving three disparate domains: Amazon reviews, fanﬁction short stories, and Reddit comments. We ﬁnd that although a surprising degree of transfer is possible between certain domains, it is not so successful between others. We examine properties of these domains that inﬂuence generalization and propose simple but effective methods to improve transfer.},
	language = {en},
	urldate = {2025-04-29},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rivera-Soto, Rafael A. and Miano, Olivia Elizabeth and Ordonez, Juanita and Chen, Barry Y. and Khan, Aleem and Bishop, Marcus and Andrews, Nicholas},
	year = {2021},
	pages = {913--919},
	file = {Rivera-Soto et al. - 2021 - Learning Universal Authorship Representations.pdf:/Users/klara/Zotero/storage/852DK2NS/Rivera-Soto et al. - 2021 - Learning Universal Authorship Representations.pdf:application/pdf},
}

@article{weerasinghe_feature_vector_difference_2021,
	title = {Feature Vector Difference based Authorship Verification for Open-World Settings},
	abstract = {This paper describes the approach we took to create a machine learning model for the PAN 2021 Authorship Verification Task. The goal of this task is to predict if a given pair of documents are written by the same author. For each document pair, we extracted stylometric features from the documents and used the absolute difference between the feature vectors as input to our classifier. Our new model is similar to out last year’s model with minor improvements to the feature set and the classifier. We trained two models on the two small and large datasets which achieved AUCs of 0.967 and 0.972 in the final evaluations.},
	language = {en},
	year = {2021},
	author = {Weerasinghe, Janith and Singh, Rhia and Greenstadt, Rachel},
	file = {Weerasinghe et al. - Notebook for PAN at CLEF 2021.pdf:/Users/klara/Zotero/storage/P39KQDFU/Weerasinghe et al. - Notebook for PAN at CLEF 2021.pdf:application/pdf},
}

@article{koppel_determining_2014,
	title = {Determining if two documents are written by the same author},
	volume = {65},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {2330-1635, 2330-1643},
	url = {https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.22954},
	doi = {10.1002/asi.22954},
	abstract = {Almost any conceivable authorship attribution problem can be reduced to one fundamental problem: whether a pair of (possibly short) documents were written by the same author. In this article, we offer an (almost) unsupervised method for solving this problem with surprisingly high accuracy. The main idea is to use repeated feature subsampling methods to determine if one document of the pair allows us to select the other from among a background set of “impostors” in a sufficiently robust manner.},
	language = {en},
	number = {1},
	urldate = {2025-04-30},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Koppel, Moshe and Winter, Yaron},
	month = jan,
	year = {2014},
	pages = {178--187},
	file = {Koppel und Winter - 2014 - Determining if two documents are written by the sa.pdf:/Users/klara/Zotero/storage/7T2SLYNI/Koppel und Winter - 2014 - Determining if two documents are written by the sa.pdf:application/pdf},
}

@article{kocher_unine_2015,
	title = {{UniNE} at {CLEF} 2015: {Author} {Identification}},
	abstract = {This paper describes and evaluates an unsupervised authorship verification model called SPATIUM-L1. The suggested strategy can be adapted without any problem to different languages (such as Dutch, English, Greek, and Spanish) with their genre and topic differ significantly. As features, we suggest using the k most frequent terms of the disputed text (isolated words and punctuation symbols with k may vary from 200 to 300). Applying a simple distance measure and a set of impostors, we determine whether or not the disputed text was written by the proposed author. Moreover, based on a simple rule, we can define when there is enough evidence to propose an answer with a high degree of confidence or when the attribution scheme is given without certainty. The evaluations are based on four test collections (PAN AUTHOR IDENTIFICATION task at CLEF 2015).},
	language = {en},
	year = {2015},
	author = {Kocher, Mirco and Savoy, Jacques},
	file = {Kocher und Savoy - UniNE at CLEF 2015 Author Identification.pdf:/Users/klara/Zotero/storage/MVLGHL6G/Kocher und Savoy - UniNE at CLEF 2015 Author Identification.pdf:application/pdf},
}

@article{abbasi_writeprints_2008,
	title = {Writeprints: {A} stylometric approach to identity-level identification and similarity detection in cyberspace},
	volume = {26},
	issn = {1046-8188, 1558-2868},
	shorttitle = {Writeprints},
	url = {https://dl.acm.org/doi/10.1145/1344411.1344413},
	doi = {10.1145/1344411.1344413},
	abstract = {One of the problems often associated with online anonymity is that it hinders social accountability, as substantiated by the high levels of cybercrime. Although identity cues are scarce in cyberspace, individuals often leave behind textual identity traces. In this study we proposed the use of stylometric analysis techniques to help identify individuals based on writing style. We incorporated a rich set of stylistic features, including lexical, syntactic, structural, content-specific, and idiosyncratic attributes. We also developed the Writeprints technique for identification and similarity detection of anonymous identities. Writeprints is a Karhunen-Loeve transforms-based technique that uses a sliding window and pattern disruption algorithm with individual author-level feature sets. The Writeprints technique and extended feature set were evaluated on a testbed encompassing four online datasets spanning different domains: email, instant messaging, feedback comments, and program code. Writeprints outperformed benchmark techniques, including SVM, Ensemble SVM, PCA, and standard Karhunen-Loeve transforms, on the identification and similarity detection tasks with accuracy as high as 94\% when differentiating between 100 authors. The extended feature set also significantly outperformed a baseline set of features commonly used in previous research. Furthermore, individual-author-level feature sets generally outperformed use of a single group of attributes.},
	language = {en},
	number = {2},
	urldate = {2025-05-09},
	journal = {ACM Transactions on Information Systems},
	author = {Abbasi, Ahmed and Chen, Hsinchun},
	month = mar,
	year = {2008},
	pages = {1--29},
	file = {Abbasi und Chen - 2008 - Writeprints A stylometric approach to identity-le.pdf:/Users/klara/Zotero/storage/CBLLDQLP/Abbasi und Chen - 2008 - Writeprints A stylometric approach to identity-le.pdf:application/pdf},
}

@inproceedings{potthast_stylometric_2018,
	address = {Melbourne, Australia},	
	title = {A {Stylometric} {Inquiry} into {Hyperpartisan} and {Fake} {News}},
	url = {http://aclweb.org/anthology/P18-1022},
	doi = {10.18653/v1/P18-1022},
	abstract = {We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97\% of the 299 fake news articles identiﬁed are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share signiﬁcantly more stylistic similarities than either does with the mainstream. This result is robust: it has been conﬁrmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.},
	language = {en},
	urldate = {2025-05-19},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Potthast, Martin and Kiesel, Johannes and Reinartz, Kevin and Bevendorff, Janek and Stein, Benno},
	year = {2018},
	pages = {231--240},
	file = {Potthast et al. - 2018 - A Stylometric Inquiry into Hyperpartisan and Fake .pdf:/Users/klara/Zotero/storage/TLGBD2NY/Potthast et al. - 2018 - A Stylometric Inquiry into Hyperpartisan and Fake .pdf:application/pdf},
}

@article{llm_detection_av_2025,
	title = {The Two Paradigms of LLM Detection: Authorship Attribution versus Authorship Verification},
	language = {en},
	journal = {ACL},
	author = {TODO},
	year = {2025},
}

@inproceedings{koppel_authorship_2004,
	address = {Banff, Alberta, Canada},
	title = {Authorship verification as a one-class classification problem},
	url = {http://portal.acm.org/citation.cfm?doid=1015330.1015448},
	doi = {10.1145/1015330.1015448},
	language = {en},
	urldate = {2025-05-27},
	booktitle = {Twenty-first international conference on {Machine} learning  - {ICML} '04},
	publisher = {ACM Press},
	author = {Koppel, Moshe and Schler, Jonathan},
	year = {2004},
	pages = {62},
	file = {Koppel und Schler - 2004 - Authorship verification as a one-class classificat.pdf:/Users/klara/Zotero/storage/5T4Q35BF/Koppel und Schler - 2004 - Authorship verification as a one-class classificat.pdf:application/pdf},
}

% Paraphrasing

@article{zhou_paraphrase_2025,
	title = {Paraphrase {Identification} {With} {Deep} {Learning}: {A} {Review} of {Datasets} and {Methods}},
	volume = {13},
	issn = {2169-3536},
	shorttitle = {Paraphrase {Identification} {With} {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/10946892/},
	doi = {10.1109/ACCESS.2025.3556899},
	abstract = {The rapid advancement of Natural Language Processing (NLP) has greatly improved text-generation tools like ChatGPT and Claude, offering significant utility but also posing risks to media credibility through paraphrased plagiarism—a subtle yet widespread form of content misuse. Despite progress in automated paraphrase detection, inconsistencies in training datasets often limit their effectiveness. This study examines traditional and modern approaches to paraphrase identification, revealing how the under-representation of certain paraphrase types in widely-used datasets, including those for training Large Language Models (LLMs), undermines plagiarism detection accuracy. To address these issues, we introduce and validate ReParaphrased, a refined paraphrase typology, and extend the Extended Typology Paraphrase Corpus (ETPC) with meticulous manual annotations to enhance reliability. Using the augmented ETPC, we fine-tune the LLama3.1-7B-instruct model, uncovering significant disparities in paraphrase type distribution across existing datasets. A detailed analysis of the MRPC benchmark dataset further highlights critical distributional issues and their implications. We propose four key solutions to address dataset limitations, providing both theoretical and practical guidance for improving dataset quality. These contributions aim to establish a more robust foundation for NLP model training and evaluation. Finally, we outline future research directions and suggest improvements for dataset development to advance AI-driven paraphrase detection.},
	urldate = {2025-06-05},
	journal = {IEEE Access},
	author = {Zhou, Chao and Qiu, Cheng and Liang, Lizhen and Acuna, Daniel E.},
	year = {2025},
	keywords = {Training, Deep learning, deep learning, Semantics, datasets, Electronic mail, Natural language processing, Paraphrase identification, plagiarism, Plagiarism, review, Reviews, Sports, Switches, Syntactics},
	pages = {65797--65822},
	file = {Full Text PDF:/Users/klara/Zotero/storage/FQ9W73EX/Zhou et al. - 2025 - Paraphrase Identification With Deep Learning A Re.pdf:application/pdf},
}

@article{krishna_paraphrasing_2023,
	title = {Paraphrasing evades detectors of {AI}-generated text, but retrieval is an effective defense},
	language = {en},
	year = {2023},
	author = {Krishna, Kalpesh and Song, Yixiao and Karpinska, Marzena and Wieting, John and Iyyer, Mohit},
	file = {Krishna et al. - Paraphrasing evades detectors of AI-generated text.pdf:/Users/klara/Zotero/storage/P5MAS48T/Krishna et al. - Paraphrasing evades detectors of AI-generated text.pdf:application/pdf},
}

@article{palivela_optimization_2021,
	title = {Optimization of paraphrase generation and identification using language models in natural language processing},
	volume = {1},
	issn = {2667-0968},
	url = {https://www.sciencedirect.com/science/article/pii/S2667096821000185},
	doi = {10.1016/j.jjimei.2021.100025},
	abstract = {Paraphrase Generation is one of the most important and challenging tasks in the field of Natural Language Generation. The paraphrasing techniques help to identify or to extract/generate phrases/sentences conveying the similar meaning. The paraphrasing task can be bifurcated into two sub-tasks namely, Paraphrase Identification (PI) and Paraphrase Generation (PG). Most of the existing proposed state-of-the-art systems have the potential to solve only one problem at a time. This paper proposes a light-weight unified model that can simultaneously classify whether given pair of sentences are paraphrases of each other and the model can also generate multiple paraphrases given an input sentence. Paraphrase Generation module aims to generate fluent and semantically similar paraphrases and the Paraphrase Identification system aims to classify whether sentences pair are paraphrases of each other or not. The proposed approach uses an amalgamation of data sampling or data variety with a granular fine-tuned Text-To-Text Transfer Transformer (T5) model. This paper proposes a unified approach which aims to solve the problems of Paraphrase Identification and generation by using carefully selected data-points and a fine-tuned T5 model. The highlight of this study is that the same light-weight model trained by keeping the objective of Paraphrase Generation can also be used for solving the Paraphrase Identification task. Hence, the proposed system is light-weight in terms of the model’s size along with the data used to train the model which facilitates the quick learning of the model without having to compromise with the results. The proposed system is then evaluated against the popular evaluation metrics like BLEU (BiLingual Evaluation Understudy):, ROUGE (Recall-Oriented Understudy for Gisting Evaluation), METEOR, WER (Word Error Rate), and GLEU (Google-BLEU) for Paraphrase Generation and classification metrics like accuracy, precision, recall and F1-score for Paraphrase Identification system. The proposed model achieves state-of-the-art results on both the tasks of Paraphrase Identification and paraphrase Generation.},
	number = {2},
	urldate = {2025-06-13},
	journal = {International Journal of Information Management Data Insights},
	author = {Palivela, Hemant},
	month = nov,
	year = {2021},
	keywords = {Encoder decoder, Language model, Natural language generation, Paraphrase generation, Paraphrase identification, Transformer},
	pages = {100025},
	file = {ScienceDirect Snapshot:/Users/klara/Zotero/storage/LQ5KQPWR/S2667096821000185.html:text/html},
}


@article{kurt_pehlivanoglu_comparative_2024,
	title = {Comparative analysis of paraphrasing performance of {ChatGPT}, {GPT}-3, and {T5} language models using a new {ChatGPT} generated dataset: {ParaGPT}},
	volume = {41},
	issn = {1468-0394},
	shorttitle = {Comparative analysis of paraphrasing performance of {ChatGPT}, {GPT}-3, and {T5} language models using a new {ChatGPT} generated dataset},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13699},
	doi = {10.1111/exsy.13699},
	abstract = {Paraphrase generation is a fundamental natural language processing (NLP) task that refers to the process of generating a well-formed and coherent output sentence that exhibits both syntactic and/or lexical diversity from the input sentence, while simultaneously ensuring that the semantic similarity between the two sentences is preserved. However, the availability of high-quality paraphrase datasets has been limited, particularly for machine-generated sentences. In this paper, we present ParaGPT, a new paraphrase dataset of 81,000 machine-generated sentence pairs, including 27,000 reference sentences (ChatGPT-generated sentences), and 81,000 paraphrases obtained by using three different large language models (LLMs): ChatGPT, GPT-3, and T5. We used ChatGPT to generate 27,000 sentences that cover a diverse array of topics and sentence structures, thus providing diverse inputs for the models. In addition, we evaluated the quality of the generated paraphrases using various automatic evaluation metrics. Furthermore, we provide insights into the strengths and drawbacks of each LLM in generating paraphrases by conducting a comparative analysis of the paraphrasing performance of the three LLMs. According to our findings, ChatGPT's performance, as per the evaluation metrics provided, was deemed impressive and commendable, owing to its higher-than-average scores for semantic similarity, which implies a higher degree of similarity between the generated paraphrase and the reference sentence, and its relatively lower scores for syntactic diversity, indicating a greater diversity of syntactic structures in the generated paraphrase. ParaGPT is a valuable resource for researchers working on NLP tasks like paraphrasing, text simplification, and text generation. We make the ParaGPT dataset publicly accessible to researchers, and as far as we are aware, this is the first paraphrase dataset produced based on ChatGPT.},
	language = {en},
	number = {11},
	urldate = {2025-06-05},
	journal = {Expert Systems},
	author = {Kurt Pehlivanoğlu, Meltem and Gobosho, Robera Tadesse and Syakura, Muhammad Abdan and Shanmuganathan, Vimal and de-la-Fuente-Valentín, Luis},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.13699},
	keywords = {machine learning, ChatGPT, generative artificial intelligence, large language models},
	pages = {e13699},
	file = {Full Text PDF:/Users/klara/Zotero/storage/9PX934QS/Kurt Pehlivanoğlu et al. - 2024 - Comparative analysis of paraphrasing performance o.pdf:application/pdf;Snapshot:/Users/klara/Zotero/storage/L5EBWTI9/exsy.html:text/html},
}

@inproceedings{fu_learning_2024,
	address = {Miami, Florida, USA},
	title = {Learning to {Paraphrase} for {Alignment} with {LLM} {Preference}},
	url = {https://aclanthology.org/2024.findings-emnlp.134},
	doi = {10.18653/v1/2024.findings-emnlp.134},
	abstract = {Large Language Models (LLMs) exhibit the issue of paraphrase divergence. This means that when a question is phrased in a slightly different but semantically similar way, LLM may output a wrong response despite being able to answer the original question correctly. Previous research has regarded this issue as a problem of the model’s robustness to question paraphrase and proposed a retraining method to address it. However, retraining faces challenges in meeting the computational costs and privacy security demands of LLMs. In this paper, we regard this issue as a problem of alignment with model preferences and propose PEARL (Preference-drivEn pAraphRase Learning). This is a black-box method that enhances model performance by paraphrasing questions in expressions preferred by the model. We validate PEARL across six datasets spanning three tasks: open-domain QA, commonsense reasoning, and math word problem. Extensive experiments demonstrated not only the outstanding performance but also the composability, transferability, and immense potential of PEARL, shedding new light on the blackbox tuning of LLMs1.},
	language = {en},
	urldate = {2025-06-05},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Fu, Junbo and Zhao, Guoshuai and Deng, Yimin and Mi, Yunqi and Qian, Xueming},
	year = {2024},
	pages = {2394--2407},
	file = {Fu et al. - 2024 - Learning to Paraphrase for Alignment with LLM Pref.pdf:/Users/klara/Zotero/storage/FQPVE2LL/Fu et al. - 2024 - Learning to Paraphrase for Alignment with LLM Pref.pdf:application/pdf},
}

@inproceedings{zhou_paraphrase_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Paraphrase {Generation}: {A} {Survey} of the {State} of the {Art}},
	shorttitle = {Paraphrase {Generation}},
	url = {https://aclanthology.org/2021.emnlp-main.414},
	doi = {10.18653/v1/2021.emnlp-main.414},
	abstract = {This paper focuses on paraphrase generation, which is a widely studied natural language generation task in NLP. With the development of neural models, paraphrase generation research has exhibited a gradual shift to neural methods in the recent years. This has provided architectures for contextualized representation of an input text and generating ﬂuent, diverse and human-like paraphrases. This paper surveys various approaches to paraphrase generation with a main focus on neural methods.},
	language = {en},
	urldate = {2025-06-05},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Jianing and Bhat, Suma},
	year = {2021},
	pages = {5075--5086},
	file = {Zhou und Bhat - 2021 - Paraphrase Generation A Survey of the State of th.pdf:/Users/klara/Zotero/storage/D9BHR2AF/Zhou und Bhat - 2021 - Paraphrase Generation A Survey of the State of th.pdf:application/pdf},
}

@article{hassanipour_ability_2024,
	title = {The {Ability} of {ChatGPT} in {Paraphrasing} {Texts} and {Reducing} {Plagiarism}: {A} {Descriptive} {Analysis}},
	volume = {10},
	issn = {2369-3762},
	shorttitle = {The {Ability} of {ChatGPT} in {Paraphrasing} {Texts} and {Reducing} {Plagiarism}},
	url = {https://mededu.jmir.org/2024/1/e53308},
	doi = {10.2196/53308},
	abstract = {Background: The introduction of ChatGPT by OpenAI has garnered significant attention. Among its capabilities, paraphrasing stands out.
Objective: This study aims to investigate the satisfactory levels of plagiarism in the paraphrased text produced by this chatbot.
Methods: Three texts of varying lengths were presented to ChatGPT. ChatGPT was then instructed to paraphrase the provided texts using five different prompts. In the subsequent stage of the study, the texts were divided into separate paragraphs, and ChatGPT was requested to paraphrase each paragraph individually. Lastly, in the third stage, ChatGPT was asked to paraphrase the texts it had previously generated.
Results: The average plagiarism rate in the texts generated by ChatGPT was 45\% (SD 10\%). ChatGPT exhibited a substantial reduction in plagiarism for the provided texts (mean difference −0.51, 95\% CI −0.54 to −0.48; P{\textless}.001). Furthermore, when comparing the second attempt with the initial attempt, a significant decrease in the plagiarism rate was observed (mean difference −0.06, 95\% CI −0.08 to −0.03; P{\textless}.001). The number of paragraphs in the texts demonstrated a noteworthy association with the percentage of plagiarism, with texts consisting of a single paragraph exhibiting the lowest plagiarism rate (P{\textless}.001).
Conclusion: Although ChatGPT demonstrates a notable reduction of plagiarism within texts, the existing levels of plagiarism remain relatively high. This underscores a crucial caution for researchers when incorporating this chatbot into their work.},
	language = {en},
	urldate = {2025-06-12},
	journal = {JMIR Medical Education},
	author = {Hassanipour, Soheil and Nayak, Sandeep and Bozorgi, Ali and Keivanlou, Mohammad-Hossein and Dave, Tirth and Alotaibi, Abdulhadi and Joukar, Farahnaz and Mellatdoust, Parinaz and Bakhshi, Arash and Kuriyakose, Dona and Polisetty, Lakshmi D and Chimpiri, Mallika and Amini-Salehi, Ehsan},
	month = jul,
	year = {2024},
	pages = {e53308--e53308},
	file = {Hassanipour et al. - 2024 - The Ability of ChatGPT in Paraphrasing Texts and R.pdf:/Users/klara/Zotero/storage/PPZZ3928/Hassanipour et al. - 2024 - The Ability of ChatGPT in Paraphrasing Texts and R.pdf:application/pdf},
}

@article{master_thesis_paraphrasing_2024,
	title = {Paraphrasing {User} {Stories} with   {Large} {Language} {Models}},
	language = {en},
	year = {2024},
	author = {Azimov, Sherkhan},
	file = {Azimov - Paraphrasing User Stories with   Large Language Mo.pdf:/Users/klara/Zotero/storage/SLEN2RQB/Azimov - Paraphrasing User Stories with   Large Language Mo.pdf:application/pdf},
}


@article{lin_rouge_2004,
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.},
	language = {en},
	author = {Lin, Chin-Yew},
	year = {2004},
	file = {Lin - ROUGE A Package for Automatic Evaluation of Summa.pdf:/Users/klara/Zotero/storage/UIM8ZYGU/Lin - ROUGE A Package for Automatic Evaluation of Summa.pdf:application/pdf},
}

@article{banerjee_METEOR_2005,
	title = {METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
	abstract = {We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.},
	language = {en},
	journal = {Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},
	author = {Banerjee, Satanjeev and Lavie, Alon},
	year = {2005},
	file = {Proceedings of the....pdf:/Users/klara/Zotero/storage/P5RDPJJ7/Proceedings of the....pdf:application/pdf},
}

@article{hanna_fine_grained_2021,
	title = {A {Fine}-{Grained} {Analysis} of {BERTScore}},
	language = {en},
	author = {Hanna, Michael and Bojar, Ondřej},
	year = {2021},
	file = {Hanna und Bojar - A Fine-Grained Analysis of BERTScore.pdf:/Users/klara/Zotero/storage/USQP95XN/Hanna und Bojar - A Fine-Grained Analysis of BERTScore.pdf:application/pdf},
}

@article{gohsen_task-oriented_nodate,
	title = {Task-{Oriented} {Paraphrase} {Analytics}},
	abstract = {Since paraphrasing is an ill-defined task, the term “paraphrasing” covers text transformation tasks with different characteristics. Consequently, existing paraphrasing studies have applied quite different (explicit and implicit) criteria as to when a pair of texts is to be considered a paraphrase, all of which amount to postulating a certain level of semantic or lexical similarity. In this paper, we conduct a literature review and propose a taxonomy to organize the 25 identified paraphrasing (sub-)tasks. Using classifiers trained to identify the tasks that a given paraphrasing instance fits, we find that the distributions of task-specific instances in the known paraphrase corpora vary substantially. This means that the use of these corpora, without the respective paraphrase conditions being clearly defined (which is the normal case), must lead to incomparable and misleading results.},
	language = {en},
	author = {Gohsen, Marcel and Hagen, Matthias and Potthast, Martin and Stein, Benno},
	file = {Gohsen et al. - Task-Oriented Paraphrase Analytics.pdf:/Users/klara/Zotero/storage/HUXHJHQS/Gohsen et al. - Task-Oriented Paraphrase Analytics.pdf:application/pdf},
}

@misc{gohsen_webis_2023,
	title = {Webis {Wikipedia}-{IPC}},
	copyright = {Creative Commons Attribution 4.0 International, Open Access},
	url = {https://zenodo.org/record/7621320},
	doi = {10.5281/ZENODO.7621320},
	abstract = {We propose to use image captions from the Web as a previously underutilized resource for paraphrases (i.e., texts with the same “message”) and to create and analyze a corresponding dataset. When an image is reused on the Web, an original caption is often assigned. We hypothesize that different captions for the same image naturally form a set of mutual paraphrases. To demonstrate the suitability of this idea, we analyze captions in the English Wikipedia, where editors frequently relabel the same image for different articles. The paper introduces the underlying mining technology, the resulting Wikipedia-IPC dataset, and compares known paraphrase corpora with respect to their syntactic and semantic paraphrase similarity to our new resource. In this context, we introduce characteristic maps along the two similarity dimensions to identify the style of paraphrases coming from different sources. An annotation study demonstrates the high reliability of the algorithmically determined characteristic maps.},
	language = {en},
	urldate = {2025-06-27},
	publisher = {Zenodo},
	author = {Gohsen, Marcel and Hagen, Matthias and Potthast, Martin and Stein, Benno},
	month = feb,
	year = {2023},
	keywords = {data mining, image captions, paraphrasing, wikipedia},
	annote = {Other
Bronze quality will be released soon.},
	file = {Gohsen et al. - 2023 - Webis Wikipedia-IPC.pdf:/Users/klara/Zotero/storage/GN5IZYGQ/Gohsen et al. - 2023 - Webis Wikipedia-IPC.pdf:application/pdf},
}

@article{hagen_authorship_nodate,
	title = {Authorship {Analysis} and {Obfuscation}},
	language = {en},
	author = {Hagen, Matthias and Potthast, Martin},
	file = {Hagen und Potthast - Authorship Analysis and Obfuscation.pdf:/Users/klara/Zotero/storage/UV2K3G3M/Hagen und Potthast - Authorship Analysis and Obfuscation.pdf:application/pdf},
}

@inproceedings{papineni_bleu_2001,
	address = {Philadelphia, Pennsylvania},
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	shorttitle = {{BLEU}},
	url = {http://portal.acm.org/citation.cfm?doid=1073083.1073135},
	doi = {10.3115/1073083.1073135},
	language = {en},
	urldate = {2025-06-29},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}  - {ACL} '02},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2001},
	pages = {311},
	file = {Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine.pdf:/Users/klara/Zotero/storage/84YUUBPJ/Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine.pdf:application/pdf},
}

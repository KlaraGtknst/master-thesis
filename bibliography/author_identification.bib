
@article{stamatatos_survey_2009,
	title = {A survey of modern authorship attribution methods},
	volume = {60},
	issn = {1532-2890},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.21001},
	doi = {10.1002/asi.21001},
	abstract = {Authorship attribution supported by statistical or computational methods has a long history starting from the 19th century and is marked by the seminal study of Mosteller and Wallace (1964) on the authorship of the disputed “Federalist Papers.” During the last decade, this scientific field has been developed substantially, taking advantage of research advances in areas such as machine learning, information retrieval, and natural language processing. The plethora of available electronic texts (e.g., e-mail messages, online forum messages, blogs, source code, etc.) indicates a wide variety of applications of this technology, provided it is able to handle short and noisy text from multiple candidate authors. In this article, a survey of recent advances of the automated approaches to attributing authorship is presented, examining their characteristics for both text representation and text classification. The focus of this survey is on computational requirements and settings rather than on linguistic or literary issues. We also discuss evaluation methodologies and criteria for authorship attribution studies and list open questions that will attract future work in this area.},
	language = {en},
	number = {3},
	urldate = {2025-03-08},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Stamatatos, Efstathios},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.21001},
	pages = {538--556},
	file = {Full Text PDF:/Users/klara/Zotero/storage/ZYKIXQAS/Stamatatos - 2009 - A survey of modern authorship attribution methods.pdf:application/pdf;Snapshot:/Users/klara/Zotero/storage/Q5JSF69B/asi.html:text/html},
}

@article{stein_intrinsic_2011,
	title = {Intrinsic plagiarism analysis},
	volume = {45},
	copyright = {http://www.springer.com/tdm},
	issn = {1574-020X, 1574-0218},
	url = {http://link.springer.com/10.1007/s10579-010-9115-y},
	doi = {10.1007/s10579-010-9115-y},
	abstract = {Research in automatic text plagiarism detection focuses on algorithms that compare suspicious documents against a collection of reference documents. Recent approaches perform well in identifying copied or modiﬁed foreign sections, but they assume a closed world where a reference collection is given. This article investigates the question whether plagiarism can be detected by a computer program if no reference can be provided, e.g., if the foreign sections stem from a book that is not available in digital form. We call this problem class intrinsic plagiarism analysis; it is closely related to the problem of authorship veriﬁcation.},
	language = {en},
	number = {1},
	urldate = {2025-03-08},
	journal = {Language Resources and Evaluation},
	author = {Stein, Benno and Lipka, Nedim and Prettenhofer, Peter},
	month = mar,
	year = {2011},
	pages = {63--82},
	file = {Stein et al. - 2011 - Intrinsic plagiarism analysis.pdf:/Users/klara/Zotero/storage/4Z53GB8R/Stein et al. - 2011 - Intrinsic plagiarism analysis.pdf:application/pdf},
}

@article{ayele_overview_2024,
	title = {Overview of {PAN} 2024: {Multi}-{Author} {Writing} {Style} {Analysis}, {Multilingual} {Text} {Detoxification}, {Oppositional} {Thinking} {Analysis}, and {Generative} {AI} {Authorship} {Verification}},
	abstract = {The goal of the PAN lab is to advance the state of the art in text forensics and stylometry through an objective evaluation of new and established methods on new benchmark datasets. In 2024, we organized four shared tasks: (1) multi-author writing style analysis, which we continue from 2023; (2) multilingual text detoxification, a new task that aims to re-formulate text in a non-toxic way for multiple languages; (3) oppositional thinking analysis, a new task that aims to discriminate critical thinking from conspiracy narratives and identify their core actors; and (4) generative AI authorship verification, which formulates the detection of AI-generated text as an authorship problem. PAN 2024 concluded as one of our most successful editions with 74 notebook papers by 147 participating teams.},
	language = {en},
	author = {Ayele, Abinew Ali and Babakov, Nikolay and Bevendorff, Janek and Casals, Xavier Bonet and Chulvi, Berta and Dementieva, Daryna and Elnagar, Ashaf and Freitag, Dayne and Fröbe, Maik and Korenčić, Damir and Mayerl, Maximilian and Moskovskiy, Daniil and Mukherjee, Animesh and Panchenko, Alexander and Potthast, Martin and Rangel, Francisco and Rizwan, Naquee and Rosso, Paolo and Schneider, Florian and Smirnova, Alisa and Stamatatos, Efstathios and Stakovskii, Elisei and Stein, Benno and Taulé, Mariona and Ustalov, Dmitry and Wang, Xintong and Wiegmann, Matti and Yimam, Seid Muhie and Zangerle, Eva},
	year = {2024},
	file = {Ayele et al. - Overview of PAN 2024 Multi-Author Writing Style A.pdf:/Users/klara/Zotero/storage/QBR9KC9B/Ayele et al. - Overview of PAN 2024 Multi-Author Writing Style A.pdf:application/pdf},
}

@article{zangerle_overview_2024,
	title = {Overview of the {Multi}-{Author} {Writing} {Style} {Analysis} {Task} at {PAN} 2024},
	abstract = {Analyzing the writing style of individual authors in texts in which several authors are involved is a fundamental task in attributing authorship and detecting plagiarism, as it makes it possible to identify the points at which authorship changes. This year’s multi-author writing style analysis task focuses on identifying all instances of paragraph-level writing style changes within a given text. We provide datasets with three different degrees of topical homogeneity to investigate how different degrees of topic consistency affect the detection of writing style changes. This paper gives an overview of the task, its definition and the data used, the approaches proposed by the participants, and the results obtained.},
	language = {en},
	author = {Zangerle, Eva and Mayerl, Maximilian and Potthast, Martin and Stein, Benno},
	year = {2024},
	file = {Zangerle et al. - Overview of the Multi-Author Writing Style Analysi.pdf:/Users/klara/Zotero/storage/XDW2LQF7/Zangerle et al. - Overview of the Multi-Author Writing Style Analysi.pdf:application/pdf},
}

@inproceedings{emmery_adversarial_2021,
	address = {Online},
	title = {Adversarial {Stylometry} in the {Wild}: {Transferable} {Lexical} {Substitution} {Attacks} on {Author} {Profiling}},
	shorttitle = {Adversarial {Stylometry} in the {Wild}},
	url = {https://aclanthology.org/2021.eacl-main.203},
	doi = {10.18653/v1/2021.eacl-main.203},
	abstract = {Written language contains stylistic cues that can be exploited to automatically infer a variety of potentially sensitive author information. Adversarial stylometry intends to attack such models by rewriting an author’s text. Our research proposes several components to facilitate deployment of these adversarial attacks in the wild, where neither data nor target models are accessible. We introduce a transformerbased extension of a lexical replacement attack, and show it achieves high transferability when trained on a weakly labeled corpus—decreasing target model performance below chance. While not completely inconspicuous, our more successful attacks also prove notably less detectable by humans. Our framework therefore provides a promising direction for future privacy-preserving adversarial attacks.},
	language = {en},
	urldate = {2025-04-10},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Emmery, Chris and Kádár, Ákos and Chrupała, Grzegorz},
	year = {2021},
	pages = {2388--2402},
	file = {Emmery et al. - 2021 - Adversarial Stylometry in the Wild Transferable L.pdf:/Users/klara/Zotero/storage/4JTXSK7D/Emmery et al. - 2021 - Adversarial Stylometry in the Wild Transferable L.pdf:application/pdf},
}

@inproceedings{bevendorff_generalizing_2019,
	address = {Minneapolis, Minnesota},
	title = {Generalizing {Unmasking} for {Short} {Texts}},
	url = {http://aclweb.org/anthology/N19-1068},
	doi = {10.18653/v1/N19-1068},
	abstract = {Authorship veriﬁcation is the problem of inferring whether two texts were written by the same author. For this task, unmasking is one of the most robust approaches as of today with the major shortcoming of only being applicable to book-length texts. In this paper, we present a generalized unmasking approach which allows for authorship veriﬁcation of texts as short as four printed pages with very high precision at an adjustable recall tradeoff. Our generalized approach therefore reduces the required material by orders of magnitude, making unmasking applicable to authorship cases of more practical proportions. The new approach is on par with other state-ofthe-art techniques that are optimized for texts of this length: it achieves accuracies of 75–80 \%, while also allowing for easy adjustment to forensic scenarios that require higher levels of conﬁdence in the classiﬁcation.},
	language = {en},
	urldate = {2025-04-14},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {Bevendorff, Janek and Stein, Benno and Hagen, Matthias and Potthast, Martin},
	year = {2019},
	pages = {654--659},
	file = {Bevendorff et al. - 2019 - Generalizing Unmasking for Short Texts.pdf:/Users/klara/Zotero/storage/GYIDVVE6/Bevendorff et al. - 2019 - Generalizing Unmasking for Short Texts.pdf:application/pdf},
}

@inproceedings{bevendorff_bias_2019,
	address = {Florence, Italy},
	title = {Bias {Analysis} and {Mitigation} in the {Evaluation} of {Authorship} {Verification}},
	url = {https://www.aclweb.org/anthology/P19-1634},
	doi = {10.18653/v1/P19-1634},
	abstract = {The PAN series of shared tasks is well known for its continuous and high quality research in the ﬁeld of digital text forensics. Among others, PAN contributions include original corpora, tailored benchmarks, and standardized experimentation platforms. In this paper we review, theoretically and practically, the authorship veriﬁcation task and conclude that the underlying experiment design cannot guarantee pushing forward the state of the art—in fact, it allows for top benchmarking with a surprisingly straightforward approach. In this regard, we present a “Basic and Fairly Flawed” (BAFF) authorship veriﬁer that is on a par with the best approaches submitted so far, and that illustrates sources of bias that should be eliminated. We pinpoint these sources in the evaluation chain and present a reﬁned authorship corpus as effective countermeasure.},
	language = {en},
	urldate = {2025-04-14},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bevendorff, Janek and Hagen, Matthias and Stein, Benno and Potthast, Martin},
	year = {2019},
	pages = {6301--6306},
	file = {Bevendorff et al. - 2019 - Bias Analysis and Mitigation in the Evaluation of .pdf:/Users/klara/Zotero/storage/HIJXCAYF/Bevendorff et al. - 2019 - Bias Analysis and Mitigation in the Evaluation of .pdf:application/pdf},
}

@article{bischoff_importance_2020,
	title = {The {Importance} of {Suppressing} {Domain} {Style} in {Authorship} {Analysis}},
	abstract = {The prerequisite of many approaches to authorship analysis is a representation of writing style. But despite decades of research, it still remains unclear to what extent commonly used and widely accepted representations like character trigram frequencies actually represent an author’s writing style, in contrast to more domain-speciﬁc style components or even topic. We address this shortcoming for the ﬁrst time in a novel experimental setup of ﬁxed authors but swapped domains between training and testing. With this setup, we reveal that approaches using character trigram features are highly susceptible to favor domain information when applied without attention to domains, suffering drops of up to 55.4 percentage points in classiﬁcation accuracy under domain swapping. We further propose a new remedy based on domain-adversarial learning and compare it to ones from the literature based on heuristic rules. Both can work well, reducing accuracy losses under domain swapping to 3.6\% and 3.9\%, respectively.},
	language = {en},
	author = {Bischoff, Sebastian and Deckers, Niklas and Schliebs, Marcel and Hagen, Matthias and Stamatatos, Efstathios and Stein, Benno and Thies, Ben and Potthast, Martin},
	year = {2020},
	file = {Bischoff et al. - The Importance of Suppressing Domain Style in Auth.pdf:/Users/klara/Zotero/storage/9Q76AM5F/Bischoff et al. - The Importance of Suppressing Domain Style in Auth.pdf:application/pdf},
}

@article{bevendorff_overview_2024,
	title = {Overview of the “{Voight}-{Kampff}” {Generative} {AI} {Authorship} {Verification} {Task} at {PAN} and {ELOQUENT}{\textasciitilde}2024},
	abstract = {The “Voight-Kampff” Generative AI Authorship Verification task aims to determine whether a text was generated by an AI or written by a human. As in its fictional inspiration,1 the Voight-Kampff task structures AI detection as a builder-breaker challenge: The builders, participants in the PAN lab, submit software to detect AI-written text and the breakers, participants in the ELOQUENT lab, submit AI-written text with the goal of fooling the builders. We formulate the task in a way that is reminiscent of a traditional authorship verification problem, where given a pair of texts, their human or machine authorship is to be inferred. For this first task installment, we further restrict the problem so that each pair is guaranteed to contain one human and one machine text. Hence the task description reads: Given two texts, one authored by a human, one by a machine: pick out the human.},
	language = {en},
	author = {Bevendorff, Janek and Wiegmann, Matti and Karlgren, Jussi and Dürlich, Luise and Gogoulou, Evangelia and Talman, Aarne and Stamatatos, Efstathios and Potthast, Martin and Stein, Benno},
	year = {2024},
	file = {Bevendorff et al. - Overview of the “Voight-Kampff” Generative AI Auth.pdf:/Users/klara/Zotero/storage/6L3LMH8A/Bevendorff et al. - Overview of the “Voight-Kampff” Generative AI Auth.pdf:application/pdf},
}

@article{bevendorff_divergence-based_2020,
	title = {On divergence-based author obfuscation: {An} attack on the state of the art in statistical authorship verification},
	volume = {62},
	issn = {2196-7032, 1611-2776},
	shorttitle = {On divergence-based author obfuscation},
	url = {https://www.degruyter.com/document/doi/10.1515/itit-2019-0046/html},
	doi = {10.1515/itit-2019-0046},
	abstract = {Authorship veriﬁcation is the task of determining whether two texts were written by the same author based on a writing style analysis. Author obfuscation is the adversarial task of preventing a successful veriﬁcation by altering a text’s style so that it does not resemble that of its original author anymore. This paper introduces new algorithms for both tasks and reports on a comprehensive evaluation to ascertain the merits of the state of the art in authorship veriﬁcation to withstand obfuscation. After introducing a new generalization of the well-known unmasking algorithm for short texts, thus completing our collection of state-of-the-art algorithms for veriﬁcation, we introduce an approach that (1) models writing style diﬀerence as the Jensen-Shannon distance between the character n-gram distributions of texts, and (2) manipulates an author’s writing style in a sophisticated manner using heuristic search. For obfuscation, we explore the huge space of textual variants in order to ﬁnd a paraphrased version of the to-be-obfuscated text that has a suﬃciently high Jensen-Shannon distance at minimal costs in terms of text quality loss. We analyze, quantify, and illustrate the rationale of this approach, deﬁne paraphrasing operators, derive text length-invariant thresholds for termination, and develop an eﬀective obfuscation framework. Our authorship obfuscation approach defeats the presented state-of-the-art veriﬁcation approaches, while keeping text changes at a minimum. As a ﬁnal contribution, we discuss and experimentally evaluate a reverse obfuscation attack against our obfuscation approach as well as possible remedies.},
	language = {en},
	number = {2},
	urldate = {2025-04-14},
	journal = {it - Information Technology},
	author = {Bevendorff, Janek and Wenzel, Tobias and Potthast, Martin and Hagen, Matthias and Stein, Benno},
	month = apr,
	year = {2020},
	pages = {99--115},
	file = {Bevendorff et al. - 2020 - On divergence-based author obfuscation An attack .pdf:/Users/klara/Zotero/storage/TCAVY4H2/Bevendorff et al. - 2020 - On divergence-based author obfuscation An attack .pdf:application/pdf},
}

@inproceedings{bevendorff_smauc_2023,
	address = {Santa Fe, NM, USA},
	title = {{SMAuC} - {The} {Scientific} {Multi}-{Authorship} {Corpus}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350399318},
	url = {https://ieeexplore.ieee.org/document/10266094/},
	doi = {10.1109/JCDL57899.2023.00013},
	abstract = {The rapidly growing volume of scientific publications offers an interesting challenge for research on methods for analyzing the authorship of documents with one or more authors. However, most existing datasets lack scientific documents or the necessary metadata for constructing new experiments and test cases. We introduce SMAuC, a comprehensive, metadata-rich corpus tailored to scientific authorship analysis. Comprising over 3 million publications across various disciplines from over 5 million authors, SMAuC is the largest openly accessible corpus for this purpose. It encompasses scientific texts from humanities and natural sciences, accompanied by extensive, curated metadata, including unambiguous author IDs. SMAuC aims to significantly advance the domain of authorship analysis in scientific texts.},
	language = {en},
	urldate = {2025-04-14},
	booktitle = {2023 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} ({JCDL})},
	publisher = {IEEE},
	author = {Bevendorff, Janek and Sauer, Philipp and Gienapp, Lukas and Kircheis, Wolfgang and Körner, Erik and Stein, Benno and Potthast, Martin},
	month = jun,
	year = {2023},
	pages = {25--29},
	file = {Bevendorff et al. - 2023 - SMAuC - The Scientific Multi-Authorship Corpus.pdf:/Users/klara/Zotero/storage/WSEDD5NK/Bevendorff et al. - 2023 - SMAuC - The Scientific Multi-Authorship Corpus.pdf:application/pdf},
}

@misc{tyo_state_2022,
	title = {On the {State} of the {Art} in {Authorship} {Attribution} and {Authorship} {Verification}},
	url = {http://arxiv.org/abs/2209.06869},
	doi = {10.48550/arXiv.2209.06869},
	abstract = {Statistics While older methods focused on small sets of summary statistics, more modern methods are able to combine all of these into a single model. Weerasinghe et al. (2021) provide the best example of this, calculating a plethora of hand-crafted features and Ngrams for each document (distribution of word lengths, hapax-legomena, Maas’ a2, Herdan’s Vm, and more). The authors take the difference between these large feature vectors for two texts and then train a logistic regression classiﬁer to predict if the texts were written by the same author or not. Despite its simplicity, this method performs well.},
	language = {en},
	urldate = {2025-04-14},
	publisher = {arXiv},
	author = {Tyo, Jacob and Dhingra, Bhuwan and Lipton, Zachary C.},
	month = oct,
	year = {2022},
	note = {arXiv:2209.06869 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Tyo et al. - 2022 - On the State of the Art in Authorship Attribution .pdf:/Users/klara/Zotero/storage/TK6RXFEV/Tyo et al. - 2022 - On the State of the Art in Authorship Attribution .pdf:application/pdf},
}

@article{koppel_authorship_2011,
	title = {Authorship attribution in the wild},
	volume = {45},
	issn = {1574-0218},
	url = {https://doi.org/10.1007/s10579-009-9111-2},
	doi = {10.1007/s10579-009-9111-2},
	abstract = {Most previous work on authorship attribution has focused on the case in which we need to attribute an anonymous document to one of a small set of candidate authors. In this paper, we consider authorship attribution as found in the wild: the set of known candidates is extremely large (possibly many thousands) and might not even include the actual author. Moreover, the known texts and the anonymous texts might be of limited length. We show that even in these difficult cases, we can use similarity-based methods along with multiple randomized feature sets to achieve high precision. Moreover, we show the precise relationship between attribution precision and four parameters: the size of the candidate set, the quantity of known-text by the candidates, the length of the anonymous text and a certain robustness score associated with a attribution.},
	language = {en},
	number = {1},
	urldate = {2025-04-14},
	journal = {Language Resources and Evaluation},
	author = {Koppel, Moshe and Schler, Jonathan and Argamon, Shlomo},
	month = mar,
	year = {2011},
	keywords = {Authorship attribution, Open candidate set, Randomized feature set},
	pages = {83--94},
	file = {Full Text PDF:/Users/klara/Zotero/storage/68IC9WZN/Koppel et al. - 2011 - Authorship attribution in the wild.pdf:application/pdf},
}

@inproceedings{chen_web_2008,
  title={Web page genre classification},
  author={Chen, Guangyu and Choi, Ben},
  booktitle={Proceedings of the 2008 ACM symposium on Applied computing},
  pages={2353--2357},
  year={2008}
}

@inproceedings{altakrori_topic_2021,
	address = {Punta Cana, Dominican Republic},
	title = {The {Topic} {Confusion} {Task}: {A} {Novel} {Evaluation} {Scenario} for {Authorship} {Attribution}},
	shorttitle = {The {Topic} {Confusion} {Task}},
	url = {https://aclanthology.org/2021.findings-emnlp.359},
	doi = {10.18653/v1/2021.findings-emnlp.359},
	language = {en},
	urldate = {2025-04-29},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Altakrori, Malik and Cheung, Jackie Chi Kit and Fung, Benjamin C. M.},
	year = {2021},
	pages = {4242--4256},
	file = {Altakrori et al. - 2021 - The Topic Confusion Task A Novel Evaluation Scena.pdf:/Users/klara/Zotero/storage/F47RGGGF/Altakrori et al. - 2021 - The Topic Confusion Task A Novel Evaluation Scena.pdf:application/pdf},
}

@inproceedings{barlas_cross-domain_2020,
	address = {Cham},
	title = {Cross-{Domain} {Authorship} {Attribution} {Using} {Pre}-trained {Language} {Models}},
	isbn = {978-3-030-49161-1},
	doi = {10.1007/978-3-030-49161-1_22},
	abstract = {Authorship attribution attempts to identify the authors behind texts and has important applications mainly in cyber-security, digital humanities and social media analytics. An especially challenging but very realistic scenario is cross-domain attribution where texts of known authorship (training set) differ from texts of disputed authorship (test set) in topic or genre. In this paper, we modify a successful authorship verification approach based on a multi-headed neural network language model and combine it with pre-trained language models. Based on experiments on a controlled corpus covering several text genres where topic and genre is specifically controlled, we demonstrate that the proposed approach achieves very promising results. We also demonstrate the crucial effect of the normalization corpus in cross-domain attribution.},
	language = {en},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Barlas, Georgios and Stamatatos, Efstathios},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	keywords = {Authorship Attribution, Neural network language models, Pre-trained language models},
	pages = {255--266},
	file = {Full Text PDF:/Users/klara/Zotero/storage/74N4C2HQ/Barlas und Stamatatos - 2020 - Cross-Domain Authorship Attribution Using Pre-trai.pdf:application/pdf},
}

@inproceedings{bevendorff_generalizing_2019,
	address = {Minneapolis, Minnesota},
	title = {Generalizing {Unmasking} for {Short} {Texts}},
	url = {http://aclweb.org/anthology/N19-1068},
	doi = {10.18653/v1/N19-1068},
	abstract = {Authorship veriﬁcation is the problem of inferring whether two texts were written by the same author. For this task, unmasking is one of the most robust approaches as of today with the major shortcoming of only being applicable to book-length texts. In this paper, we present a generalized unmasking approach which allows for authorship veriﬁcation of texts as short as four printed pages with very high precision at an adjustable recall tradeoff. Our generalized approach therefore reduces the required material by orders of magnitude, making unmasking applicable to authorship cases of more practical proportions. The new approach is on par with other state-ofthe-art techniques that are optimized for texts of this length: it achieves accuracies of 75–80 \%, while also allowing for easy adjustment to forensic scenarios that require higher levels of conﬁdence in the classiﬁcation.},
	language = {en},
	urldate = {2025-04-29},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {Bevendorff, Janek and Stein, Benno and Hagen, Matthias and Potthast, Martin},
	year = {2019},
	pages = {654--659},
	file = {Bevendorff et al. - 2019 - Generalizing Unmasking for Short Texts.pdf:/Users/klara/Zotero/storage/YEQI4VUC/Bevendorff et al. - 2019 - Generalizing Unmasking for Short Texts.pdf:application/pdf},
}

@misc{bischoff_importance_2020,
	title = {The {Importance} of {Suppressing} {Domain} {Style} in {Authorship} {Analysis}},
	url = {http://arxiv.org/abs/2005.14714},
	doi = {10.48550/arXiv.2005.14714},
	abstract = {The prerequisite of many approaches to authorship analysis is a representation of writing style. But despite decades of research, it still remains unclear to what extent commonly used and widely accepted representations like character trigram frequencies actually represent an author’s writing style, in contrast to more domain-speciﬁc style components or even topic. We address this shortcoming for the ﬁrst time in a novel experimental setup of ﬁxed authors but swapped domains between training and testing. With this setup, we reveal that approaches using character trigram features are highly susceptible to favor domain information when applied without attention to domains, suffering drops of up to 55.4 percentage points in classiﬁcation accuracy under domain swapping. We further propose a new remedy based on domain-adversarial learning and compare it to ones from the literature based on heuristic rules. Both can work well, reducing accuracy losses under domain swapping to 3.6\% and 3.9\%, respectively.},
	language = {en},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Bischoff, Sebastian and Deckers, Niklas and Schliebs, Marcel and Thies, Ben and Hagen, Matthias and Stamatatos, Efstathios and Stein, Benno and Potthast, Martin},
	month = may,
	year = {2020},
	note = {arXiv:2005.14714 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Bischoff et al. - 2020 - The Importance of Suppressing Domain Style in Auth.pdf:/Users/klara/Zotero/storage/4Y9WQG8U/Bischoff et al. - 2020 - The Importance of Suppressing Domain Style in Auth.pdf:application/pdf},
}

@misc{boenninghoff_o2d2_2021,
	title = {{O2D2}: {Out}-{Of}-{Distribution} {Detector} to {Capture} {Undecidable} {Trials} in {Authorship} {Verification}},
	shorttitle = {{O2D2}},
	url = {http://arxiv.org/abs/2106.15825},
	doi = {10.48550/arXiv.2106.15825},
	abstract = {The PAN 2021 authorship verification (AV) challenge is part of a three-year strategy, moving from a cross-topic/closed-set AV task to a cross-topic/open-set AV task over a collection of fanfiction texts. In this work, we present a novel hybrid neural-probabilistic framework that is designed to tackle the challenges of the 2021 task. Our system is based on our 2020 winning submission, with updates to significantly reduce sensitivities to topical variations and to further improve the system’s calibration by means of an uncertainty adaptation layer. Our framework additionally includes an out-of-distribution detector (O2D2) for defining non-responses. Our proposed system outperformed all other systems that participated in the PAN 2021 AV task.},
	language = {en},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Boenninghoff, Benedikt and Nickel, Robert M. and Kolossa, Dorothea},
	month = jul,
	year = {2021},
	note = {arXiv:2106.15825 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: PAN@CLEF 2021},
	file = {Boenninghoff et al. - 2021 - O2D2 Out-Of-Distribution Detector to Capture Unde.pdf:/Users/klara/Zotero/storage/P3ZISRKE/Boenninghoff et al. - 2021 - O2D2 Out-Of-Distribution Detector to Capture Unde.pdf:application/pdf},
}

@article{elmanarelbouanani_authorship_2014,
	title = {Authorship {Analysis} {Studies}: {A} {Survey}},
	volume = {86},
	issn = {09758887},
	shorttitle = {Authorship {Analysis} {Studies}},
	url = {http://research.ijcaonline.org/volume86/number12/pxc3893384.pdf},
	doi = {10.5120/15038-3384},
	abstract = {The objective in this paper is to provide a review of the different studies done on authorship analysis. Focus is on outlining the Stylometric features that allow distinguishing between authors and on listing the diverse techniques used to classify an author’s texts.},
	language = {en},
	number = {12},
	urldate = {2025-04-29},
	journal = {International Journal of Computer Applications},
	author = {Elmanarelbouanani, Sara and Kassou, Ismail},
	month = jan,
	year = {2014},
	pages = {22--29},
	file = {Elmanarelbouanani und Kassou - 2014 - Authorship Analysis Studies A Survey.pdf:/Users/klara/Zotero/storage/ESJTHQEQ/Elmanarelbouanani und Kassou - 2014 - Authorship Analysis Studies A Survey.pdf:application/pdf},
}

@article{embarcadero-ruiz_graph-based_2022,
	title = {Graph-{Based} {Siamese} {Network} for {Authorship} {Verification}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/10/2/277},
	doi = {10.3390/math10020277},
	abstract = {In this work, we propose a novel approach to solve the authorship identification task on a cross-topic and open-set scenario. Authorship verification is the task of determining whether or not two texts were written by the same author. We model the documents in a graph representation and then a graph neural network extracts relevant features from these graph representations. We present three strategies to represent the texts as graphs based on the co-occurrence of the POS labels of words. We propose a Siamese Network architecture composed of graph convolutional networks along with pooling and classification layers. We present different variants of the architecture and discuss the performance of each one. To evaluate our approach we used a collection of fanfiction texts provided by the PAN@CLEF 2021 shared task in two settings: a “small” corpus and a “large” corpus. Our graph-based approach achieved average scores (AUC ROC, F1, Brier score, F0.5u, and C@1) between 90\% and 92.83\% when training on the “small” and “large” corpus, respectively. Our model obtain results comparable to those of the state of the art in this task and greater than traditional baselines.},
	language = {en},
	number = {2},
	urldate = {2025-04-29},
	journal = {Mathematics},
	author = {Embarcadero-Ruiz, Daniel and Gómez-Adorno, Helena and Embarcadero-Ruiz, Alberto and Sierra, Gerardo},
	month = jan,
	year = {2022},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {authorship verification, graph neural networks, POS tags, Siamese network, text graphs},
	pages = {277},
	file = {Full Text PDF:/Users/klara/Zotero/storage/CRCVHJ26/Embarcadero-Ruiz et al. - 2022 - Graph-Based Siamese Network for Authorship Verific.pdf:application/pdf},
}

@article{futrzynski_notebook_nodate,
	title = {Notebook for {PAN} at {CLEF} 2021},
	abstract = {In this paper, we propose to use a standard BERT model for the PAN 2021 Authorship Verification task where two texts must be determined to either have the same or different authors. The model is chiefly trained to classify short sequences of text as belonging to one of three thousand authors selected from the large training dataset. Additional tasks are also used simultaneously during training in order to capitalize on the information available, namely, a masked language model task, a fandom classification task, and an author-fandom separation task. To perform Authorship Verification, an embedding is extracted from the trained BERT model. In order to reduce the computational cost, only a short sample of text is processed by BERT, but the same text is sampled a hundred times at random locations, and the embeddings from each sample are reduced to a single representation using the median. The representations from two texts are compared by cosine similarity, which is rescaled empirically so that most of the ambiguous pairs lie on the 0.5 threshold. Evaluated on authors and topics absent from the training dataset, this model achieved F1=0.832 and AUC=0.798.},
	language = {en},
	author = {Futrzynski, Romain},
	file = {Futrzynski - Notebook for PAN at CLEF 2021.pdf:/Users/klara/Zotero/storage/42WVQ3RZ/Futrzynski - Notebook for PAN at CLEF 2021.pdf:application/pdf},
}

@article{jafariakinabad_self-supervised_2022,
	title = {A {Self}-{Supervised} {Representation} {Learning} of {Sentence} {Structure} for {Authorship} {Attribution}},
	volume = {16},
	issn = {1556-4681, 1556-472X},
	url = {https://dl.acm.org/doi/10.1145/3491203},
	doi = {10.1145/3491203},
	abstract = {The syntactic structure of sentences in a document substantially informs about its authorial writing style. Sentence representation learning has been widely explored in recent years and it has been shown that it improves the generalization of different downstream tasks across many domains. Even though utilizing probing methods in several studies suggests that these learned contextual representations implicitly encode some amount of syntax, explicit syntactic information further improves the performance of deep neural models in the domain of authorship attribution. These observations have motivated us to investigate the explicit representation learning of syntactic structure of sentences. In this article, we propose a self-supervised framework for learning structural representations of sentences. The self-supervised network contains two components; a lexical sub-network and a syntactic sub-network which take the sequence of words and their corresponding structural labels as the input, respectively. Due to the
              n
              -to-1 mapping of words to their structural labels, each word will be embedded into a vector representation which mainly carries structural information. We evaluate the learned structural representations of sentences using different probing tasks, and subsequently utilize them in the authorship attribution task. Our experimental results indicate that the structural embeddings significantly improve the classification tasks when concatenated with the existing pre-trained word embeddings.},
	language = {en},
	number = {4},
	urldate = {2025-04-29},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Jafariakinabad, Fereshteh and Hua, Kien A.},
	month = aug,
	year = {2022},
	pages = {1--16},
	file = {Jafariakinabad und Hua - 2022 - A Self-Supervised Representation Learning of Sente.pdf:/Users/klara/Zotero/storage/C9FV3LWV/Jafariakinabad und Hua - 2022 - A Self-Supervised Representation Learning of Sente.pdf:application/pdf},
}

@article{neal_surveying_2018,
	title = {Surveying {Stylometry} {Techniques} and {Applications}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3132039},
	doi = {10.1145/3132039},
	abstract = {The analysis of authorial style, termed stylometry, assumes that style is quantifiably measurable for evaluation of distinctive qualities. Stylometry research has yielded several methods and tools over the past 200 years to handle a variety of challenging cases. This survey reviews several articles within five prominent subtasks: authorship attribution, authorship verification, authorship profiling, stylochronometry, and adversarial stylometry. Discussions on datasets, features, experimental techniques, and recent approaches are provided. Further, a current research challenge lies in the inability of authorship analysis techniques to scale to a large number of authors with few text samples. Here, we perform an extensive performance analysis on a corpus of 1,000 authors to investigate authorship attribution, verification, and clustering using 14 algorithms from the literature. Finally, several remaining research challenges are discussed, along with descriptions of various open-source and commercial software that may be useful for stylometry subtasks.},
	language = {en},
	number = {6},
	urldate = {2025-04-29},
	journal = {ACM Computing Surveys},
	author = {Neal, Tempestt and Sundararajan, Kalaivani and Fatima, Aneez and Yan, Yiming and Xiang, Yingfei and Woodard, Damon},
	month = nov,
	year = {2018},
	pages = {1--36},
	file = {Neal et al. - 2018 - Surveying Stylometry Techniques and Applications.pdf:/Users/klara/Zotero/storage/GGXMQHIB/Neal et al. - 2018 - Surveying Stylometry Techniques and Applications.pdf:application/pdf},
}

@article{ordonez_will_nodate,
	title = {Will {Longformers} {PAN} {Out} for {Authorship} {Veriﬁcation}?},
	abstract = {Authorship veriﬁcation, the task of identifying if two text excerpts are from the same author, is an important part of evaluating the veracity and authenticity of writings and is one of the challenges for this year’s PAN @ CLEF 2020 event. In this paper, we describe our PAN authorship veriﬁcation submission system, a neural network that learns useful features for authorship veriﬁcation from fanﬁction texts and their corresponding fandoms. Our system uses the Longformer, a variant of state-of-the-art transformer models, that is pre-trained on large amounts of text. This model combines global self-attention and local self-attention to enable efﬁcient processing of long text inputs (like the fanﬁction data used for PAN @ CLEF 2020), and we augment the pre-trained Longformer model with additional fully-connected layers and ﬁne-tune it to learn features that are useful for author veriﬁcation. Finally, our model incorporates fandom information via the use of a multi-task loss function that optimizes for both authorship veriﬁcation and topic correspondence, allowing it to learn useful fandom features for author veriﬁcation indirectly. On a held-out subset of the PAN-provided “large training” set, our Longformer-based system attained a 0.963 overall veriﬁcation score, outperforming the PAN text compression baseline by 32.8\% relative. However, on the ofﬁcial PAN test set, our system attained a 0.685 overall score, underperforming the PAN text compression baseline by 7.6\% relative.},
	language = {en},
	author = {Ordoñez, Juanita and Soto, Rafael Rivera and Chen, Barry Y},
	file = {Ordoñez et al. - Will Longformers PAN Out for Authorship Veriﬁcatio.pdf:/Users/klara/Zotero/storage/7PUBPE5T/Ordoñez et al. - Will Longformers PAN Out for Authorship Veriﬁcatio.pdf:application/pdf},
}

@inproceedings{rivera-soto_learning_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Learning {Universal} {Authorship} {Representations}},
	url = {https://aclanthology.org/2021.emnlp-main.70},
	doi = {10.18653/v1/2021.emnlp-main.70},
	abstract = {Determining whether two documents were composed by the same author, also known as authorship veriﬁcation, has traditionally been tackled using statistical methods. Recently, authorship representations learned using neural networks have been found to outperform alternatives, particularly in large-scale settings involving hundreds of thousands of authors. But do such representations learned in a particular domain transfer to other domains? Or are these representations inherently entangled with domain-speciﬁc features? To study these questions, we conduct the ﬁrst large-scale study of cross-domain transfer for authorship veriﬁcation considering zero-shot transfers involving three disparate domains: Amazon reviews, fanﬁction short stories, and Reddit comments. We ﬁnd that although a surprising degree of transfer is possible between certain domains, it is not so successful between others. We examine properties of these domains that inﬂuence generalization and propose simple but effective methods to improve transfer.},
	language = {en},
	urldate = {2025-04-29},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rivera-Soto, Rafael A. and Miano, Olivia Elizabeth and Ordonez, Juanita and Chen, Barry Y. and Khan, Aleem and Bishop, Marcus and Andrews, Nicholas},
	year = {2021},
	pages = {913--919},
	file = {Rivera-Soto et al. - 2021 - Learning Universal Authorship Representations.pdf:/Users/klara/Zotero/storage/852DK2NS/Rivera-Soto et al. - 2021 - Learning Universal Authorship Representations.pdf:application/pdf},
}

@article{weerasinghe_notebook_nodate,
	title = {Notebook for {PAN} at {CLEF} 2021},
	abstract = {This paper describes the approach we took to create a machine learning model for the PAN 2021 Authorship Verification Task. The goal of this task is to predict if a given pair of documents are written by the same author. For each document pair, we extracted stylometric features from the documents and used the absolute difference between the feature vectors as input to our classifier. Our new model is similar to out last year’s model with minor improvements to the feature set and the classifier. We trained two models on the two small and large datasets which achieved AUCs of 0.967 and 0.972 in the final evaluations.},
	language = {en},
	author = {Weerasinghe, Janith and Singh, Rhia and Greenstadt, Rachel},
	file = {Weerasinghe et al. - Notebook for PAN at CLEF 2021.pdf:/Users/klara/Zotero/storage/P39KQDFU/Weerasinghe et al. - Notebook for PAN at CLEF 2021.pdf:application/pdf},
}

@article{koppel_determining_2014,
	title = {Determining if two documents are written by the same author},
	volume = {65},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {2330-1635, 2330-1643},
	url = {https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.22954},
	doi = {10.1002/asi.22954},
	abstract = {Almost any conceivable authorship attribution problem can be reduced to one fundamental problem: whether a pair of (possibly short) documents were written by the same author. In this article, we offer an (almost) unsupervised method for solving this problem with surprisingly high accuracy. The main idea is to use repeated feature subsampling methods to determine if one document of the pair allows us to select the other from among a background set of “impostors” in a sufficiently robust manner.},
	language = {en},
	number = {1},
	urldate = {2025-04-30},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Koppel, Moshe and Winter, Yaron},
	month = jan,
	year = {2014},
	pages = {178--187},
	file = {Koppel und Winter - 2014 - Determining if two documents are written by the sa.pdf:/Users/klara/Zotero/storage/7T2SLYNI/Koppel und Winter - 2014 - Determining if two documents are written by the sa.pdf:application/pdf},
}
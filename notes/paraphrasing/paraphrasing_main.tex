\section{Paraphrasing}
\label{sec:paraphrasing}
\newcommand{\bluert}{\href{https://github.com/google-research/bleurt}{BLUERT}}

\subsection{Approaches to Paraphrasing}

Traditional approaches include thesaurus-based methods, where paraphrases are generated by 
replacing words with their synonyms from a thesaurus \cite{zhou_paraphrase_2021}.
Hence, this is called word-level paraphrasing.
First, all synonyms for the words to be replaced are retrieved from a thesaurus 
(or alignment tables based on IBM model, or WordNet).
Then, the optimal candidate is selected based on the context of the sentence. 
Although simple, this approach lacks diversity \cite{zhou_paraphrase_2021}.

\citet{fu_learning_2024} propose PEARL, a black-box paraphrasing model to meet \ac{llm} expression style.

UPRISE is a universal prompt auto-retrieval method that tunes a lightweight prompt retriever based on contrastive learning \cite{fu_learning_2024}.
There is also a official recommended template for manual prompt construction \cite{fu_learning_2024}.

\citet{zhou_paraphrase_2021}'s survey includes ParaNMT (originated around 2018), 
a dataset which was automatically generated by using 
back-translation to translate the non-English side of a large Czech-English parallel corpus.
This approach is a traditional approach, motivated by paraphrasing being a special case of statistical machine translation (SMT), 
i.e. monolingual translation. 
The goal is to find the best paraphrase $\hat{t}$ of a 
text in the source side $s$ to a text in the target side $t$: 
$ \hat{t} = argmax_{t \in t*} p(s|t)p(t) $.

\citet{zhou_paraphrase_2021} talk about neural methods to paraphrase generation.
They state that encoder encodes the source texts into a contextualized vector representation, 
along with a list of vector representations capturing the semantics of each word and context.
The decoder will generate paraphrases based on the vectors given by the encoder.
There is greedy decoding, where the words with the highest probability across the vocabulary are selected, 
and there is beam search, where the top $k$ paths are identified.
Since both greedy decoding and beam search are not specialized on paraphrase generation, but rather generic text generation, 
there exist methods to improve the quality of paraphrases.
One such method is to use blocking the words from the source text.
Other improvements based on encoder-decoder architectures include
\begin{itemize}
    \item \textbf{Copy mechanism}: The model can copy words from the source text to the target text (to counter the effect of rare and out-of-vocabulary words).
    \item \textbf{Attention mechanism}: The model can focus on different parts of the source text when generating each word in the target text.
    \item \textbf{\ac{rl}}: The model is trained to maximize a reward function that measures the quality of the generated paraphrases 
    (rather than minimize a loss that might not be aligned with metric used to evaluate paraphrase generation quality). 
    Discriminators of Generative Adversarial Networks (GANs) with policy gradient act like reward function in \ac{rl}.
\end{itemize}

Another neural method for paraphrase generation is to use \acp{vae} \cite{zhou_paraphrase_2021}.
The encoder learns a latent representation $z ~ N(\mu, \sigma)$.
The decoder generates realistic outputs conditioned on the latent representation $z$.
The learning objective is to reconstruct the original input from the latent representation $z$.
Paraphrase patterns are encoded into the latent representation $z$, where multiple paraphrase 
patterns and related words/ phrases are grouped under the same latent assignment.
Every time we sample a latent representation $z$, we get a different paraphrase pattern.

\citet{zhou_paraphrase_2021} also distinguish explicit and implicit syntax control:
Explicit syntax control methods encode the syntax tree of the source text into a list of vector representations 
and feed them into decoder at each timestep when decoding.
Implicit syntax control methods do not explicitly encode the syntax tree, 
but learns distributions over syntax information by \ac{vae}. 
The latent syntax variable is sampled from the learned distribution and will be fed into the decoder ar each decoding step.
Implicit method do not require exemplar sentences like exemplar methods.

\subsection{Paraphrase divergence}
\citet{fu_learning_2024} state that paraphrase divergence (cf. \autoref{sec:definitions}) can be explained as language models 
not only learn knowledge but also expression patterns associated with the knowledge from a corpus during pre-training.
Ideally, prompt learning should be independent of questions, but in reality, prompts are task- or domain-specific.
Hence, preferences for a certain format within a particular task or domain learnt during training persist in the model.
In other words, \acp{llm} may exhibit different preferences for various semantics.

\subsection{Metrics and Evaluation}
There is manual (by humans) evaluation and automatic evaluation for paraphrase generation \cite{fu_learning_2024,zhou_paraphrase_2021}.
According to \citet{zhou_paraphrase_2021}, automatic evaluation metrics mainly focus on the n-gram overlaps instead of meaning, 
and hence, human evaluation is more accurate and has a higher quality.
In the following, we focus on automatic evaluation.

\bluert{} is machine evaluation metric for paraphrase generation.
\citet{fu_learning_2024} use \bluert{} to filter out incorrect paraphrases (i.e. using a threshold $\theta$).

BLEU (2002) was developed for machine translation \cite{zhou_paraphrase_2021}.
According to \citet{zhou_paraphrase_2021}'s survey, BLEU is the most frequently used metric for paraphrase generation.
BLEU is unable to measure semantic equivalents when applied to low-resource languages.
Moreover, BLEU fails to capture good paraphrases that are not similar to the reference text.
\citet{zhou_paraphrase_2021} suggest combining BLEU with human evaluation to overcome its limitations.
METEOR (2014) aims to address BLEU's shortcomings.
It has better correlation with human judgement at the sentence/segment level than BLEU \cite{zhou_paraphrase_2021}.
ROUGE (2004) is a recall-based metric developed for text summarization \cite{zhou_paraphrase_2021}.
ROUGE can focus on the word variations and diversity.
It has multiple versions, the most popular ones include ROUGE-N (computing the n-gram recall), 
ROUGE-L (computing the longest common subsequence).
TER (2006) was developed for machine translation \cite{zhou_paraphrase_2021}.
It computes the number of edits required to change the translation until it matches the reference translation.
It ranges from 0 (i.e. no edits needed) to 1 (i.e. all words need to be changed).,


Popular paraphrase categories include \cite{fu_learning_2024}:
\begin{itemize}
    \item Top-level classification perspective: 
        \begin{itemize}
            \item Lexicon-based changes
            \item Morphology-based changes
            \item others
        \end{itemize}
    \item Second-level classification perspective:
        \begin{itemize}
            \item Change of format
            \item Semantic-based
            \item Change of order
        \end{itemize}
\end{itemize}

\citet{fu_learning_2024} give three prompt tips:
\begin{enumerate}
    \item Make question/ prompt as clear as possible even if some restrictive requirements may seem unnecessary.
    \item Place significant details, including restrictive elements such as time, place, manner, reason, purpose and conditions, at the beginning of the prompt.
    \item Pay attention to spelling, i.e. proper nouns, title, honorifics, abbreviations, acronyms and observe capitalization.
\end{enumerate}

\citet{zhou_paraphrase_2021} claim it is difficult to control the style of generated paraphrases.


% metric
\citet{fu_learning_2024} describe the Gini Coefficient as a measure of inequality in a distribution, ranging from 0 (i.e. even distribution across categories) to 1.

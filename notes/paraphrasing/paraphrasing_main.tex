\section{Paraphrasing}
\label{sec:paraphrasing}
\newcommand{\bluert}{\href{https://github.com/google-research/bleurt}{BLUERT}}

\subsection{Approaches to Paraphrasing}

Traditional approaches include thesaurus-based methods, where paraphrases are generated by 
replacing words with their synonyms from a thesaurus \cite{zhou_paraphrase_2021}.
Hence, this is called word-level paraphrasing.
First, all synonyms for the words to be replaced are retrieved from a thesaurus 
(or alignment tables based on IBM model, or WordNet).
Then, the optimal candidate is selected based on the context of the sentence. 
Although simple, this approach lacks diversity \cite{zhou_paraphrase_2021}.

\citet{fu_learning_2024} propose PEARL, a black-box paraphrasing model to meet \ac{llm} expression style.

UPRISE is a universal prompt auto-retrieval method that tunes a lightweight prompt retriever based on contrastive learning \cite{fu_learning_2024}.
There is also an official recommended template for manual prompt construction \cite{fu_learning_2024}.

\citet{zhou_paraphrase_2021}'s survey includes ParaNMT (originated around 2018), 
a dataset which was automatically generated by using 
back-translation to translate the non-English side of a large Czech-English parallel corpus.
This approach is a traditional approach, motivated by paraphrasing being a special case of statistical machine translation (SMT), 
i.e. monolingual translation. 
The goal is to find the best paraphrase $\hat{t}$ of a 
text in the source side $s$ to a text in the target side $t$: 
$ \hat{t} = argmax_{t \in t*} p(s|t)p(t) $.

\citet{zhou_paraphrase_2021} talk about neural methods to paraphrase generation.
They state that encoder encodes the source texts into a contextualized vector representation, 
along with a list of vector representations capturing the semantics of each word and context.
The decoder will generate paraphrases based on the vectors given by the encoder.
There is greedy decoding, where the words with the highest probability across the vocabulary are selected, 
and there is beam search, where the top $k$ paths are identified.
Since both greedy decoding and beam search are not specialized on paraphrase generation, but rather generic text generation, 
there exist methods to improve the quality of paraphrases.
One such method is to use blocking the words from the source text.
Other improvements based on encoder-decoder architectures include
\begin{itemize}
    \item \textbf{Copy mechanism}: The model can copy words from the source text to the target text (to counter the effect of rare and out-of-vocabulary words).
    \item \textbf{Attention mechanism}: The model can focus on different parts of the source text when generating each word in the target text.
    \item \textbf{\ac{rl}}: The model is trained to maximize a reward function that measures the quality of the generated paraphrases 
    (rather than minimize a loss that might not be aligned with metric used to evaluate paraphrase generation quality). 
    Discriminators of Generative Adversarial Networks (GANs) with policy gradient act like reward function in \ac{rl}.
\end{itemize}

Another neural method for paraphrase generation is to use \acp{vae} \cite{zhou_paraphrase_2021}.
The encoder learns a latent representation $z ~ N(\mu, \sigma)$.
The decoder generates realistic outputs conditioned on the latent representation $z$.
The learning objective is to reconstruct the original input from the latent representation $z$.
Paraphrase patterns are encoded into the latent representation $z$, where multiple paraphrase 
patterns and related words/ phrases are grouped under the same latent assignment.
Every time we sample a latent representation $z$, we get a different paraphrase pattern.

\citet{zhou_paraphrase_2021} also distinguish explicit and implicit syntax control:
Explicit syntax control methods encode the syntax tree of the source text into a list of vector representations 
and feed them into decoder at each timestep when decoding \cite{zhou_paraphrase_2021,palivela_optimization_2021}.
Implicit syntax control methods do not explicitly encode the syntax tree, 
but learns distributions over syntax information by \ac{vae}. 
The latent syntax variable is sampled from the learned distribution and will be fed into the decoder at each decoding step.
Implicit method do not require exemplar sentences like exemplar methods \cite{zhou_paraphrase_2021}.
\citet{palivela_optimization_2021} second major category of \ac{pg} is using pre-trained language models finetuning \acp{llm}.

\citet{palivela_optimization_2021} describe the \ac{t5} model as an algorithm that aims to convert every language problem into a text-to-text format.
\ac{t5} was trained on a mix of labeled (Colossal Clean Crawled Corpus) and unlabeled data \cite{palivela_optimization_2021}.

\citet{master_thesis_paraphrasing_2024} compare open-source \acp{llm} for paraphrasing.
They also provide descriptions of the models, including their architecture.
The models include Gemma (which they found to add asterics around the output), LLaMA (diverse version, where LLaMA3 significantly outperformed all other models), 
Mistral, and Phi3, Solar, and Starling (which they found to produce unclear outputs, due to excessive explanation of the model's actions).
Their first prompting strategy is \texttt{Paraphrase the following user story and output only paraphrased version:\\
\{user\_story\}}.
They tried including the stylometry metric and how they wanted to alter the metric: 
\texttt{Based on the following instruction: \{option\} \{stylometry\_metric\}.\\
Paraphrase the following user story and output only paraphrased version:\\
\{user\_story\}}.
Options include increase, decrease, or don't change.
The styloemtric metric is one of 23 options they defined.
They did not know to what extent the \ac{llm} was familiar with the stylometry metric. 
Hence, they also tried to include a definition of the metric in the prompt:
\texttt{\{stylometry metric definition\}\\Based on the following instruction:\\
\{option\} \{stylometry\_metric\} \\
Paraphrase the following user story and output only paraphrased version: \{user\_story\}}.
\citet{master_thesis_paraphrasing_2024} found that \acp{llm} struggle to adhere instructions with regard to specific stylometry metrics, 
since it generally did not enhance the performance of the models.
They also finetuned the \acp{llm} via the Unsloth library (pg. 21 \cite{master_thesis_paraphrasing_2024}).
Their experiments where run on Google Colab (pg. 28).


\subsection{Paraphrase divergence}
\citet{fu_learning_2024} state that paraphrase divergence (cf. \autoref{sec:definitions}) can be explained as language models 
not only learn knowledge but also expression patterns associated with the knowledge from a corpus during pre-training.
Ideally, prompt learning should be independent of questions, but in reality, prompts are task- or domain-specific.
Hence, preferences for a certain format within a particular task or domain learnt during training persist in the model.
In other words, \acp{llm} may exhibit different preferences for various semantics.


\subsection{Metrics and Evaluation}
There is manual (by humans) evaluation and automatic evaluation for paraphrase generation \cite{fu_learning_2024,zhou_paraphrase_2021}.
According to \citet{zhou_paraphrase_2021}, automatic evaluation metrics mainly focus on the n-gram overlaps instead of meaning, 
and hence, human evaluation is more accurate and has a higher quality.
In the following, we focus on automatic evaluation.

\bluert{} is machine evaluation metric for paraphrase generation.
\citet{fu_learning_2024} use \bluert{} to filter out incorrect paraphrases (i.e. using a threshold $\theta$).

BLEU (Bilingual Evaluation Understudy \cite{palivela_optimization_2021}) (2002) was developed for machine translation \cite{zhou_paraphrase_2021}.
According to \citet{zhou_paraphrase_2021}'s survey, BLEU is the most frequently used metric for paraphrase generation.
BLEU counts the matching n-grams in the generated text to the gold/ reference text.
Unigrams are token-wise and bi-grams are word-pairs \citet{palivela_optimization_2021}.
BLEU is unable to measure semantic equivalents when applied to low-resource languages \cite{zhou_paraphrase_2021}.
Moreover, BLEU fails to capture good paraphrases that are not similar to the reference text.
\citet{zhou_paraphrase_2021} suggest combining BLEU with human evaluation to overcome its limitations.
GLEU (Google-BLEU) is a variant of BLEU that was developed to be closer to human judgement, and to 
overcome BLEU's drawback of per sentence reward objective \cite{palivela_optimization_2021}.
GLEU computes n-gram precisions overgold/reference paraphrases and weighs n-grams by their change from the source text \cite{palivela_optimization_2021}.

METEOR (Metric for Evaluation of Translation with Explicit Ordering \cite{palivela_optimization_2021}) (2014) aims to address BLEU's shortcomings.
It has better correlation with human judgement at the sentence/segment level than BLEU \cite{zhou_paraphrase_2021}.
It computes a weighted F-score based on unigrams with a penality function whenever an incorrect word is encountered \cite{palivela_optimization_2021}.

ROUGE (Recall-Oriented Understudy for Gisting Evaluation \cite{palivela_optimization_2021}) (2004) 
is a recall-based metric developed for text summarization \cite{zhou_paraphrase_2021,palivela_optimization_2021}.
ROUGE can focus on the word variations and diversity.
It has multiple versions, the most popular ones include 
ROUGE-N (computing the n-gram recall) \cite{zhou_paraphrase_2021,palivela_optimization_2021}, 
ROUGE-L (computing the longest common subsequence) \cite{zhou_paraphrase_2021,palivela_optimization_2021}, 
ROUGE-W (Weighted longest common subsequence) \cite{palivela_optimization_2021}, 
ROUGE-S (skip-bigram co-occurrence statistics) \cite{palivela_optimization_2021}.
REOUGE-1 computes the recall by analysing the matching unigrams between the generated paraphrase and the reference paraphrase \cite{palivela_optimization_2021}.

TER (2006) was developed for machine translation \cite{zhou_paraphrase_2021}.
It computes the number of edits required to change the translation until it matches the reference translation.
It ranges from 0 (i.e. no edits needed) to 1 (i.e. all words need to be changed).

\citet{fu_learning_2024} describe the Gini Coefficient as a measure of inequality in a distribution, ranging from 0 (i.e. even distribution across categories) to 1.

\citet{master_thesis_paraphrasing_2024} include a table (tab. 3.1, pg. 18) with numerous stylometric metric including readbility, vocabulary richness, and word/character counts.

\citet{palivela_optimization_2021} state that accuracy, precision, recall and F1-score are suitable for \ac{pi}.
For \ac{pg}, they suggest ROUGE, BLEU, GLEU, WER (Word Error Rate), and METEOR as suitable metrics.
WER is the number of substitutions (replacements of words), insertions (adding words) and deletions (removing words) 
divided by the total number of words in the reference text \cite{palivela_optimization_2021}.


Popular paraphrase categories include \cite{fu_learning_2024}:
\begin{itemize}
    \item Top-level classification perspective: 
        \begin{itemize}
            \item Lexicon-based changes
            \item Morphology-based changes
            \item others
        \end{itemize}
    \item Second-level classification perspective:
        \begin{itemize}
            \item Change of format
            \item Semantic-based
            \item Change of order
        \end{itemize}
\end{itemize}

\citet{fu_learning_2024} give three prompt tips:
\begin{enumerate}
    \item Make question/ prompt as clear as possible even if some restrictive requirements may seem unnecessary.
    \item Place significant details, including restrictive elements such as time, place, manner, reason, purpose and conditions, at the beginning of the prompt.
    \item Pay attention to spelling, i.e. proper nouns, title, honorifics, abbreviations, acronyms and observe capitalization.
\end{enumerate}

\citet{zhou_paraphrase_2021} claim it is difficult to control the style of generated paraphrases.


\subsection{\ac{llm} training}

\citet{master_thesis_paraphrasing_2024} describes the training of \acp{llm} as a three-stage development process:
Starting with pre-training on a vast dataset to acquire comprehensive knowledge of language and the world, 
followed by supervised finetuning for specific downstream tasks and finally,
human preference alignment training to mimic human behaviour.

\subsection{Preprocessing of Dataset}

\citet{palivela_optimization_2021} propose the following transformation steps for the dataset:
\begin{enumerate}
    \item Remove sentence pairs having more than 60$\%$ unigram, bigram or trigram overlap 
    (to discourage model from copying input sentence and increases diversity of paraphrases).
    \item Remove sentence pairs having very little semnatic similarity by using Sentence-BERT 
    (to force model to generate semantically similar paraphrases).
    \item If dataset contains non-paraphrase sentence pairs, remove them.   
\end{enumerate}


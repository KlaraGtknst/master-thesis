\section{Paraphrasing}
\label{sec:paraphrasing}
\newcommand{\bluert}{\href{https://github.com/google-research/bleurt}{BLUERT}}

\citet{kurt_pehlivanoglu_comparative_2024} intentionally decided to use synthetic data to meet the study's specific goals.
According to them, synthetic data allows for a controlled and reproducible setup, ensuring all reference sentences are consistently generated.
They define dataset quality beyond mere reference-paraphrase sentence pairs, but also include multiple evaluation metrics.
Their code is available on \href{https://github.com/massyakur/ParaGPT}{GitHub}.

\subsection{Approaches to Paraphrasing}

Traditional approaches include thesaurus-based methods, where paraphrases are generated by 
replacing words with their synonyms from a thesaurus \cite{zhou_paraphrase_2021}.
Hence, this is called word-level paraphrasing.
First, all synonyms for the words to be replaced are retrieved from a thesaurus 
(or alignment tables based on IBM model, or WordNet).
Then, the optimal candidate is selected based on the context of the sentence. 
Although simple, this approach lacks diversity \cite{zhou_paraphrase_2021}.

\citet{fu_learning_2024} propose PEARL, a black-box paraphrasing model to meet \ac{llm} expression style.

UPRISE is a universal prompt auto-retrieval method that tunes a lightweight prompt retriever based on contrastive learning \cite{fu_learning_2024}.
There is also an official recommended template for manual prompt construction \cite{fu_learning_2024}.

\citet{zhou_paraphrase_2021}'s survey includes ParaNMT (originated around 2018), 
a dataset which was automatically generated by using 
back-translation to translate the non-English side of a large Czech-English parallel corpus.
This approach is a traditional approach, motivated by paraphrasing being a special case of statistical machine translation (SMT), 
i.e. monolingual translation. 
The goal is to find the best paraphrase $\hat{t}$ of a 
text in the source side $s$ to a text in the target side $t$: 
$ \hat{t} = argmax_{t \in t*} p(s|t)p(t) $.

\citet{zhou_paraphrase_2021} talk about neural methods to paraphrase generation.
They state that encoder encodes the source texts into a contextualized vector representation, 
along with a list of vector representations capturing the semantics of each word and context.
The decoder will generate paraphrases based on the vectors given by the encoder.
There is greedy decoding, where the words with the highest probability across the vocabulary are selected, 
and there is beam search, where the top $k$ paths are identified.
Since both greedy decoding and beam search are not specialized on paraphrase generation, but rather generic text generation, 
there exist methods to improve the quality of paraphrases.
One such method is to use blocking the words from the source text.
Other improvements based on encoder-decoder architectures include
\begin{itemize}
    \item \textbf{Copy mechanism}: The model can copy words from the source text to the target text (to counter the effect of rare and out-of-vocabulary words).
    \item \textbf{Attention mechanism}: The model can focus on different parts of the source text when generating each word in the target text.
    \item \textbf{\ac{rl}}: The model is trained to maximize a reward function that measures the quality of the generated paraphrases 
    (rather than minimize a loss that might not be aligned with metric used to evaluate paraphrase generation quality). 
    Discriminators of Generative Adversarial Networks (GANs) with policy gradient act like reward function in \ac{rl}.
\end{itemize}

Another neural method for paraphrase generation is to use \acp{vae} \cite{zhou_paraphrase_2021}.
The encoder learns a latent representation $z ~ N(\mu, \sigma)$.
The decoder generates realistic outputs conditioned on the latent representation $z$.
The learning objective is to reconstruct the original input from the latent representation $z$.
Paraphrase patterns are encoded into the latent representation $z$, where multiple paraphrase 
patterns and related words/ phrases are grouped under the same latent assignment.
Every time we sample a latent representation $z$, we get a different paraphrase pattern.

\citet{zhou_paraphrase_2021} also distinguish explicit and implicit syntax control:
Explicit syntax control methods encode the syntax tree of the source text into a list of vector representations 
and feed them into decoder at each timestep when decoding \cite{zhou_paraphrase_2021,palivela_optimization_2021}.
Implicit syntax control methods do not explicitly encode the syntax tree, 
but learns distributions over syntax information by \ac{vae}. 
The latent syntax variable is sampled from the learned distribution and will be fed into the decoder at each decoding step.
Implicit method do not require exemplar sentences like exemplar methods \cite{zhou_paraphrase_2021}.
\citet{palivela_optimization_2021} second major category of \ac{pg} is using pre-trained language models finetuning \acp{llm}.

\citet{palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024} describe the 
\ac{t5} model as an algorithm that aims to convert every language problem into a text-to-text format.
\ac{t5} was trained on a mix of labeled (Colossal Clean Crawled Corpus) and unlabeled data \cite{palivela_optimization_2021}.
Its pre-training was done on data-rich tasks before fine-tuning on downstream tasks \cite{kurt_pehlivanoglu_comparative_2024}.

\citet{master_thesis_paraphrasing_2024} compare open-source \acp{llm} for paraphrasing.
They also provide descriptions of the models, including their architecture.
The models include Gemma (which they found to add asterics around the output), LLaMA (diverse version, where LLaMA3 significantly outperformed all other models), 
Mistral, and Phi3, Solar, and Starling (which they found to produce unclear outputs, due to excessive explanation of the model's actions).
Their first prompting strategy is \texttt{Paraphrase the following user story and output only paraphrased version:\\
\{user\_story\}}.
They tried including the stylometry metric and how they wanted to alter the metric: 
\texttt{Based on the following instruction: \{option\} \{stylometry\_metric\}.\\
Paraphrase the following user story and output only paraphrased version:\\
\{user\_story\}}.
Options include increase, decrease, or don't change.
The styloemtric metric is one of 23 options they defined.
They did not know to what extent the \ac{llm} was familiar with the stylometry metric. 
Hence, they also tried to include a definition of the metric in the prompt:
\texttt{\{stylometry metric definition\}\\Based on the following instruction:\\
\{option\} \{stylometry\_metric\} \\
Paraphrase the following user story and output only paraphrased version: \{user\_story\}}.
\citet{master_thesis_paraphrasing_2024} found that \acp{llm} struggle to adhere instructions with regard to specific stylometry metrics, 
since it generally did not enhance the performance of the models.
They also finetuned the \acp{llm} via the Unsloth library (pg. 21 \cite{master_thesis_paraphrasing_2024}).
Their experiments where run on Google Colab (pg. 28).

\citet{krishna_paraphrasing_2023} built \href{https://huggingface.co/kalpeshk2011/dipper-paraphraser-xxl}{DIPPER} (NeurIPS 2023, not accepted).
 

\citet{kurt_pehlivanoglu_comparative_2024} use ChatGPT to generate paraphrases with the prompt:
\texttt{paraphrase:\{sentence\}}.
They have a table of existing datasets (tab. 1, pg. 4) with genre specified, including MRPC a news dataset.
They use more specific prompts, such as:
\begin{itemize}
    \item \texttt{Generate sentences of kind simple}
    \item \texttt{Generate sentences of kind simple, compound, complex and compound-complex}
    \item \texttt{Generate compound-complex sentences}
    \item \texttt{Generate sentences with a complex conditional clause}
\end{itemize}
Other models they us include GPT-3 (Api using the Text-Davinci-003 model with temperature$=0.7$ and top\_p$=1$), 
and a pre-trained \ac{t5}-based model known as 'prithivida/parrot\_paraphraser\_on\_\ac{t5}' on Huggingface (fine-tuned for paraphrasing tasks).
Since some \ac{t5} paraphrases were the same as the input sentence, they used \textcolor{red}{beam search} technique to rephrase these identical sentences.
Even though, \ac{t5} was the best in terms of BERTScore, 
\citet{kurt_pehlivanoglu_comparative_2024} found that it had the lowest fluency (i.e. grammatical scceptability) measured by \ac{t5}-CoLA.

The Parrot Paraphraser is a rule-based \ac{t5}-based paraphrase generation framework whose 
rules balance adequacy (semantic preservation), fluency (text smoothness), and diversity (lexical/syntactic variation) \cite{zhou_paraphrase_2025}.

\citet{hassanipour_ability_2024} use the following prompts to generate paraphrases:
\begin{itemize}
    \item \texttt{Paraphrase the text}
    \item \texttt{Rephrase the text}
    \item \texttt{Reduce the plagiarism of the text}
    \item \texttt{Rephrase it in a way that conveys the same meaning using different words and sentence structure}
    \item \texttt{Reword this text using different language}
\end{itemize}
They found no significant difference between the prompts in terms of flagged plagiarism.
They propose that the prompts are too brief, too similar to each other, or that ChatGPT understands the intention of the user when reading any of these prompts.
They found that generated texts based on single paragraph were less prone to be flagged as plagiarized than those based on fragmented text, i.e. multiple paragraphs
(maybe the context and coherence improves ChatGPT's understanding of the text).
Multiple iterations if paraphrasing achieved modest improvements in reducing plagiarism, indicating that ChatGPT has the ability to learn.

\citet{zhou_paraphrase_2025} state that prompts should aligned with the \ac{llm} preference (of prompts).

Few-shot prompting for paraphrases is a technique where \acp{llm} are tasked to paraphrase text using a few samples of the desired paraphrase transformation \cite{zhou_paraphrase_2025}.


\subsection{Paraphrase divergence}
\citet{fu_learning_2024} state that paraphrase divergence (cf. \autoref{sec:definitions}) can be explained as language models 
not only learn knowledge but also expression patterns associated with the knowledge from a corpus during pre-training.
Ideally, prompt learning should be independent of questions, but in reality, prompts are task- or domain-specific.
Hence, preferences for a certain format within a particular task or domain learnt during training persist in the model.
In other words, \acp{llm} may exhibit different preferences for various semantics.


\subsection{Metrics and Evaluation}
There is manual (by humans) evaluation and automatic evaluation for paraphrase generation \cite{fu_learning_2024,zhou_paraphrase_2021}.
According to \citet{zhou_paraphrase_2021}, automatic evaluation metrics mainly focus on the n-gram overlaps instead of meaning, 
and hence, human evaluation is more accurate and has a higher quality.
In the following, we focus on automatic evaluation.

\bluert{} is machine evaluation metric for paraphrase generation.
\citet{fu_learning_2024} use \bluert{} to filter out incorrect paraphrases (i.e. using a threshold $\theta$).

BLEU (Bilingual Evaluation Understudy \cite{palivela_optimization_2021,zhou_paraphrase_2025}) (2002) was developed for machine translation \cite{zhou_paraphrase_2021}.
According to \citet{zhou_paraphrase_2021}'s survey, BLEU is the most frequently used metric for paraphrase generation.
BLEU counts the matching (precision \cite{kurt_pehlivanoglu_comparative_2024}) n-grams in the generated text to the gold/ reference text \cite{palivela_optimization_2021}.
If the generated is significantly shorter than the reference text, a brevity penalty is applied.
A BLEU score approaching 1 signifies limited syntactic diversity (i.e. inadequate paraphrase) \cite{kurt_pehlivanoglu_comparative_2024}.
Unigrams are token-wise and bi-grams are word-pairs \citet{palivela_optimization_2021}.
BLEU is unable to measure semantic equivalents \cite{kurt_pehlivanoglu_comparative_2024,zhou_paraphrase_2021} 
when applied to low-resource languages \cite{zhou_paraphrase_2021}.
Moreover, BLEU fails to capture good paraphrases that are not similar to the reference text \cite{zhou_paraphrase_2021}.
\citet{kurt_pehlivanoglu_comparative_2024} found that BLEU tends to overestimate the quality of paraphrases.
\citet{zhou_paraphrase_2021} suggest combining BLEU with human evaluation to overcome its limitations.
GLEU (Google-BLEU) (ranges from 0 to 1 \cite{kurt_pehlivanoglu_comparative_2024}) is a variant of BLEU that was developed to be closer to human judgement, and to 
overcome BLEU's drawback of per sentence reward objective \cite{palivela_optimization_2021}.
GLEU computes n-gram precisions (overlaps \cite{kurt_pehlivanoglu_comparative_2024}) overgold/reference paraphrases 
and weighs n-grams by their change from the source text \cite{palivela_optimization_2021}.
GLEU assesses the fluency, order of n-grams, structural and semantic accuracy 
and penalizes shorter average m-gram lengths in the generated text compared to the reference \cite{kurt_pehlivanoglu_comparative_2024}.
Lower GLEU scores indicate greater diversity \cite{kurt_pehlivanoglu_comparative_2024}.

METEOR (Metric for Evaluation of Translation with Explicit Ordering \cite{palivela_optimization_2021}) 
(ranges from 0 to 1 \cite{kurt_pehlivanoglu_comparative_2024}) (2014) aims to address BLEU's shortcomings.
It has better correlation with human judgement at the sentence/segment level than BLEU \cite{zhou_paraphrase_2021}, 
because it not only consists of simple n-gram matching but also including synonymy and stemming \cite{kurt_pehlivanoglu_comparative_2024}.
It computes a weighted F-score (precision and recall according to \citet{kurt_pehlivanoglu_comparative_2024}) based on unigrams 
with a penality function whenever an incorrect word is encountered \cite{palivela_optimization_2021} 
(fragmentation to assess order and coherence of text according to \cite{kurt_pehlivanoglu_comparative_2024}).

ROUGE (Recall-Oriented Understudy for Gisting Evaluation \cite{palivela_optimization_2021}) (ranges from 0 to 1 \cite{kurt_pehlivanoglu_comparative_2024}) (2004) 
is a recall-based metric developed for text summarization \cite{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}.
ROUGE can focus on the word variations and diversity.
It has multiple versions, the most popular ones include 
ROUGE-N (computing the n-gram recall) \cite{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}, 
ROUGE-L (computing the longest common subsequence) \cite{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}, 
ROUGE-W (Weighted longest common subsequence) \cite{palivela_optimization_2021}, 
ROUGE-S (skip-bigram co-occurrence statistics) \cite{palivela_optimization_2021}.
REOUGE-1 computes the recall by analysing the matching unigrams between the generated paraphrase and the reference paraphrase \cite{palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}.
\citet{kurt_pehlivanoglu_comparative_2024} claim that ROUGE may not be adequate to assess semantic similarity and fluency.
Lower ROUGE scores indicate greater diversity \cite{kurt_pehlivanoglu_comparative_2024}.

TER (2006) was developed for machine translation \cite{zhou_paraphrase_2021}.
It computes the number of edits required to change the translation until it matches the reference translation.
It ranges from 0 (i.e. no edits needed) to 1 (i.e. all words need to be changed).

\citet{fu_learning_2024} describe the Gini Coefficient as a measure of inequality in a distribution, ranging from 0 (i.e. even distribution across categories) to 1.

\citet{master_thesis_paraphrasing_2024} include a table (tab. 3.1, pg. 18) with numerous stylometric metric including readbility, vocabulary richness, and word/character counts.

\citet{palivela_optimization_2021} state that accuracy, precision, recall and F1-score are suitable for \ac{pi}.
For \ac{pg}, they suggest ROUGE, BLEU, GLEU, WER (Word Error Rate), and METEOR as suitable metrics.
WER is the number of substitutions (replacements of words), insertions (adding words) and deletions (removing words) 
divided by the total number of words in the reference text \cite{palivela_optimization_2021}.

\citet{kurt_pehlivanoglu_comparative_2024} additionally use T5-STSB.
The metric is based on \ac{t5} model adapted to the Semantic Textual Similarity Benchmark (STSB).
It evaluates semantic equivalence by assigning a similarity score from 0 (no similarity) to 5 (complete equivalence) \cite{kurt_pehlivanoglu_comparative_2024}.

BERTScore calculates the cosine similarity between the contextual embeddings of the reference and generated texts. 
Hence, is assesses semantic equivalence and correlates well with human judgement \cite{kurt_pehlivanoglu_comparative_2024}.

\ac{t5}-CoLA metric (ranges from 0 to 5 \cite{kurt_pehlivanoglu_comparative_2024}) utilizes the Corpus of Linguistic Acceptability (CoLA) to evaluate the grammatical correctness of sentences and thus, 
contributes linguistic evaluation \cite{kurt_pehlivanoglu_comparative_2024}.

Generally, \citet{kurt_pehlivanoglu_comparative_2024} order the metrics by their contribution area:
\begin{itemize}
    \item semantic: BERTScore, STSB, METEOR
    \item Fluency: CoLA
    \item Diversity: ROUGE1/2/L, BLEU, GLEU
\end{itemize}

\citet{krishna_paraphrasing_2023} compute the lexical diversity using unigram token overlap and call it F1 score.
As a semantic similarity score, they use the ACL Antology 2022 published \href{https://aclanthology.org/2022.emnlp-demos.38.pdf}{P-SP}.

Popular paraphrase categories include \cite{fu_learning_2024}:
\begin{itemize}
    \item Top-level classification perspective: 
        \begin{itemize}
            \item Lexicon-based changes
            \item Morphology-based changes
            \item others
        \end{itemize}
    \item Second-level classification perspective:
        \begin{itemize}
            \item Change of format
            \item Semantic-based
            \item Change of order
        \end{itemize}
\end{itemize}
\citet{zhou_paraphrase_2025} (pg. 3, tab.1, and examples afterwards) define a topology of paraphrase types:
\begin{itemize}
    \item Morphology based: inflection changes (e.g. singular to plural), derivation changes (e.g. adjective to verb), functional word substitution (e.g. this to that).
    \item Lexicon based: Same polarity substitution (e.g. synonym), opposite polarity substitution (e.g. antonym), converse substitution (e.g. opposite view point), spelling changes, synthetic/ analytic substitution, relational substitution
    \item Syntax based: Negation switching (i.e. other negation), diathesis alternation (e.g. change position of verb), etc.
    \item Discourse based: Indirect/direct substitutions, sentence modality changes, punctuation changes, etc.
    \item Other changes: Change of order, change of format, etc.
\end{itemize}

\citet{fu_learning_2024} give three prompt tips:
\begin{enumerate}
    \item Make question/ prompt as clear as possible even if some restrictive requirements may seem unnecessary.
    \item Place significant details, including restrictive elements such as time, place, manner, reason, purpose and conditions, at the beginning of the prompt.
    \item Pay attention to spelling, i.e. proper nouns, title, honorifics, abbreviations, acronyms and observe capitalization.
\end{enumerate}

\citet{zhou_paraphrase_2021} claim it is difficult to control the style of generated paraphrases.

Another approach to paraphrasing is back-translation, which may limit diversity \cite{zhou_paraphrase_2025}.


\subsection{\ac{llm} training}

\citet{master_thesis_paraphrasing_2024} describes the training of \acp{llm} as a three-stage development process:
Starting with pre-training on a vast dataset to acquire comprehensive knowledge of language and the world, 
followed by supervised finetuning for specific downstream tasks and finally,
human preference alignment training to mimic human behaviour.

\subsection{Preprocessing of Dataset}

\citet{palivela_optimization_2021} propose the following transformation steps for the dataset:
\begin{enumerate}
    \item Remove sentence pairs having more than 60$\%$ unigram, bigram or trigram overlap 
    (to discourage model from copying input sentence and increases diversity of paraphrases).
    \item Remove sentence pairs having very little semantic similarity by using Sentence-BERT 
    (to force model to generate semantically similar paraphrases).
    \item If dataset contains non-paraphrase sentence pairs, remove them.   
\end{enumerate}

\subsection{Plagiarism Detection}
\citet{hassanipour_ability_2024} analyse texts for plagiarism using the \textcolor{orange}{iThenticate} tool.
It is really \href{https://www.ithenticate.com/pricing}{expensive}.
Alternatives to detectors of \ac{ai}-generated text include watermarking and statistical outlier detection 
(\ac{ai}-generated text's artefacts such as irregularities in entropy and perplexity) \cite{krishna_paraphrasing_2023}.
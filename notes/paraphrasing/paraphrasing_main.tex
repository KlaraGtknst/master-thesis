\section{Paraphrasing}
\label{sec:paraphrasing}


\citet{kurt_pehlivanoglu_comparative_2024} intentionally decided to use synthetic data to meet the study's specific goals.
According to them, synthetic data allows for a controlled and reproducible setup, ensuring all reference sentences are consistently generated.
They define dataset quality beyond mere reference-paraphrase sentence pairs, but also include multiple evaluation metrics.
Their code is available on \href{https://github.com/massyakur/ParaGPT}{GitHub}.

\subsection{Approaches to Paraphrasing}

Traditional approaches include thesaurus-based methods, where paraphrases are generated by 
replacing words with their synonyms from a thesaurus \citep{zhou_paraphrase_2021}.
Hence, this is called word-level paraphrasing.
First, all synonyms for the words to be replaced are retrieved from a thesaurus 
(or alignment tables based on IBM model, or WordNet).
Then, the optimal candidate is selected based on the context of the sentence. 
Although simple, this approach lacks diversity \citep{zhou_paraphrase_2021}.

\citet{fu_learning_2024} propose PEARL, a black-box paraphrasing model to meet \ac{llm} expression style.

UPRISE is a universal prompt auto-retrieval method that tunes a lightweight prompt retriever based on contrastive learning \citep{fu_learning_2024}.
There is also an official recommended template for manual prompt construction \citep{fu_learning_2024}.

\citet{zhou_paraphrase_2021}'s survey includes ParaNMT (originated around 2018), 
a dataset which was automatically generated by using 
back-translation to translate the non-English side of a large Czech-English parallel corpus.
This approach is a traditional approach, motivated by paraphrasing being a special case of statistical machine translation (SMT), 
i.e. monolingual translation. 
The goal is to find the best paraphrase $\hat{t}$ of a 
text in the source side $s$ to a text in the target side $t$: 
$ \hat{t} = argmax_{t \in t*} p(s|t)p(t) $.

\citet{zhou_paraphrase_2021} talk about neural methods to paraphrase generation.
They state that encoder encodes the source texts into a contextualized vector representation, 
along with a list of vector representations capturing the semantics of each word and context.
The decoder will generate paraphrases based on the vectors given by the encoder.
There is greedy decoding, where the words with the highest probability across the vocabulary are selected, 
and there is beam search, where the top $k$ paths are identified.
Since both greedy decoding and beam search are not specialized on paraphrase generation, but rather generic text generation, 
there exist methods to improve the quality of paraphrases.
One such method is to use blocking the words from the source text.
Other improvements based on encoder-decoder architectures include
\begin{itemize}
    \item \textbf{Copy mechanism}: The model can copy words from the source text to the target text (to counter the effect of rare and out-of-vocabulary words).
    \item \textbf{Attention mechanism}: The model can focus on different parts of the source text when generating each word in the target text.
    \item \textbf{\ac{rl}}: The model is trained to maximize a reward function that measures the quality of the generated paraphrases 
    (rather than minimize a loss that might not be aligned with metric used to evaluate paraphrase generation quality). 
    Discriminators of Generative Adversarial Networks (GANs) with policy gradient act like reward function in \ac{rl}.
\end{itemize}

Another neural method for paraphrase generation is to use \acp{vae} \citep{zhou_paraphrase_2021}.
The encoder learns a latent representation $z ~ N(\mu, \sigma)$.
The decoder generates realistic outputs conditioned on the latent representation $z$.
The learning objective is to reconstruct the original input from the latent representation $z$.
Paraphrase patterns are encoded into the latent representation $z$, where multiple paraphrase 
patterns and related words/ phrases are grouped under the same latent assignment.
Every time we sample a latent representation $z$, we get a different paraphrase pattern.

\citet{zhou_paraphrase_2021} also distinguish explicit and implicit syntax control:
Explicit syntax control methods encode the syntax tree of the source text into a list of vector representations 
and feed them into decoder at each timestep when decoding \citep{zhou_paraphrase_2021,palivela_optimization_2021}.
Implicit syntax control methods do not explicitly encode the syntax tree, 
but learns distributions over syntax information by \ac{vae}. 
The latent syntax variable is sampled from the learned distribution and will be fed into the decoder at each decoding step.
Implicit method do not require exemplar sentences like exemplar methods \citep{zhou_paraphrase_2021}.
\citet{palivela_optimization_2021} second major category of \ac{pg} is using pre-trained language models finetuning \acp{llm}.

\citet{palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024} describe the 
\ac{t5} model as an algorithm that aims to convert every language problem into a text-to-text format.
\ac{t5} was trained on a mix of labeled (Colossal Clean Crawled Corpus) and unlabeled data \citep{palivela_optimization_2021}.
Its pre-training was done on data-rich tasks before fine-tuning on downstream tasks \citep{kurt_pehlivanoglu_comparative_2024}.

\citet{master_thesis_paraphrasing_2024} compare open-source \acp{llm} for paraphrasing.
They also provide descriptions of the models, including their architecture.
The models include Gemma (which they found to add asterics around the output), LLaMA (diverse version, where LLaMA3 significantly outperformed all other models), 
Mistral, and Phi3, Solar, and Starling (which they found to produce unclear outputs, due to excessive explanation of the model's actions).
Their first prompting strategy is \texttt{Paraphrase the following user story and output only paraphrased version:\\
\{user\_story\}}.
They tried including the stylometry metric and how they wanted to alter the metric: 
\texttt{Based on the following instruction: \{option\} \{stylometry\_metric\}.\\
Paraphrase the following user story and output only paraphrased version:\\
\{user\_story\}}.
Options include increase, decrease, or don't change.
The styloemtric metric is one of 23 options they defined.
They did not know to what extent the \ac{llm} was familiar with the stylometry metric. 
Hence, they also tried to include a definition of the metric in the prompt:
\texttt{\{stylometry metric definition\}\\Based on the following instruction:\\
\{option\} \{stylometry\_metric\} \\
Paraphrase the following user story and output only paraphrased version: \{user\_story\}}.
\citet{master_thesis_paraphrasing_2024} found that \acp{llm} struggle to adhere instructions with regard to specific stylometry metrics, 
since it generally did not enhance the performance of the models.
They also finetuned the \acp{llm} via the Unsloth library (pg. 21 \citep{master_thesis_paraphrasing_2024}).
Their experiments where run on Google Colab (pg. 28).

\citet{krishna_paraphrasing_2023} built \href{https://huggingface.co/kalpeshk2011/dipper-paraphraser-xxl}{DIPPER} (NeurIPS 2023, not accepted).
 

\citet{kurt_pehlivanoglu_comparative_2024} use ChatGPT to generate paraphrases with the prompt:
\texttt{paraphrase:\{sentence\}}.
They have a table of existing datasets (tab. 1, pg. 4) with genre specified, including MRPC a news dataset.
They use more specific prompts, such as:
\begin{itemize}
    \item \texttt{Generate sentences of kind simple}
    \item \texttt{Generate sentences of kind simple, compound, complex and compound-complex}
    \item \texttt{Generate compound-complex sentences}
    \item \texttt{Generate sentences with a complex conditional clause}
\end{itemize}
Other models they us include GPT-3 (Api using the Text-Davinci-003 model with temperature$=0.7$ and top\_p$=1$), 
and a pre-trained \ac{t5}-based model known as 'prithivida/parrot\_paraphraser\_on\_\ac{t5}' on Huggingface (fine-tuned for paraphrasing tasks).
Since some \ac{t5} paraphrases were the same as the input sentence, they used \textcolor{red}{beam search} technique to rephrase these identical sentences.
Even though, \ac{t5} was the best in terms of BERTScore, 
\citet{kurt_pehlivanoglu_comparative_2024} found that it had the lowest fluency (i.e. grammatical scceptability) measured by \ac{t5}-CoLA.

The Parrot Paraphraser is a rule-based \ac{t5}-based paraphrase generation framework whose 
rules balance adequacy (semantic preservation), fluency (text smoothness), and diversity (lexical/syntactic variation) \citep{zhou_paraphrase_2025}.

\citet{hassanipour_ability_2024} use the following prompts to generate paraphrases:
\begin{itemize}
    \item \texttt{Paraphrase the text}
    \item \texttt{Rephrase the text}
    \item \texttt{Reduce the plagiarism of the text}
    \item \texttt{Rephrase it in a way that conveys the same meaning using different words and sentence structure}
    \item \texttt{Reword this text using different language}
\end{itemize}
They found no significant difference between the prompts in terms of flagged plagiarism.
They propose that the prompts are too brief, too similar to each other, or that ChatGPT understands the intention of the user when reading any of these prompts.
They found that generated texts based on single paragraph were less prone to be flagged as plagiarized than those based on fragmented text, i.e. multiple paragraphs
(maybe the context and coherence improves ChatGPT's understanding of the text).
Multiple iterations if paraphrasing achieved modest improvements in reducing plagiarism, indicating that ChatGPT has the ability to learn.

\citet{zhou_paraphrase_2025} state that prompts should aligned with the \ac{llm} preference (of prompts).

Few-shot prompting for paraphrases is a technique where \acp{llm} are tasked to paraphrase text using a few samples of the desired paraphrase transformation \citep{zhou_paraphrase_2025}.

High-quality paraphrases are texts with high semantic similarity and high lexical and syntactic diversity to the original text \citep{gohsen_captions_2023}.


\subsection{Paraphrase divergence}
\citet{fu_learning_2024} state that paraphrase divergence (cf. \autoref{sec:definitions}) can be explained as language models 
not only learn knowledge but also expression patterns associated with the knowledge from a corpus during pre-training.
Ideally, prompt learning should be independent of questions, but in reality, prompts are task- or domain-specific.
Hence, preferences for a certain format within a particular task or domain learnt during training persist in the model.
In other words, \acp{llm} may exhibit different preferences for various semantics.


\subsection{Metrics and Evaluation}
There is manual (by humans) evaluation and automatic evaluation for paraphrase generation \citep{fu_learning_2024,zhou_paraphrase_2021}.
According to \citet{zhou_paraphrase_2021}, automatic evaluation metrics mainly focus on the n-gram overlaps instead of meaning, 
and hence, human evaluation is more accurate and has a higher quality.
In the following, we focus on automatic evaluation.

There is syntactic and semantic evaluation of paraphrases \citep{gohsen_captions_2023}.
Metrics for syntactic evaluation include BLEU, ROUGE-1, ROUGE-L, 
while metrics for semantic similarity include BERTScore, 
cosine similarity of dense vector representations derived from a BERT-based sentence transformer, 
and Word Mover Distance \citep{gohsen_captions_2023}.
The Word Mover Distance computes the minimum amount of distance that embedded words of a text need to travel 
to reach the embedded words of another text \citep{gohsen_captions_2023}.
\citet{gohsen_captions_2023} normalized all metrics and averaged the semantic and syntactic scores separately.

\bluert{} is machine evaluation metric for paraphrase generation.
\citet{fu_learning_2024} use \bluert{} to filter out incorrect paraphrases (i.e. using a threshold $\theta$).

BLEU (Bilingual Evaluation Understudy \citep{palivela_optimization_2021,zhou_paraphrase_2025,papineni_bleu_2001}) (2002) 
was developed for machine translation \citep{zhou_paraphrase_2021,papineni_bleu_2001}.
It can take values from 0 to 1 \citep{papineni_bleu_2001}.
BLEU is a precision measure \citep{kurt_pehlivanoglu_comparative_2024,papineni_bleu_2001}.
It counts the matching n-grams (unigrams) in the generated/candidate text that appear in any of the gold/ reference texts \citep{palivela_optimization_2021,papineni_bleu_2001}, 
and then divides them by the total number of n-grams (unigrams) in the candidate text \citep{papineni_bleu_2001}.
Since candidates consisting only of high-probability n-grams (e.g. "the") would receive a high score without deserving it, 
\citet{papineni_bleu_2001} introduced a clipping mechanism to limit the count of n-grams in the candidate text to the maximum count of that n-gram in any of the reference texts.
The clipped n-grams occurences are added up and divided by the total number of unclipped n-grams in the candidate text \citep{papineni_bleu_2001}.
\citet{papineni_bleu_2001} state that unigrams are used to test adequacy, while longer n-grams are used to test fluency.
BLEU's basic unit of evaluation is a sentence. 
In order to compute the BLEU score from \autoref{eq:notes_bleu} for more than one sentence, one (1) computes the n-grams matches sentence by sentence, 
then (2) add the clipped n-grams matches across all sentences, 
and finally (3) divides the total clipped n-grams matches by the total number of unclipped n-grams in all candidate sentences \citep{papineni_bleu_2001}.
\begin{equation}
    p_n = \frac{\sum_{\mathcal{C} \in \left\{ Candidates \right\}}\sum_{n-gram \in\mathcal{C}}Count_{clip}(n-gram)}{\sum_{\mathcal{C'} \in \left\{ Candidates \right\}}\sum_{n-gram' \in\mathcal{C'}}Count(n-gram')}
\label{eq:notes_bleu}
\end{equation}
BLEU combines the scores for different n-grams (separately computed) using the average logarithm with uniform weights, 
which is equivalent to using the geometric mean of the scores \citep{papineni_bleu_2001,banerjee_METEOR_2005}.
\citet{gohsen_captions_2023} use up to 4-grams.
BLEU automatically penalizes n-grams appearing in the candidate text but not in the reference text, as well as n-grams appearing more often in the candidate than in the reference text \citep{papineni_bleu_2001}.
According to \citet{papineni_bleu_2001}, they need to add a brevity penalty to the BLEU score to enforce proper length of the candidate text. 
For multiple sentences, they (1) add the best match (among the reference texts) length for each candidate sentence, and (2) divide this sum $r$ by the total length of all candidate sentences $c$. 
Hence, the brevity penalty $BP$ is defined as follows in \autoref{eq:notes_bleu_brevity_penalty}:
\begin{equation}
    BP = \begin{cases}
        1 & \text{if } c > r \\
        e^{1 - \frac{r}{c}} & \text{if } c \leq r
    \end{cases}
\label{eq:notes_bleu_brevity_penalty}
\end{equation}
Combining all these, the final BLEU score is computed as follows in \autoref{eq:notes_bleu_final}:
\begin{equation}
    \text{BLEU} = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \cdot \log p_n\right)
\label{eq:notes_bleu_final}
\end{equation}
They cannot use recall for length-related problems here, because BLEU uses multiple reference texts, which may have different lengths \citep{papineni_bleu_2001,banerjee_METEOR_2005}.
If the generated candidate is significantly shorter than the reference text, the brevity penalty $BP$ is applied.
A BLEU score approaching 1 signifies the candidate matches one reference almost exactly \citep{papineni_bleu_2001}, 
and thus, limited syntactic diversity (i.e. inadequate paraphrase) \citep{kurt_pehlivanoglu_comparative_2024}.
Note that more reference texts lead to higher BLEU scores \citep{papineni_bleu_2001}.
Unigrams are token-wise and bi-grams are word-pairs \citet{palivela_optimization_2021}.
According to \citet{zhou_paraphrase_2021}'s survey, BLEU is the most frequently used metric for paraphrase generation.
BLEU is unable to measure semantic equivalents \citep{kurt_pehlivanoglu_comparative_2024,zhou_paraphrase_2021} 
when applied to low-resource languages \citep{zhou_paraphrase_2021}.
Moreover, BLEU fails to capture good paraphrases that are not similar to the reference text \citep{zhou_paraphrase_2021}.
\citet{kurt_pehlivanoglu_comparative_2024} found that BLEU tends to overestimate the quality of paraphrases.
\citet{zhou_paraphrase_2021} suggest combining BLEU with human evaluation to overcome its limitations.

GLEU (Google-BLEU) (ranges from 0 to 1 \citep{kurt_pehlivanoglu_comparative_2024}) is a variant of BLEU that was developed to be closer to human judgement, and to 
overcome BLEU's drawback of per sentence reward objective \citep{palivela_optimization_2021}.
GLEU computes n-gram precisions (overlaps \citep{kurt_pehlivanoglu_comparative_2024}) overgold/reference paraphrases 
and weighs n-grams by their change from the source text \citep{palivela_optimization_2021}.
GLEU assesses the fluency, order of n-grams, structural and semantic accuracy 
and penalizes shorter average m-gram lengths in the generated text compared to the reference \citep{kurt_pehlivanoglu_comparative_2024}.
Lower GLEU scores indicate greater diversity \citep{kurt_pehlivanoglu_comparative_2024}.

METEOR (Metric for Evaluation of Translation with Explicit Ordering \citep{palivela_optimization_2021,banerjee_METEOR_2005}) 
(ranges from 0 to 1 \citep{kurt_pehlivanoglu_comparative_2024}) (2014) aims to address BLEU's shortcomings.
First a mapping, so-called aligment, between the unigrams in the candidate text and the reference texts is created \citep{banerjee_METEOR_2005}.
Each unigram has zero or one match.
This aligment is created incrementally in repeating two steps:
(1) List all possible unigram mappings derived from different modules (i.e. exact matches, Porter stemmed matches, synonym matches), 
and (2) select the largest subset of unigram mappings that constitute a valid alignment (matches obtained from different modules are treated the same).
(3) Choose the subset with the largest cardinality and if there are multiple, choose the one with the fewest unigram mapping crosses \citep{banerjee_METEOR_2005}.
METEOR computes a weighted F-score 
(unigram-precision, unigram-recall \citep{kurt_pehlivanoglu_comparative_2024,banerjee_METEOR_2005} 
and a measure of fragmentation \citep{banerjee_METEOR_2005,kurt_pehlivanoglu_comparative_2024})
with a penality function whenever an incorrect word is encountered \citep{palivela_optimization_2021} as displayed in \autoref{eq:notes_meteor}.
\begin{equation}
    METEOR = F_{mean} = \frac{10 \cdot P \cdot R}{R + 9P} \cdot (1 - Penalty)
\label{eq:notes_meteor}
\end{equation}
The penality is designed to reduce the $F_{mean}$ score to $50\%$ if there are no bigram or longer matches \citep{banerjee_METEOR_2005}.
It has better correlation with human judgement at the sentence/segment level than BLEU \citep{zhou_paraphrase_2021}, 
because it not only consists of simple n-gram matching but also including synonymy and stemming \citep{kurt_pehlivanoglu_comparative_2024}.

ROUGE (Recall-Oriented Understudy for Gisting Evaluation \citep{palivela_optimization_2021,lin_rouge_2004}) 
(ranges from 0 to 1 \citep{kurt_pehlivanoglu_comparative_2024}) (2004) 
is a recall-based metric developed for text summarization \citep{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024,lin_rouge_2004}.
ROUGE can focus on the word variations and diversity.
It has multiple versions, the most popular ones include 
ROUGE-N (computing the n-gram recall) \citep{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}, 
ROUGE-L (computing the longest common subsequence) \citep{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}, 
ROUGE-W (Weighted longest common subsequence) \citep{palivela_optimization_2021}, 
ROUGE-S (skip-bigram co-occurrence statistics) \citep{palivela_optimization_2021}.
ROUGE-1 computes the recall by analysing the matching unigrams between the generated paraphrase and the reference paraphrase \citep{palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}.
% ROUGE-N
ROUGE-N is an n-gram recall between the candidate text and the reference texts \citep{lin_rouge_2004} as displayed in \autoref{eq:notes_rouge_n}.
\begin{equation}
    ROUGE-N = \frac{\sum_{\mathcal{S} \in \left\{ References \right\}}\sum_{n-gram \in\mathcal{S}}Count_{match}(n-gram)}{\sum_{\mathcal{S'} \in \left\{ References \right\}}\sum_{n-gram' \in\mathcal{S'}}Count(n-gram')}
\label{eq:notes_rouge_n}
\end{equation}
$Count_{match}(n-gram)$ is the maximum number of n-grams co-occuring in the candidate text and the set of reference texts \citep{lin_rouge_2004}.
The nominator sums over all references and thus, gives more weight to matching n-grams that occur in multiple references (i.e. a consensus between references) \citep{lin_rouge_2004}.
Refer to \citet{lin_rouge_2004} for more details on the work with multiple references (I do not understand that, because I thought we already use multiple).
% ROUGE-L
For ROUGE-L, the intuition is that the longer the longest common subsequence (LCS) between the candidate and reference texts, the more similar they are \citep{lin_rouge_2004}.
For a candidate $Y$ of length $n$ and a reference $X$ of length $m$, the ROUGE-L score is defined as follows in \autoref{eq:notes_rouge_l}:
\begin{equation}
    ROUGE-L = F_{lcs} = \frac{(1 + \beta^2)R_{lcs}P_{lcs}}{R_{lcs} + \beta^2 P_{lcs}}
\label{eq:notes_rouge_l}
\end{equation}
where $R_{lcs} = \frac{LCS(X,Y)}{m}$ and $P_{lcs} = \frac{LCS(X,Y)}{n}$ \citep{lin_rouge_2004}.
ROUGE-L requires in-sequence matches that reflect the sentence level word order as n-grams \citep{lin_rouge_2004}.
Moreover, no predefined $n$ is necessary, because ROUGE-L includes the longest in-sequence common n-grams \citep{lin_rouge_2004}.
However, ROUGE-L does not include shorter sequences or alternative LCSes in the final score \citep{lin_rouge_2004}.
% ROUGE-S
A skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps \citep{lin_rouge_2004}.
ROUGE-S measures the overlap of skip-bigrams between the candidate text and the reference texts \citep{lin_rouge_2004}.
Hence, if the candidate text is the reverse of the reference text, the ROUGE-S score is 0 even though it is not as bad as completely unrelated candidates \citep{lin_rouge_2004}.
ROUGE-SU extends ROUGE-S with unigrams to solve this issue \citep{lin_rouge_2004}.
% ROUGE generally
\citet{kurt_pehlivanoglu_comparative_2024} claim that ROUGE may not be adequate to assess semantic similarity and fluency.
Lower ROUGE scores indicate greater diversity \citep{kurt_pehlivanoglu_comparative_2024}.

TER (2006) was developed for machine translation \citep{zhou_paraphrase_2021}.
It computes the number of edits required to change the translation until it matches the reference translation.
It ranges from 0 (i.e. no edits needed) to 1 (i.e. all words need to be changed).

\citet{fu_learning_2024} describe the Gini Coefficient as a measure of inequality in a distribution, ranging from 0 (i.e. even distribution across categories) to 1.

\citet{master_thesis_paraphrasing_2024} include a table (tab. 3.1, pg. 18) with numerous stylometric metric including readbility, vocabulary richness, and word/character counts.

\citet{palivela_optimization_2021} state that accuracy, precision, recall and F1-score are suitable for \ac{pi}.
For \ac{pg}, they suggest ROUGE, BLEU, GLEU, WER (Word Error Rate), and METEOR as suitable metrics.
WER is the number of substitutions (replacements of words), insertions (adding words) and deletions (removing words) 
divided by the total number of words in the reference text \citep{palivela_optimization_2021}.

\citet{kurt_pehlivanoglu_comparative_2024} additionally use T5-STSB.
The metric is based on \ac{t5} model adapted to the Semantic Textual Similarity Benchmark (STSB).
It evaluates semantic equivalence by assigning a similarity score from 0 (no similarity) to 5 (complete equivalence) \citep{kurt_pehlivanoglu_comparative_2024}.

BERTScore calculates the cosine similarity between the contextual embeddings of the reference and generated texts. 
Hence, is assesses semantic equivalence and correlates well with human judgement \citep{kurt_pehlivanoglu_comparative_2024}.
First, token vector representations are computed for both the reference and generated texts using a pre-trained BERT model \citep{hanna_fine_grained_2021}.
Let reference $z$ and candidate $\hat{z}$ be the vector representations of the reference and candidate texts, respectively.
Then, the BERTScore precision, recall and $F_1$ score is computed as follows in \autoref{eq:notes_bert_p}, \autoref{eq:notes_bert_r}, and \autoref{eq:notes_bert_f1}, respectively:
\begin{equation}
    P_{BERT} = \frac{1}{|\hat{z}|} \sum_{\hat{z}_j \in \hat{z}} \max_{z_j \in z} z_i\top \hat{z}_j
\label{eq:notes_bert_p}
\end{equation}
\begin{equation}
    R_{BERT} = \frac{1}{|z|} \sum_{z_j \in z} \max_{\hat{z}_j \in \hat{z}} z_i\top \hat{z}_j
\label{eq:notes_bert_r}
\end{equation}
\begin{equation}
    F_1 = \frac{2 P_{BERT} R_{BERT}}{P_{BERT} + R_{BERT}} 
\label{eq:notes_bert_f1}
\end{equation}
Since $F_1 \in \left[-1,1\right]$ it can be rescaled to $[0,1]$ by modifying the precision and recall calculation 
to $\hat{P}_{BERT} = \frac{P_{BERT} - a}{1 - a}$ ($R_{BERT}$ analoguous), where $a$ is the empirical lower bound on the BERTScore \citep{hanna_fine_grained_2021}.
The BERTScore has difficulties on datasets with lexically similar (i.e. lexical overlap of content words) incorrect candidates 
opposed to lexically different more correct candidates \citep{hanna_fine_grained_2021}.


\ac{t5}-CoLA metric (ranges from 0 to 5 \citep{kurt_pehlivanoglu_comparative_2024}) utilizes the Corpus of Linguistic Acceptability (CoLA) to evaluate the grammatical correctness of sentences and thus, 
contributes linguistic evaluation \citep{kurt_pehlivanoglu_comparative_2024}.

Generally, \citet{kurt_pehlivanoglu_comparative_2024} order the metrics by their contribution area:
\begin{itemize}
    \item semantic: BERTScore, STSB, METEOR
    \item Fluency: CoLA
    \item Diversity: ROUGE1/2/L, BLEU, GLEU
\end{itemize}

\citet{krishna_paraphrasing_2023} compute the lexical diversity using unigram token overlap and call it F1 score.
As a semantic similarity score, they use the ACL Antology 2022 published \href{https://aclanthology.org/2022.emnlp-demos.38.pdf}{P-SP}.

According to \citet{gohsen_task_oriented_2024}, there are two perspectives to paraphrasing: 
Lexical (i.e. changes at word level) and syntactic (i.e. changes at syntactic level).
Paraphrase types can be classified into surface and semantic level. Finer levels are outlined in \citep{gohsen_task_oriented_2024}.

\textcolor{red}{Do we want semantically similar paraphrases even though our task is style transfer belonging to semantically equivalent paraphrasing?}

Popular paraphrase categories include \citep{fu_learning_2024}:
\begin{itemize}
    \item Top-level classification perspective: 
        \begin{itemize}
            \item Lexicon-based changes
            \item Morphology-based changes
            \item others
        \end{itemize}
    \item Second-level classification perspective:
        \begin{itemize}
            \item Change of format
            \item Semantic-based
            \item Change of order
        \end{itemize}
\end{itemize}
\citet{zhou_paraphrase_2025} (pg. 3, tab.1, and examples afterwards) define a topology of paraphrase types:
\begin{itemize}
    \item Morphology based: inflection changes (e.g. singular to plural), derivation changes (e.g. adjective to verb), functional word substitution (e.g. this to that).
    \item Lexicon based: Same polarity substitution (e.g. synonym), opposite polarity substitution (e.g. antonym), converse substitution (e.g. opposite view point), spelling changes, synthetic/ analytic substitution, relational substitution
    \item Syntax based: Negation switching (i.e. other negation), diathesis alternation (e.g. change position of verb), etc.
    \item Discourse based: Indirect/direct substitutions, sentence modality changes, punctuation changes, etc.
    \item Other changes: Change of order, change of format, etc.
\end{itemize}

\citet{fu_learning_2024} give three prompt tips:
\begin{enumerate}
    \item Make question/ prompt as clear as possible even if some restrictive requirements may seem unnecessary.
    \item Place significant details, including restrictive elements such as time, place, manner, reason, purpose and conditions, at the beginning of the prompt.
    \item Pay attention to spelling, i.e. proper nouns, title, honorifics, abbreviations, acronyms and observe capitalization.
\end{enumerate}

\citet{zhou_paraphrase_2021} claim it is difficult to control the style of generated paraphrases.

Another approach to paraphrasing is back-translation, which may limit diversity \citep{zhou_paraphrase_2025}.


\subsection{\ac{llm} training}

\citet{master_thesis_paraphrasing_2024} describes the training of \acp{llm} as a three-stage development process:
Starting with pre-training on a vast dataset to acquire comprehensive knowledge of language and the world, 
followed by supervised finetuning for specific downstream tasks and finally,
human preference alignment training to mimic human behaviour.

\subsection{Preprocessing of Dataset}

\citet{palivela_optimization_2021} propose the following transformation steps for the dataset:
\begin{enumerate}
    \item Remove sentence pairs having more than 60$\%$ unigram, bigram or trigram overlap 
    (to discourage model from copying input sentence and increases diversity of paraphrases).
    \item Remove sentence pairs having very little semantic similarity by using Sentence-BERT 
    (to force model to generate semantically similar paraphrases).
    \item If dataset contains non-paraphrase sentence pairs, remove them.   
\end{enumerate}

\citet{gohsen_captions_2023} remove exact or near-duplicates from their paraphrase dataset.
Near-duplicates differ only in punctuation, capitalization or whitespaces \citep{gohsen_captions_2023}.

\subsection{Plagiarism Detection}
\citet{hassanipour_ability_2024} analyse texts for plagiarism using the \textcolor{orange}{iThenticate} tool.
It is really \href{https://www.ithenticate.com/pricing}{expensive}.
Alternatives to detectors of \ac{ai}-generated text include watermarking and statistical outlier detection 
(\ac{ai}-generated text's artefacts such as irregularities in entropy and perplexity) \citep{krishna_paraphrasing_2023}.
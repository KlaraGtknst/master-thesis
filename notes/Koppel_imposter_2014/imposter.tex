\section{Imposter method}
\label{sec:imposter_method}

The problem to solve is the \ac{av} problem:
For two documents $X, Y$ determine if they were written by the same author.
As displayed in \autoref{fig:problem_reduction}, \citet{koppel_determining_2014} propose reducing the \ac{av} problem to the many-candidates problem.

\begin{figure}[htbp]
    \centering
    \includesvg{notes/Koppel_imposter_2014/reduction_closed_set_AV_to_open_set_AA}
    \caption{Reducing the \ac{av} problem to the many-candidates problem.}
    \label{fig:problem_reduction}
\end{figure}

This method produces a set of \textit{imposter} documents.
Then, \citet{koppel_determining_2014} determine whether $X$ is sufficiently more similar to $Y$ than to the imposter documents.
There are multiple important settings:
\begin{itemize}
    \item Proper methods to select the imposter documents.
    \item Proper methods to measure the similarity between documents.
\end{itemize}
Similar to unmasking, \citet{koppel_determining_2014} repeatedly select random subsets of features that serve as the basis for comparing documents.
If a documents $Y$ is more similar to document $X$ than any other document for many feature subsets, 
it is likely that $X$ and $Y$ are by the same author.
% FIXME: autoref doesn't know lst 
The algorithmic approach is displayed in \autoref{alg:imposter_algo}.
\citet{koppel_determining_2014} state that k=100 iterations are sufficient.
The threshold $\sigma^*$ varies the recall-precision tradeoff.
\citet{koppel_determining_2014} claim that the imposter method obtains strong results even for documents with no more than 500 words.
Moreover, they highlight the similarity between the imposter method and an ensemble of classifiers learning different subset of features.
% TODO: two paragraphs below conflicting? cf. koppel_determining_2014 pg. 181, 182
Furthermore, the performance of the method improves as the number of candidate author diminishes.
In an open-set scenario, fewer candidate authors makes the problem more difficult for the imposter method 
since one author being consistently more similar to the text than the others candidates across multiple feature subsets is less likely in large sets of competing candidate authors.
A greater number of imposters reduces the number of \acp{fp} and \acp{fn}.
% cf. koppel_determining_2014 pg. 182, but conflicting with above:
% Small sets of candidate authors are more likely to generate \acp{fp} 
% while large sets may generate \acp{fn} and thus, creating a tradeoff.

\begin{algorithm}
    \caption{Imposters Method for Author Verification}
    \label{alg:imposter_algo}
    \begin{algorithmic}[1]
    \Procedure{IsSameAuthor}{$X$, $Y$, $\sigma^*$}
        \Comment{$X$, $Y$: input documents; $\sigma^*$: decision threshold}
    
        \State Initialize $scores = \{\}$ 
    
        \ForAll{$d \in \{X, Y\}$}  
            \State $d' \gets \{X, Y\} \setminus \{d\}$ 
            \Comment{Disputed document $d'$}
            \State $\mathcal{I}_d \gets \text{GenerateImposters}(d)$ 
            \Comment{$m$ imposter documents for candidate $d$}
    
            \State $scores[d] \gets 0$ 
    
            \For{$i = 1$ to $100$} 
                \Comment{100 random feature subsets}
                \State $F_i \gets \text{RandomSubset}(\text{Features})$ 
                \State $S \gets \{ \text{Similarity}(d', I_j \mid F_i) : I_j \in \mathcal{I}_d \cup \{d\} \}$ 
                \State $j^* \gets \arg\max_j S_j$ 
                \If{$I_{j^*} = d$} \Comment{Count times $d=$ top match}
                    \State $scores[d] \gets scores[d] + 1$ 

                \EndIf
            \EndFor
        \EndFor
    
        \State \Return $\left( \frac{scores[X] + scores[Y]}{2} > \sigma^* \right)$ 
        \Comment{Return \textbf{True} if average $> \sigma^*$}
    \EndProcedure
    \end{algorithmic}
    \end{algorithm}
    
% Tradeoff/ Risks
If we want to generate imposters for a document $Y$ and compare it to a document $X$, 
the imposters should be similar (e.g., in terms of genre) to $Y$ rather than $X$.
Otherwise, the $Y$ would never be the top match and thus, producing \acp{fn}.
Unconvincing imposter documents would lead to \acp{fp} due to the lack of similarity between $X$ and the imposter documents making $Y$ consistently the top match.

% Choice of the imposter documents 
First the $m$ most similar imposter (in terms of min-max similarity) documents are selected.
Then, $n$ random imposter documents are selected from the $m$ imposter documents.
\citet{koppel_determining_2014} found that using a selection of $n$ imposters rather than all $m$ imposter documents produces better results.
The approach is insensitive to $m,n$.
\citet{koppel_determining_2014} propose three options of generating the imposter documents.
\begin{itemize}
    \item Fixed
    \item On-the-fly
    \item Blogs
\end{itemize}

Fixed imposter documents can be aggregated results of random (English) Google queries.
They have not special relation to the document pair in question.

% sicher, non-reference???
On-the-fly imposter documents are generated by randomly selecting medium-frequency words from the document $Y$ (i.e. the \textcolor{red}{non-reference document} in \autoref{alg:imposter_algo}).
The words are then used to query Google and aggregate the top results of the respective queries. 
Hence, the imposter documents are similar to the document $Y$ in terms of content.

When using blogs as imposter documents, the imposter documents are selected from other bloggers. 
Hence, the imposter documents are similar to both documents $X,Y$ in terms of assuming $X,Y$ share a genre.
According to \citet{koppel_determining_2014}, this method produces the best results.

\citet{koppel_determining_2014} claim that similar imposters to the document $Y$ reduce the number of imposter documents needed to achieve a certain level of performance.
They also state that the search engine is used if no information about the input documents is available.
\section{Prior work by feature extraction}
\label{sec:prior_work_by_feat_extraction}

\citet{tyo_state_2022} provide a comprehensive overview over prior work in the realm of \ac{aa} and \ac{av} methods.
They organize their survey by the feature extraction methods used in the respective work.
\todo{Add visualization similar to Fig.1 in \cite{tyo_state_2022}}

% 1. feature based
% 1.1. n-grams
In the context of feature based features the present ngrams.
\input{notes/state_of_art_AA_AV_2022/ngram_representation}
For \ac{aa}, \citet{tyo_state_2022} claim, that ngrams are state-of-the-art if there are less than 50000 words per author.
% in this context: unmasking & Imposter method, LDA, sentence syntax trees

% 1.2. summary statistics
Summary statistics, including distribution of word lengths, \todo{hapax-legomena, Maas' $a^2$, Herdan's $V_m$}, 
can be used for \ac{av} tasks: 
Calculate the difference of these feature vectors between to texts and train a logistic regression classifier 
to predict whether the texts were written by the same author.
\citet{tyo_state_2022} claim that this performs well despite its simplicity.

% 1.3. Co-occurrance graphs: cf. tyo_state_2022 pg. 5

% 2. embedding based
Another category of feature extraction methods are embedding based methods.
% 2.1. char embeddings
\citet{tyo_state_2022} first present character embeddings.
Starting with RNN models shared across multiple authors with individual heads, 
further presenting CNNs and % CNN for AA: char better than word embeddings.
compression-based methods such as ZIP, RAR (i.e. a variant of \ac{ppm}), \ac{ppm}, 
to build representations comparable via distance metrics.
For \ac{aa}, \citet{tyo_state_2022} claim, that \ac{ppm} is a low performer and scales poorly to large datasets.
\citet{bevendorff_divergence_based_2020} quote someone else, who recommend compression-based cosine (CBC) calculated on text pairs after compression with the PPMD algorithm.
They explain that natural language allows very good for compression ratios due to its predictability (English has an entropy of at most 1.75 bits per character).
PPMD uses finite-order Markov language models for compression, which work well for predicting characters in a sentence but are sensitive to increased entropy stemming from obfuscation.
% sind sie dann noch geeignet? Wenn sie direkt altered Texte erkennen, w√§re das doof.

% 2.2. word embeddings
In the context of word embeddings, \citet{tyo_state_2022} present the following methods:
\begin{itemize}
    \item Fasttext word embeddings concatenated with CNN embeddings as input to (Siamese) nested BiLSTMs trained with contrastive loss
    \item GloVe-like embeddings for sentences via parse-tree as input to Siamese BiLSTMs trained with contrastive loss
    \item POS tages along with word embeddings
\end{itemize}

% 2.3 Transformers
\citet{tyo_state_2022} also present research in diverse language model architectures, including RNN, BERT and GPT2.
For \ac{aa}, \citet{tyo_state_2022} claim, that BERT is state-of-the-art method if the dataset contains over 100000 words per author.

% 3. combination
\citet{tyo_state_2022} state that some researches combine transformers with summary statistics.

\section{Attribution Methods}
\label{sec:attribution_methods}

For a comparison (pro/contra) of profile-based and instance-based methods, see \citet{stamatatos_survey_2009}.

% profile-based
Methods belonging to the profile-based category concatenate all the available training texts per author in one big file 
and extract a cumulative representation of that author's style from this concatenated text. % tyo_state_2022: "representation of all author texts"
Hence, methods in this category are better if only short texts are available for training.
The difference of texts written by the same author are disregarded \cite{stamatatos_survey_2009}.
The unseen text is compared to each author file and the most similar one based on a distance measure is selected as the predicted author:
$$ author(x) = argmin_{a \in A} d(x, x_a) $$
where $x$ is the text to be classified, $A$ is the set of authors, and $d(x, x_a)$ is a distance measure 
between the text and the author file $x_a$ \cite{stamatatos_survey_2009}.
% probabilistic models
Probabilistic models are a special case of profile-based methods.
They attempt to maximize the probability $P(x|a)$ of the text $x$ belonging to candidate author $a$.
The attribution model seeks the author that maximizes the similarity metric: 
$$ author(x) = argmax_{a \in A} \frac{P(x|a)}{P(x|\overline{a})} $$
where the conditional probabilities are estimated by $x_a$ for author $a$ and the rest of the texts, respectively \cite{stamatatos_survey_2009}.
Naive Bayes is a variant of this probabilistic  classifier.
% compression models, e.g. RAR or GZIP (more info Paper)
Compression models are based on the idea that the text of one author can be compressed more efficiently than the text of multiple authors.
The new text is concatenated with the author profile and then compressed.
The differences between the compressed concatenation with the unseen text and compressed author profiles without the unseen text are computed.
The author profile with the lowest difference is selected as the predicted author.
\citet{stamatatos_survey_2009} claim that probabilistic approaches are faster in comparison to compression models.
% Common n-grams
For the \ac{cng} method, the author profile is composed of the $L$ most common $n$-grams of the training texts.
The similarity between to texts is estimated by a distance measure based on relative frequencies of the $n$-grams.
\ac{cng} favours author profiles shorter than $L$ or in imbalanced cases.
% Simplified Profile Intersection
\ac{spi} is a simpler distance measure to mitigate the disadvantages of \ac{cng}.
It is based on the idea that the more common $n$-grams two texts share, the more similar they are.
It counts the number of common $n$-grams between the two texts and disregards the rest.
% most similar in terms of distance
\cite{koppel_authorship_2011} describe the similarity-based paradigm as a profile-based approach 
where the unseen text is attributed to the author whose profile is closest in terms of a distance metric.
As a distance metric, \citet{koppel_authorship_2011} suggest the cosine distance in a vector space 
defined by the space-free character 4-gram frequencies.

% instance-based
The family of instance-based methods, on the other hand, require multiple training text samples per author. 
Each sample is a separate instance of authorial style \cite{stamatatos_survey_2009,altakrori_topic_2021}.
If only one training sample is available, the method segments the sample into multiple parts, probably of equal length.
\citet{stamatatos_survey_2009} state that samples of variable length should be normalized and 
shorter samples should be discarded.
% vector space models
Each text is represented as a vector in a multivariate space.
\citet{stamatatos_survey_2009} list a number of statistical and machine learning algorithms as classifiers.
They stress that \acp{svm} are extremely popular in high-dimensional spaces.
However, they also state that class imbalance is a problem 
which should be overcome by segmentation, filtering or oversampling.
\citet{koppel_authorship_2011,koppel_determining_2014} denote this approach belonging to the machine learning paradigm.
\citet{koppel_determining_2014} claim that machine learning methods are not suitable for large number of candidate authors 
since they are designed for small number of classes. 
They also state that the introduction of ensembles of multiple binary classifiers is not a solution to this problem 
due to the ambiguity of multiple positive answers.
% similarity-based measures
Similarity-based measures are used to measure the distance between the unseen text and all other training texts.
The most likely author is estimated based on a $k$-nearest-neighbour algorithm.
If $k=1$, the approaches are sensitive to noise.
However, for $k>1$ and majority vote or weighted vote schemes, the methods are more robust.
% others in Paper
Compression-based models can also be considered similarity-based measures which are slow 
since the compression algorithm is called for each training text.
% Meta-learning models
Existing classification algorithms can be used as meta-learning models.
Unmasking is a meta-learning approach which is based on the idea that
omitting discriminant features and the consequent drop in accuracy of the classifier 
can be used for inference of the author of the unseen text.
For each unseen text, a \ac{svm} is built to discriminate it, i.e. its segments, 
from the training texts of each candidate author.
Hence, for each candidate author, a \ac{svm} is trained.
After a few iterations, the classifier is no longer able to discriminate between the unseen text and 
the training texts of the true author, i.e. low accuracy.


% hybrid
Hybrid approaches include a combination of profile- and instance based aspects.
Text samples are represented individually (i.e. instance-based) and 
the profile vector is built via computing the feature-wise average over the author's sample vectors.
The similarity between the unseen text and the author profile is used to predict the true author.
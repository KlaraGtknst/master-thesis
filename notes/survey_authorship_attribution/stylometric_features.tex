\section{Stylometric features}
\label{sec:stylometric_features}

Refer to \autoref{sec:definitions} for the definitions of the terms used in this section.

\begin{table}[]
    \centering
    \caption{Types of stylometric features with their computation tools and 
    resources where brackets indicate optional tools from \cite{stamatatos_survey_2009}.}
    \label{tabstylometric_features_tools}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|l|l|}
    \hline
    \rowcolor[HTML]{EFEFEF} 
    \textbf{Category} & \textbf{Features} & \textbf{Required tools and resources} \\ \hline
    Lexical & Token-based (word/ sentence length, ...) & Tokenizer, (Sentence splitter) \\
     & Vocabulary richness & Tokenizer \\
     & Word frequencies & Tokenizer, (Stemmer, Lemmatizer) \\
     & Word n-grams & Tokenizer \\
     & Errors & Tokenizer, Orthographic spell checker \\
    Character & Character types (letters, digits, ...) & Character dictionary \\
     & Character n-grams (fixed length) & - \\
     & Character n-grams (variable length) & Feature selector \\
     & Compression methods & Text compression tool \\
    Syntactic & Part-of-Speech (POS) & Tokenizer, Sentence splitter, POS tagger \\
     & Chunks & Tokenizer, Sentence splitter, (POS tagger), Text chunker \\
     & Sentence and phrase structure & Tokenizer, Sentence splitter, POS tagger, Text chunker, Partial parser \\
     & Rewrite rule frequencies & Tokenizer, Sentence splitter, POS tagger, Text chunker, Full parser \\
     & Errors & Tokenizer, Sentence splitter, Syntactic spell checker \\
    Semantic & Synonyms & Tokenizer, (POS tagger), Thesaurus \\
     & Semantic dependencies & Tokenizer, Sentence splitter, POS tagger, Text chunker, Partial parser, Semantic Parser \\
     & Functional & Tokenizer, Sentence splitter, POS tagger, Specialized dictionaries \\
    Application-specific & Structural & HTML parser, Specialized parsers \\
     & Content-specific & Tokenizer, (Stemmer, Lemmatizer), Specialized dictionaries \\
     & Language-specific & Tokenizer, (Stemmer, Lemmatizer), Specialized dictionaries
    \end{tabular}%
    }
\end{table}

% all information from stamatatos_survey_2009
% lexical features
Lexical features consider sentences grouped sequences of tokens, i.e. words, numbers or punctuation marks.
% length counts
Simple attempts of author attribution include sentence lengths counts, and word length counts.
These approaches work straightforward assuming that the tokenizer is able to identify the tokens correctly.
However, for some language such as Chinese, the tokenization is not trivial.
% vocabulary richness
Vocabulary richness functions attempt to quantify the diversity of vocabulary used in a text.
Approaches include $\frac{\text{size of vocabulary}}{\text{total number of tokens}}$ or the 
number of hapax legomena (words that occur only once in a text).
The vocabulary size depends on the text length (quick increase in the beginning, slowly increasing later).
To mitigate this effect, researchers have proposed methods to introduce stability into these metrics. 
According to \citet{stamatatos_survey_2009}, these proposed methods are questionable and should not be used alone.
% BoW
Texts can also be represented by vectors of word frequencies, i.e. the \ac{bow} model.
This representation disregards contextual, i.e., word-order, information.
While topic-based classification usually excludes so-called function words, 
i.e. most common words, since they do not carry any semantic information and are topic-independent, 
style-based text classification includes them.
In particular, function words are considered among the best features to discriminate between authors 
since they capture pure stylistic choices of authors across different topics.
Hence, style-based text classification requires much lower dimensionality (i.e., a few hundred words) 
in comparison to topic-based text classification (i.e., several thousand words).
Function words can be closed class words, i.e., articles, prepositions etc., 
or open-class words, i.e., nouns, adjectives, verbs.
Closed class words are the first dozen frequent words and 
open class words are predominantly present in the next frequent words.
% n-grams
To overcome the lack of contextual information in the \ac{bow} model, 
n-grams can be used.
However, the classification accuracy does not always increase with the usage of n-grams instead of individual word features.
Moreover, the dimensionality of the problem increases, representations become sparse and possibly content-specific information rather than stylistic information is captured.
% Errors
Similar to manual human attribution, using error measures can be used to identify the author of a text.
Spelling (i.e., letter omissions, insertions) and formatting errors (i.e., all caps words) can be used as features 
to capture the idiosyncrasies of an author's style.
Unfortunately, according to \cite{stamatatos_survey_2009}, the availability of accurate spell checkers for many natural languages is problematic.


% character features
Character feature consider texts a sequence of characters.
Simple approaches include alphabetic character count, digit character count, uppercase/ lowercase character count, letter frequencies and punctuation marks count.
\citet{stamatatos_survey_2009} claim these features are available for any natural language and prove useful.
% n-grams
They also mention frequencies of $n$-grams on character level.
This approach is robust to noise, captures lexical and contextual information, grammatical errors, use of punctuation and capitalization.
It performs better on oriental languages than token-based approaches.
However, the dimensionality of the problem increases with $n$-grams compared to tokens.
Moreover, the choice of $n$ is language-dependent since certain natural language (e.g., German) tend to have longer words than others (e.g., English).
Large $n$ on the one hand, capture lexical and contextual information but may also capture content-specific information and increases the dimensionality of the representation.
Small $n$ on the other hand, may be able to represent subwords but are less adequate to capture contextual information.
% compression-based features
The idea of compression based approaches is to use the compression model acquired from one text to compress another text. 
If both texts are from the same author, the bit-wise size of the compressed file will be relatively low.
Since compression models usually describe characteristics of the texts based on the repetition of character sequences, 
they can be considered as character-based features. \todo{Is this still the case? Paper is from 2009}


% syntactic features
Assuming that author's have a syntactic fingerprint, these approaches are considered more reliable than mere lexical information.
Unfortunately, syntactic features are language-dependent and require \ac{nlp} tools, for instance, a parser.
% Parser
Once the text has been parsed, metrics such as noun phrase counts and verb phrase counts can be computed.
% POS
Another approach is to use \ac{pos} tags on word-token or $n$-gram for frequencies.
% errors
If spell checkers are available and of sufficient quality, syntactic errors can be used as features.
% etc.


% semantic features
Since high-level tasks are prone to errors and noise, semantic analysis is not as widely used as syntactic analysis.
\todo{Is the quality still too bad? Paper is from 2009}


% application-specific features
Based on the applications, structural measures include the use of greetings, farewells, types of signatures, indentation, paragraph length 
or HTML specific metrics.
Usually, in the context of stylometry, content specific features are not used.
If all documents belong to the same thematic area, content-based information may reveal some authorial choices. 
One can compare content-specific word frequencies, even though it remains unclear how to select such words for a given text domain.


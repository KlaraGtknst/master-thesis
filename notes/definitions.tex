\section{Definitions}
\label{sec:definitions}


% \begin{definition}
%     []
% \end{definition}

\begin{definition}
    [Text]
    A sequence of tokens or characters grouped into sentences \cite{elmanarelbouanani_authorship_2014}.
\end{definition}

\begin{definition}
    [Token]
    A token can be a word, a number or a punctuation mark \cite{elmanarelbouanani_authorship_2014}.
\end{definition}

\begin{definition}
    [Monograph]
    Single author document \cite{bevendorff_smauc_2023}.
\end{definition}

\begin{definition}
    [Multi-author (i.e. Collaborative) publication]
    Document with multiple authors \cite{bevendorff_smauc_2023}.
\end{definition}

\begin{definition}
    [Author profiling]
    Task of inferring an extensive set of (sensitive) personal information.
    This includes sociolinguistic attributes like age, gender, occupation, education, socio-economic status, cultural background, 
    language familiarity and mental health issues 
    \cite{emmery_adversarial_2021,stamatatos_survey_2009,elmanarelbouanani_authorship_2014}.
    The task is also referred to as author characterization \cite{stamatatos_survey_2009,elmanarelbouanani_authorship_2014}.
\end{definition}

\begin{definition}
    [Stylometry]
    Liguistic research area, which refers to the (statistical) analysis of authorial/ literally style \cite{elmanarelbouanani_authorship_2014,neal_surveying_2018}.
    Stylometry assumes that style is quantifiably measurable for evaluation of distinctive qualities and 
    that features, such as subconscious syntactic idiosyncrasies are sufficient in defining an author's unique style \cite{neal_surveying_2018}.
    The construction of models for the quantification of writing style, text complexity, and grading level assessment.
    Stylometric features include lexical, syntactic and structural features \cite{stein_intrinsic_2011}.
    In other words, stylometry is the statistical analysis of literary style between one writer or genre and another \cite{tyo_state_2022}.
    Research includes five subtasks \cite{neal_surveying_2018}:
    \begin{itemize}
        \item \ac{aa}
        \item \ac{av}
        \item Author profiling
        \item Stylochronometry
        \item adversarial stylometry
    \end{itemize}
\end{definition}

\begin{definition}
    [Stylochronometry]
    The study and detection of changes in authorial style over time \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Stylistics]
    The study of stylometric features \cite{elmanarelbouanani_authorship_2014,abbasi_writeprints_2008}.
\end{definition}

\begin{definition}
    [Author writing style]
    Among others, syntactic structure of sentences in a document \cite{jafariakinabad_self_supervised_2022}.
    Opposed to other text categorization tasks, features do not include content \cite{koppel_authorship_2004}.
\end{definition}

\citet{elmanarelbouanani_authorship_2014} claim there are four (five \cite{abbasi_writeprints_2008,neal_surveying_2018}) types of writing style/ stylistic features:
\begin{itemize}
    \item Lexical features \citet{elmanarelbouanani_authorship_2014,abbasi_writeprints_2008,neal_surveying_2018}
    \item Syntactic features \citet{elmanarelbouanani_authorship_2014,abbasi_writeprints_2008,neal_surveying_2018}
    \item Structural features \citet{elmanarelbouanani_authorship_2014,abbasi_writeprints_2008,neal_surveying_2018}
    \item Content/Domain-specific features \citet{elmanarelbouanani_authorship_2014,abbasi_writeprints_2008,neal_surveying_2018}
    \item Idiosyncratic features \citet{abbasi_writeprints_2008}
    \item Semantic features \cite{neal_surveying_2018}
\end{itemize}

\begin{definition}
    [Online stylometric analysis]
    The analysis of authors style in online texts \cite{abbasi_writeprints_2008}.
    \citet{abbasi_writeprints_2008} define online texts as any textual documents that may be found in an online setting, 
    including computer-mediated communication, non-literary electronic documents (e.g., student essays, mews, articles, etc.), and program code.
\end{definition}

\begin{definition}
    [Style markers]
    Taxonomies of features to quantify the writing style \cite{stamatatos_survey_2009}.
\end{definition}

\begin{definition}
    [Stylistic features]
    Features that are the attributes or writing-style markers that are the most effective discriminators of authorship \cite{abbasi_writeprints_2008}.
\end{definition}

\begin{definition}
    [Lexical features]
    Lexical features are common features used in stylometry.
    They are character- (i.e. character (n-gram) frequency, ...) 
    or word-based (i.e. average word~\cite{stein_intrinsic_2011}, sentence length~\cite{stein_intrinsic_2011,abbasi_writeprints_2008}, 
    line length~\cite{abbasi_writeprints_2008}, word length distribution~\cite{abbasi_writeprints_2008}, 
    vocabulary richness~\cite{abbasi_writeprints_2008,neal_surveying_2018} ...) features. 
    These features consider text as a mere sequence of word-tokens or characters, respectively \cite{stamatatos_survey_2009}.
    Word features are more complex than character features \cite{stamatatos_survey_2009}.
    Character features are language-independent, not highly affected by noise (compared to word features), 
    and do not require natural language processors \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Syntactic features]
    Syntactic features include function words, punctuation, and \ac{pos} tag $n$-grams \cite{abbasi_writeprints_2008}.
    They are language-dependent and require natural language processors \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Semantic features]
    Semantic features capture meaning behind words, phrases, and sentences, such as through analysis of synonyms and semantic dependencies \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Structural features]
    Structural features include text organization, layout, file extensions, font, sizes, colours, 
    use of braces and comments (for analysing computer programs) \cite{abbasi_writeprints_2008,neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Content-specific features]
    Content-specific features include important keywords and phrases on certain topics such as word $n$-grams \cite{abbasi_writeprints_2008}.
    Domain-specific features include ratios of quoted words and external links, number of paragraphs, 
    and paragraphs average length for the news article domain \cite{potthast_stylometric_2018}
\end{definition}

\begin{definition}
    [Idiosyncratic features]
    Idiosyncratic features include misspellings, grammatical mistakes, and other usage anomalies \cite{abbasi_writeprints_2008,neal_surveying_2018}.
    Such features are extracted using spelling and grammar checking tools and dictionaries \cite{abbasi_writeprints_2008}.
\end{definition}

\begin{definition}
    [Feature-set type]
    There are two types of feature sets \cite{abbasi_writeprints_2008,neal_surveying_2018}:
    \begin{itemize}
        \item Author-group-level where one set of features is applied across all authors.
        \item Individual-author-level where each author has a unique set of features (e.g., 10 authors = 10 feature sets; 5000 most frequent character $n$-grams per author).
    \end{itemize}
    Individual-author-level features are effective for feature categories with potentially large feature spaces, such as $n$-grams or misspellings \cite{abbasi_writeprints_2008}.
    However, standard machine learning techniques typically require a fixed feature set for all authors \cite{abbasi_writeprints_2008}.
    Traditional single-author-group-level feature sets include \acp{svm} and \ac{pca} \cite{abbasi_writeprints_2008}.
\end{definition}

\begin{definition}
    [Static features]
    Static features include context-free categories such as function words, 
    word-length distributions, vocabulary richness measures, etc. \cite{abbasi_writeprints_2008}.
\end{definition}

\begin{definition}
    [Dynamic features]
    Dynamic features are context-dependent attributes and include $n$-grams and misspelled words \cite{abbasi_writeprints_2008}.
\end{definition}

\begin{definition}
    [Stability]
    Stability refers to how often a feature changes across authors and documents for a constant topic.
    \citet{abbasi_writeprints_2008} state that nouns are more stable than function words and thus, 
    function words are better stylistic discriminators than nouns 
    since using function words involves making choices between sets of synonyms.
\end{definition}

\begin{definition}
    [Adversarial Stylometry]
    Attack models that automatically infer a variety of potentially sensitive author information \cite{emmery_adversarial_2021} 
    via alteration of one's style \cite{neal_surveying_2018}.
    These attacks are not to be confused with adversarial learning \cite{emmery_adversarial_2021}.
    There are three forms of adversarial stylometry \cite{neal_surveying_2018}:
    \begin{itemize}
        \item Imitation: Writing to closely match the style of another author.
        \item Translation: Machine translating the language of a document to another language and back to the original language one or multiple times.
        \item Obfuscation: Paraphrasing such that the meaning of the text is preserved, but the style is changed.
    \end{itemize}
\end{definition}

\begin{definition}
    [Closed world]
    In the realm of plagiarism detection, closed world refers to the assumption 
    that a reference collection $D$ of documents, 
    that are supposed to be compared to the possibly plagiarized text, is given \cite{stein_intrinsic_2011}.
    In the realm of \ac{av} and \ac{aa} texts in the test set are assumed to be written by one of the authors in the training set \cite{boenninghoff_o2d2_2021,neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Plagiarism]
    In the context of texts, plagiarism is the usage of another author's information, language, 
    or writing without properly acknowledging the original source \cite{stein_intrinsic_2011}.
    \citet{elmanarelbouanani_authorship_2014} define plagiarism as the complete or partial replication 
    of a piece of work with or without permission of the original author.
\end{definition}

\begin{definition}
    [Plagiarism detection]
    The task of identifying plagiarized text \cite{stein_intrinsic_2011}, i.e. finding similarities between two texts \cite{stamatatos_survey_2009}.
    Plagiarism detection uses similarity detection, determining whether multiple pieces of work were produced by a single author 
    without necessarly identifying the author \cite{elmanarelbouanani_authorship_2014}.
\end{definition}

\begin{definition}
    [Intrinsic plagiarism detection]
    This task can be understood as a more general form of \ac{av}.
    By analysing undeclared changes in writing style, potential plagiarism can be detected.
    Opposed to \ac{av}, where the decision is made based on the whole text, 
    intrinsic plagiarism detects plagiarism on a section level \cite{stein_intrinsic_2011}.
    Intrinsic analysis does not use any information on authorship from external sources \cite{zangerle_overview_2024}.
\end{definition}

\begin{definition}
    [Authorship analysing]
    This problem devises into two \textcolor{orange}{(i.e. first two acc. to \cite{stein_intrinsic_2011}, all acc. to \cite{stamatatos_survey_2009})} subtasks \cite{stein_intrinsic_2011}:
    \begin{itemize}
        \item \ac{aa} \cite{stein_intrinsic_2011}
        \item \ac{av} \cite{stein_intrinsic_2011,stamatatos_survey_2009}
        \item Plagiarism detection \cite{stamatatos_survey_2009}
        \item Author profiling \cite{stamatatos_survey_2009}
        \item Detection of stylistic inconsistencies (i.e. in collaborative writing) \cite{stamatatos_survey_2009}
    \end{itemize}
\end{definition}

\begin{definition}
    [\ac{aa}]   % authorship attribution
    The task of determining the author of a text based on textual features 
    given a (canonical) set of candidate authors with undisputed writing samples 
    \cite{stein_intrinsic_2011,koppel_authorship_2004,stamatatos_survey_2009,tyo_state_2022,bischoff_importance_2020,barlas_cross_domain_2020,altakrori_topic_2021,bevendorff_divergence_based_2020,elmanarelbouanani_authorship_2014,abbasi_writeprints_2008,llm_detection_av_2025,neal_surveying_2018}.
    The decision is made based on stylistic traits rather than the content of the document \cite{neal_surveying_2018}.
    In terms of machine learning, this is a multiclass, single-label text categorization task 
    \cite{stamatatos_survey_2009,koppel_authorship_2004,elmanarelbouanani_authorship_2014} 
    or text classification task \cite{elmanarelbouanani_authorship_2014}.
    The task is also referred to as author(ship) identification \cite{stamatatos_survey_2009,elmanarelbouanani_authorship_2014}.
    \citet{barlas_cross_domain_2020} express the \ac{aa} task as a tuple $(A,K,U)$, 
    where $A$ is the set of authors, $K=\underset{a\in A}{\cup}K_a$ is the set of known texts and $U$ is the set of unknown texts.
    If closed-set \ac{aa}: Each text $d \in U$ is attributed to exactly one author $a \in A$.
    If cross-topic(/-genre) \ac{aa}: The topic(/genre) of documents in $d \in U$ is distinct 
    with respect to the topics(/genres) found in $K$ \cite{barlas_cross_domain_2020}. 
    \citet{llm_detection_av_2025,neal_surveying_2018} state that \ac{aa} with few (<20) candidate authors is typically highly effective, 
    even if only short writing samples are available \cite{llm_detection_av_2025}/ 
    where each author has training samples of at least 1000 words \cite{neal_surveying_2018}.
    Many candidate authors impede the \ac{aa} problem.
    Models that abstain in uncertain many candidates scenarios still achieve good results \cite{llm_detection_av_2025}.
    $k$-attribution/ ranking (i.e. relaxed \ac{aa}, where the classifier output the top $k$ authors ranked by their probability of being the author of the text), 
    cross-domain/ cross-genre \ac{aa} (i.e., identify author $M$ of document written in domain $A$, while only having documents from $m$ in domain $B$ and thus, 
    features of domain $B$ must be applicable to domain $A$), 
    and source code \ac{aa} 
    are forms of \ac{aa} \cite{neal_surveying_2018}.
\end{definition}

\citet{elmanarelbouanani_authorship_2014} describe the workflow of \ac{aa} as follows:
\begin{enumerate}
    \item Data cleaning
    \item Feature extraction
    \item Normalization
    \item Converting each text into a feature vector, where author is the class label
    \item Split the dataset into training and test set
\end{enumerate}
Common classifiers include \ac{svm}, decision trees, and \acp{nn} \cite{elmanarelbouanani_authorship_2014}.

\begin{definition}
    [\ac{av}]   % authorship verification
    Given a set of writing samples of author $A$ and a text $t$,    % tyo_state_2022: only one wrinting sample
    the task is to determine whether $t$ was written by $A$ \cite{stein_intrinsic_2011,stamatatos_survey_2009,koppel_authorship_2011,tyo_state_2022,kocher_unine_2015,koppel_authorship_2004}.
    This task can also be formulated as whether two texts $t_1$ and $t_2$ are written by the same author 
    \cite{bevendorff_generalizing_2019,bevendorff_divergence_based_2020,embarcadero_ruiz_graph_based_2022,rivera_soto_learning_2021,ordonez_will_2020,futrzynski_pairwise_2021,weerasinghe_feature_vector_difference_2021,llm_detection_av_2025}.
    % Gespräch Martin Potthast 19.05.2025: problem formulation 2 is less common and in the context of very sparse (metadata) information
    Related research areas include \cite{stein_intrinsic_2011}:
    \begin{itemize}
        \item stylometry
        \item outlier analysis and meta learning
        \item symbolic knowledge representation, i.e. \todo{knownledge representation, deduction, heuristic inference}
    \end{itemize}
    \citet{tyo_state_2022} state that \ac{av} is the fundamental problem of \ac{aa} \cite{tyo_state_2022}, 
    where there is only one candidate author \cite{barlas_cross_domain_2020}.
    \citet{llm_detection_av_2025,koppel_authorship_2004} state that \ac{av} is a more general and difficult (than \ac{aa}) one-class classification problem.
    Hence, the disputed document is compared only to documents from the candidate, 
    ignoring intermediate negatives (all human texts not written by the known author) 
    \cite{llm_detection_av_2025,neal_surveying_2018,koppel_authorship_2004}.
    It is impossible to assemble an exhaustive, or even representative samples of the non-target class \cite{koppel_authorship_2004}.
    \ac{av} is different from usual one-class classification problems, in which there is a lack of negative examples (opposed to non-representative ones), 
    and in which the text to attribute is not necessarily long (opposed to \citet{koppel_authorship_2004}'s paper).
    \citet{neal_surveying_2018} consider \ac{av} a binary classification problem (i.e. \textit{same-author} or \textit{different-author}), 
    where \textit{different-author} renders \ac{av} as an open-set problem.
    However, \citet{neal_surveying_2018} consider framing \ac{av} as a one-class classification problem as a common approach (cf. \cite{llm_detection_av_2025}).
    \citet{elmanarelbouanani_authorship_2014} consider \ac{av} a similarity detection task.
    \citet{neal_surveying_2018} also mention the many-candidates method where \ac{av} is framed as \ac{aa} via creating a set of imposters.
    Use cases include plagiarism detection, moderation of user-generated content, historical \ac{aa}, and forensic analysis \cite{rivera_soto_learning_2021}.
\end{definition}

\begin{definition}
    [Similarity detection]
    The task of comparing anonymous texts against other anonymous texts to assess the degree of similarity in terms of stylistic characteristics \cite{abbasi_writeprints_2008,neal_surveying_2018}.
    In the context of \ac{av}, the task is to determine whether two texts are produced by the same person without knowing the real author of the document \cite{elmanarelbouanani_authorship_2014}.
\end{definition}

\begin{definition}
    [Message-level analysis]
    The analysis attempts to categorize individual texts (e.g., whether an email was authored by an email address) \cite{abbasi_writeprints_2008}.
\end{definition}

\begin{definition}
    [Identity-level analysis]
    The analysis attempts to classify individuals belonging to a particular entity 
    (e.g., whether different email addresses belong to the same entity).
    Identity-level analysis' categorization is based on all texts written by that identity.
    Hence, there are larger text samples than for message-level analysis, facilitating the task.
    If the task is framed as classification/ID identification, the disputed identity is assigned to the identity with the highest similarity among the known identities.
    If the task is framed as similarity detection task, all identities with a similarity score above a certain threshold are grouped together and 
    considered to belong to the same entity \cite{abbasi_writeprints_2008}.
\end{definition}

\begin{table}[]
    \centering
    \caption{Building blocks for \ac{av} from \cite{stein_intrinsic_2011}.}
    \label{tab:authorship_verification_blocks}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|lll|l|}
    \hline
    \rowcolor[HTML]{EFEFEF} 
    Pre-analysis & \multicolumn{3}{l|}{\cellcolor[HTML]{EFEFEF}Modeling and classifier methods} & Post-processing \\ \hline
    \rowcolor[HTML]{EFEFEF} 
    Impurity assessment & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}Decomposition strategy} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}Style model construction} & Outlier identification & Outlier post-processing \\ \hline
    Document length analysis & \multicolumn{1}{l|}{Uniform length} & \multicolumn{1}{l|}{Lexical character features} & One-class density estimation & Heuristric voting \\
    Genre Analysis & \multicolumn{1}{l|}{Structural boundaries} & \multicolumn{1}{l|}{Lexical word features} & One-class boundary estimation & Citation analysis \\
    Analysis of issuing institution & \multicolumn{1}{l|}{Text element boundaries} & \multicolumn{1}{l|}{Syntactical features} & One-class reconstruction & Human inspection \\
     & \multicolumn{1}{l|}{Topical boundaries} & \multicolumn{1}{l|}{Structural features} & Two-class discrimination & Unmasking \\
     & \multicolumn{1}{l|}{Stylistic boundaries} & \multicolumn{1}{l|}{Language modeling} &  & Qsum \\
     & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  & Batch means
    \end{tabular}%
    }
\end{table}
Post-processing to avoid false positives, c.f. \citet{stein_intrinsic_2011} for approaches.

\begin{definition}
    [Meta learning]
    \todo{Based on learning successes and failures, the system learns to learn. }
    Approaches include:
    \begin{itemize}
        \item Unmasking: Measurement of reconstruction errors starting from a good reconstruction and iteratively impairing the reconstruction. % example on page 9 of Benno's paper
        \item Qsum heuristic: Compares the growth rates of two cumulative sums over a sequence of sentences. The sums are calculated via the deviations from the mean sentence length and the deviations of the function words.
        \item Batch means: \todo{For a series of values the variance development of the sample mean is measured while the sample size is successively increased.}
    \end{itemize}
\end{definition}

\begin{definition}
    [Unmasking]
    The idea of this meta learning approach is that with progressively omitting more and more frequent words/ 
    most discriminating features, 
    topic specific words are excluded and thus, leaving only writing style specific words \cite{stein_intrinsic_2011}.
    After several iterations, remaining features which are not powerful enough to discriminate two documents indicate that 
    these documents originate from the same author \cite{stein_intrinsic_2011,tyo_state_2022,bevendorff_divergence_based_2020,koppel_authorship_2004}.
    In other words: 
    Two texts are probably written by different authors if the differences between are robust to changes in the underlying feature set used to represent the documents.
    Differences can be measured using instance-based (meta) classification via cross-validation accuracy 
    \cite{koppel_authorship_2011,bevendorff_generalizing_2019,bevendorff_divergence_based_2020,potthast_stylometric_2018,koppel_authorship_2004}, 
    creating a performance degradation curve \cite{tyo_state_2022,koppel_authorship_2004}.
    An \ac{svm} is trained to classify the degradation curve to determine whether two text originated from the same author 
    \cite{tyo_state_2022,bevendorff_generalizing_2019,koppel_authorship_2004}.
    Cf. \cite{bevendorff_divergence_based_2020} Chapt. 2 for a detailed algorithm.
    Steep decrease in the curve indicates that the two texts are similar, and thus, 
    written by the same authors \cite{potthast_stylometric_2018,koppel_authorship_2004}.
    Provided that the unseen text is very large, this method can handle small open candidate sets \cite{koppel_authorship_2011}.
    % koppel_determining_2014, pg. 1 + bevendorff_generalizing_2019 chap. 3.1 incl. algo: based on text chunks of length >= 500 words each
    \citet{koppel_determining_2014,bevendorff_generalizing_2019} claim that effective unmasking requires input documents to be large 
    (i.e. > 10000 words~\cite{koppel_determining_2014}, book-length~\cite{bevendorff_generalizing_2019}, 
    $\geq$ 5000 words (500 words per chunk) \cite{bevendorff_divergence_based_2020}).
    Otherwise the training set becomes too sparse and no descriptive curves can be generated 
    \cite{bevendorff_generalizing_2019,bevendorff_divergence_based_2020}.
\end{definition}

\begin{definition}
    [Stop words vs. function words]
    Function words are the most common words (articles, prepositions, pronouns, etc.) 
    like "while", "upon", "though", "were", "your" \cite{stamatatos_survey_2009,elmanarelbouanani_authorship_2014}.
    They are typically regarded as context-free (and therefore less influenced by topic and genre), 
    while revealing social and personal aspects of our lives \cite{neal_surveying_2018}.
    Most function words are stop words, but not all stop words are function words \cite{stein_intrinsic_2011}.
    \citet{elmanarelbouanani_authorship_2014} state that researchers use between 150 and 675 function words as features.
    \citet{abbasi_writeprints_2008} state that function words are highly effective discriminators of authorship, since 
    the usage variations of such words are strong reflection of stylistic choices.
\end{definition}

\begin{definition}
    [One-class classification]
    A classification problem where the classifier is trained on samples of a single class.
    If counterexamples, i.e. so-called outliers, are available, they are usually not considered to be representative of \textit{non-target class}.
    Hence, the classifier has to learn the concept of the target class in the absence of discriminating features 
    \cite{stein_intrinsic_2011,koppel_authorship_2004}.
    Examples of one-class classification are intrinsic plagiarism analysis and \ac{av}.
    Approaches to one-class classification fall into the following categories \cite{stein_intrinsic_2011}:
    \begin{itemize}
        \item One-class density estimation, e.g., Naive Bayes
        \item One-class boundary estimation
        \item One-class reconstruction
    \end{itemize}
\end{definition}

\begin{definition}
    [Open-set classification]
    The true author is not necessarily included in the set of candidate authors \cite{stamatatos_survey_2009,barlas_cross_domain_2020,neal_surveying_2018}.
    It is a generalization of the closed-set classification problem allowing for an unknown author using a threshold for similarity \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Closed-set classification]
    The true author is one necessarily one of the candidate authors \cite{stamatatos_survey_2009,koppel_authorship_2011,barlas_cross_domain_2020,boenninghoff_o2d2_2021,neal_surveying_2018}.
    In other words: The set of all possible author classes is known a priori.
    Hence, closed-set problems can use supervised or unsupervised classification techniques \cite{abbasi_writeprints_2008}.
\end{definition}

\begin{definition}
    [Supervised techniques]
    Supervised techniques for stylometric analysis require (author-)class labels for categorization.
    Examples include \acp{svm}, \acp{nn}, decision trees, and linear discriminant analysis.
    \acp{svm} are very common in authorship analysis due to their robustness \cite{abbasi_writeprints_2008}.
\end{definition}

\begin{definition}
    [Unsupervised techniques]
    Unsupervised techniques make categorizations with no prior knowledge of author classes.
    Examples include \ac{pca} and cluster analysis.
    \ac{pca} has been used in previous authorship studies due to its ability to 
    capture essential variance across large number of features in a reduced dimensionality \cite{abbasi_writeprints_2008}.
\end{definition}

\begin{definition}
    [Covariate shift]
    The distribution of neural stylometric features changes between training and test set due to, for instance, topic variability \cite{boenninghoff_o2d2_2021}.
\end{definition}

\begin{definition}
    [n-gram]
    $n$ contiguous words also known as word collocations. \todo{cite, reference?\cite{koppel_authorship_2011}?}
    n-grams are no stylometric features \cite{altakrori_topic_2021}.
    % Quelle Martin Potthast Gespräch 19.05.2025:
    Tri-grams are commonly used in stylistic analysis, due to their ability to capture inflections, % Flexion/ Beugung in Deutsch
    morphemes, %  smallest meaningful constituents within a linguistic expression and particularly within a word
    and other syntactic structures for Germanic languages.
    Character-level $n$-gram features capture the frequency of $n$ consecutive characters in a text \cite{neal_surveying_2018}.
    The optimal $n$ is language dependent \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Space free n-gram]
    Removing spaces from the $n$-gram reduces the number of $n$-grams.
    \citet{koppel_authorship_2011} use these definitions:
    \begin{enumerate}
        \item a string of $n$ characters that not include spaces
        \item a string of less than $n$ characters that is surrounded by spaces
    \end{enumerate}
\end{definition}

\begin{definition}
    [Domain shift]
    Systematic statistical differences between the training and test sets \cite{tyo_state_2022}.
    These differences include:
    \begin{itemize}
        \item datasets are not identically distributed
        \item test set contains novel topics $\times_t$
        \item test set contains novel authors $\times_a$
        \item test set contains novel genres $\times_g$
    \end{itemize}
\end{definition}

\begin{definition}
    [Topic-confusion]
    In this setting, all topics appear in both training and test set. 
    However, the topics of the texts for each author changes in the test set, 
    i.e. author-topic configuration is switched between training and testing.
    For example, in the training set, the author $A_1$ writes about topic $T_1$, author $A_2$ writes about topic $T_2$ 
    and in the test set, 
    the author $A_1$ writes about topic $T_2$ while author $A_2$ writes about topic $T_1$ \cite{tyo_state_2022,altakrori_topic_2021}.
    Intuitive, the more a feature is influenced by the topic of document to identify its author, 
    the more confusion it will be to the classifier when the topic-author combination is switched, which will lead to performance deterioration \cite{altakrori_topic_2021}.
\end{definition}

\begin{definition}
    [text distortion]
    This domain-adversial method substitutes out-of-vocaublary items with asterisks $*$ \cite{tyo_state_2022}.
    Its goal is to reduce domain-specific information \cite{bischoff_importance_2020}.
    Distortion algorithms include \cite{bischoff_importance_2020}:
    \begin{itemize}
        \item Replacing tokens with multiple asterisks
        \item Replacing tokens with single asterisks
        \item Retaining only exterior characters of words in a dictionary
        \item Retaining the last two characters
    \end{itemize}
\end{definition}

\begin{definition}
    [Imposter method]
    This method extends the ngram-unmasking method, i.e. iteratively omitting most influencely features (repeated feature subsampling \cite{koppel_determining_2014})
    from a trained classifier and classifying the accuracy drop.
    It takes score of how often an author is predicted after each feature-elimination step.
    The final prediction is made based on this score \cite{tyo_state_2022}.
\end{definition}


\begin{definition}
    [Hard Negative Mining]
    This method updates the model during training only with the most difficult examples in each batch.
    In the \ac{aa} context, difficult is defined as the most similar two texts from different authors, 
    which makes the decision the most difficult.
    \citet{tyo_state_2022} claim that the \ac{av} setting is strictly easier since 
    it most compare to only a single text.
    Due to the fact, that the most difficult example is model-dependent, \ac{av} problems can be made harder 
    but they can not exist of exactly the hardest negatives.
\end{definition}

\begin{definition}
    [Domain]
    The domain include topic, genre, register, idiolect, time period etc. \cite{bischoff_importance_2020}.
\end{definition}
  
\begin{definition}
    [Domain variables]
    These include topic, genre and language \cite{bischoff_importance_2020}.
\end{definition}

\begin{definition}
    [Style transfer]
    Translation or rather paraphrasing a text from a source style to a desired target style.
    Two major problems include the lack of large-scale parallel training data (i.e., texts written in both styles), 
    and the lack of reliable evaluation metric (i.e. assessment by humans) \cite{bischoff_importance_2020}.
\end{definition}

\begin{definition}
    [Author obfuscation]
    Task of paraphrasing a text to render an author's style imperceptible.
    Usually, another text from the author is used as a reference for style similarity \cite{bischoff_importance_2020}.
    In other words: It is the adversarial task of preventing successful verification by altering the text's style so that 
    it no longer resembles the original author's style \cite{bevendorff_divergence_based_2020}.
\end{definition}

\begin{definition}
    [within-domain]
    Experiments with P=Q.
    Hence, it is necessary to ensure all texts are mutually from the same domain \cite{bischoff_importance_2020}.
    \begin{table}[]
        \centering
        \caption{Typical scheme $S_1$ for \ac{aa} problem instances, where A, B, are authors and P, Q domains and 
        the vertical mapping denotes which author has written in which domain. 
        For training, texts from A and B take turn; for testing, previously unseen texts from A and B are used \cite{bischoff_importance_2020}.}
        \label{tab:within_domain_aa}
        \begin{tabular}{|l|ll|ll|}
        \hline
        \textbf{Scheme $S_1$} & \multicolumn{2}{l|}{\textbf{training}} & \multicolumn{2}{l|}{\textbf{testing}} \\ \hline
        \textbf{authors} & \multicolumn{1}{l|}{A} & B & \multicolumn{1}{l|}{A} & B \\ \hline
        \textbf{domains} & \multicolumn{1}{l|}{P} & Q & \multicolumn{1}{l|}{P} & Q \\ \hline
        \end{tabular}%
    \end{table}
\end{definition}

\begin{definition}
    [Domain swapping]
    Experiments with P$\neq$Q \cite{bischoff_importance_2020}.
    \begin{table}[]
        \centering
        \caption{Domain-swapping scheme $S_2$ for \ac{aa} problem instances, where A, B, are authors and P, Q domains and 
        the vertical mapping denotes which author has written in which domain. 
        For training, texts from A and B take turn; for testing, previously unseen texts from A and B are used \cite{bischoff_importance_2020}.}
        \label{tab:within_domain_aa}
        \begin{tabular}{|l|ll|ll|}
        \hline
        \textbf{Scheme $S_2$} & \multicolumn{2}{l|}{\textbf{training}} & \multicolumn{2}{l|}{\textbf{testing}} \\ \hline
        \textbf{authors} & \multicolumn{1}{l|}{A} & B & \multicolumn{1}{l|}{A} & B \\ \hline
        \textbf{domains} & \multicolumn{1}{l|}{P} & Q & \multicolumn{1}{l|}{Q} & P \\ \hline
        \end{tabular}%
    \end{table}
    There are two kinds of domain swapping:
    \begin{itemize}
        \item \textbf{Zero-knowledge swapping}: Maximizes the potential for confusion during training, 
        since the models never see an author in writing in the other author's respective fandom.
        This approach aggravates adversarial training, since it needs domain knowledge to be effective.
        \item \textbf{High-imbalance swapping}: Imbalance is swapped between the training and test set. 
        This is an approximation of the zero-knowledge swapping, while still allowing adversarial learning.
    \end{itemize}
\end{definition}

\begin{definition}
    [train-test-validation split]
    \citet{bischoff_importance_2020,altakrori_topic_2021,boenninghoff_o2d2_2021} train their model on a selection of the dataset (i.e. training set), 
    optimize the model's hyperparameters on a second disjoint selection of the dataset (i.e. validation set),
    and evaluate the model on a third disjoint selection of the dataset (i.e. test set).
    \citet{bischoff_importance_2020} ensure that there is no data leakage between the training, validation and test sets 
    (i.e. prevent parts of one fanfiction being in more than one of the data splits).
    \citet{altakrori_topic_2021} ensure the classifier to be trained has no access to any information about the setup 
    (topic confusion: group configuration or topic labels).
\end{definition}

\begin{definition}
    [Cross-domain]
    Texts of known authorship (training set) differ from texts of disputed authorship (test set) 
    in topic (i.e. cross-topic) or genre (i.e. cross-genre) 
    \cite{barlas_cross_domain_2020}.
\end{definition}

\begin{definition}
    [Cross-topic]
    New, unseen topics are used in the testing phase \cite{altakrori_topic_2021}.
\end{definition}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[]
    \centering
    \caption{\ac{aa} scenarios with author $i$ is shortened with $A_i$ \cite{altakrori_topic_2021}.}
    \label{tab:aa_same_topic}
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{} & \textbf{Train} & \textbf{Test} \\ \hline
    \textbf{Topic $T_1$} & $A_1, A_2$ & $A_1, A_2$ \\ \hline
    \textbf{Topic $T_2$} & $A_1, A_2$ & $A_1, A_2$ \\ \hline
    \end{tabular}%
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[]
    \centering
    \caption{\ac{aa} scenarios with author $i$ is shortened with $A_i$ \cite{altakrori_topic_2021}.}
    \label{tab:aa_cross_topic}
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{} & \textbf{Train} & \textbf{Test} \\ \hline
    \textbf{Topic $T_1$} & $A_1, A_2$ &  \\ \hline
    \textbf{Topic $T_2$} &  & $A_1, A_2$ \\ \hline
    \end{tabular}%
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[]
    \centering
    \caption{\ac{aa} scenarios with author $i$ is shortened with $A_i$ \cite{altakrori_topic_2021}.}
    \label{tab:aa_topic_confusion}
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{} & \textbf{Train} & \textbf{Test} \\ \hline
    \textbf{Topic $T_1$} & $A_1$ & $A_2$ \\ \hline
    \textbf{Topic $T_2$} & $A_2$ & $A_1$ \\ \hline
    \end{tabular}%
\end{table}

\begin{definition}
    [Data preprocessing]
    Typical stylometry subtask for normalization and noise reduction.
    Examples include tokenization, stemming, tagging, removing non-alphabetic characters and spaces, and converting uppercase letters to lowercase \cite{neal_surveying_2018}.,
\end{definition}

\begin{definition}
    [Tokenization]
    Splitting a stream of text into words, phrases, etc. \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Stemming]
    Only retaining the root or base form of a word \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Tagging]
    Replacing words with their grammatical type \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [\ac{bow}]
    The \ac{bow} approach generally refers to lexical-level features as it represents a document as a bag (or collection) of words, 
    discarding context, grammar, and word order \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Latent Dirichlet Allocation (LDA)]
    LDA is a three-level Bayesian technique for modelling a collection over a set of topics.
    LDA is a probabilistic model where each topic is determined by word distributions \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Clustering]
    Clustering is an unsupervised machine-learning procedure, where the algorithm derives a natural separation of the feature space 
    that may or may not correlate with the class labels \cite{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Paraphrase divergence]
    This means that when a question is phrased in s slightly different but semantically similar way, 
    \ac{llm} may output a wrong response despite being able to answer to the original question correctly \cite{fu_learning_2024}.
\end{definition}

\begin{definition}
    [Paraphrases]
    Texts that convey identical meanings/semantic information while using different words (wording) or (sentence) structures 
    \cite{fu_learning_2024,zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024,gohsen_captions_2023}.
\end{definition}

\begin{definition}
    [Quasi-Paraphrases]
    Texts that have similar meanings using different words, i.e. approximate equivalents \cite{zhou_paraphrase_2025}.
\end{definition}

\begin{definition}
    [Encoder]
    The main purpose of an encoder is to extract the semantic information for the decoder \cite{zhou_paraphrase_2021}.
\end{definition}

\begin{definition}
    [Attention mechanism]
    Attention allows the model to focus on particular words/phrases in the input sequence when generating the output sequence.
    First, a weight (i.e. importance) for each token in the source sequence in each timestep is computed.
    Then, both the text input and the context vector with weights is provided to the decoder \cite{zhou_paraphrase_2021}.
\end{definition}

\begin{definition}
    [\acl{rl}]
    \ac{rl} trains agents to take actions in an environment to maximize a cumulative reward \cite{zhou_paraphrase_2021}.
\end{definition}

\begin{definition}
    [GANs]
    Generative Adversarial Networks (GANs) consist of a generator and a discriminator.
    The generator creates realistic data instances that match the real distribution, 
    while the discriminator distinguishes which instance are generated and which are real \cite{zhou_paraphrase_2021}.
\end{definition}

\begin{definition}
    [Zero-Shot]
    Zero-Shot capabilities enable models, e.g. \acp{llm}, to perform well on unseen tasks \cite{master_thesis_paraphrasing_2024}.
\end{definition}

\begin{definition}
    [Internal validity]
    Internal validity is the extent to which the study results can be attributed 
    to the manipulations of the independent variable rather than other factors \cite{master_thesis_paraphrasing_2024}.
\end{definition}

\begin{definition}
    [External validity]
    External validity is the extent to which the study results can be generalized 
    to other contexts, settings, or populations \cite{master_thesis_paraphrasing_2024}.
\end{definition}

\begin{definition}
    [Subtasks of Paraphrasing]
    There are two sub-tasks of paraphrasing \cite{palivela_optimization_2021}:
    \begin{itemize}
        \item \ac{pg}: The task of generating fluent, well-formed, coherent and semantically similar paraphrases 
        that exhibit both syntactic and/or lexical diversity from a given sentence \cite{palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}. 
        It can be solved by using simple lexical features and word ordering or restructuring methods or 
        by using templates extracted from WikiAnswers repositories \cite{palivela_optimization_2021}.
        Formally, for a sentence $S_1=\{w_1, \cdot, w_n\}$, generate one or more candidate sentences $S_2=\{w_1, \cdot, w_m\}, \cdot$.
        Sentence lengths may vary. \cite{palivela_optimization_2021}.
        \item \ac{pi}: The task of determining whether two sentences are paraphrases of each other.
        \ac{pi} ca be viewed as a discriminative task. The system output can be a probability (1 for paraphrase, 0 for non-paraphrase) 
        or a semantic score which can be normalized \cite{palivela_optimization_2021}.
        Formally, for a sentence pair $(S_1, S_2)$, find a target 1 or 0. Sentence lengths may vary. \cite{palivela_optimization_2021}.
    \end{itemize}
\end{definition}

\begin{definition}
    [Syntactic diversity]
    Syntactic diversity refers to the variety of arrangements of words, phrases, and clauses in sentences \cite{kurt_pehlivanoglu_comparative_2024}.
\end{definition}

\begin{definition}
    [Syntax]
    Syntax or syntactic structure is the structural organization of sentences.
    Grammatical function and meaning of sentence depend on syntax \cite{kurt_pehlivanoglu_comparative_2024}.
\end{definition}

\begin{definition}
    [Sentence structures]
    There are four basic types of sentence structures: Simple, compound, complex and compound-complex \cite{kurt_pehlivanoglu_comparative_2024}.
\end{definition}

\begin{definition}
    [Transfer learning]
    Transfer learning is a machine learning technique where a model trained on a data-rich task is 
    reused as the starting point (i.e. finetuning) for a model on a second task (i.e. downstream task) \cite{palivela_optimization_2021}.
\end{definition}

\begin{definition}
    [Type I error]
    \ac{fp}, or in other words: The model predicts positive when it is negative in reality \cite{palivela_optimization_2021}.
\end{definition}

\begin{definition}
    [Type II error]
    \ac{fn}, or in other words: The model predicts negative when it is positive in reality \cite{palivela_optimization_2021}.
\end{definition}

\begin{definition}
    [ChatGPT]
    Short for Conditional Generative Pre-Trained Transformer.
    It is developed by OpenAI, trained on millions of diverse conversations utilizing supervised and reinforcement learning techniques \cite{kurt_pehlivanoglu_comparative_2024}.
\end{definition}

\begin{definition}
    [Paraphrase attacks]
    \ac{ai}-generated text from a \ac{llm} is rewritten by another (smaller) model to convey approximately the same meaning using different words or syntax \cite{krishna_paraphrasing_2023}.
\end{definition}

\begin{definition}
    [Watermark]
    Watermark is modification to the generated text than can be detected post-hoc by an algorithm while remaining imperceptible to human readers \cite{krishna_paraphrasing_2023}.
\end{definition}

\begin{definition}
    [WordNet]
    WordNet is a lexical database of semantic relations \cite{zhou_paraphrase_2025}.
\end{definition}

\begin{definition}
    [Semantic parser]
    Semantic features, that are designed to construct the meaning behind a given sentence \cite{zhou_paraphrase_2025}.
\end{definition}

\begin{definition}
    [Matrix factorization]
    Common technique for reducing dimensionality of matrices in semantic \ac{pi} tasks \cite{zhou_paraphrase_2025}.
\end{definition}

\begin{definition}
    [Singular Value Decomposition (SVD)]
    SVD is a matrix factorization technique \cite{zhou_paraphrase_2025}.
\end{definition}

\begin{definition}
    [Latent semantic analysis (LSA)]
    LSA is a corpus-based measure that can be used for semantic similarity by using SVD to reduce the term-document matrix representing the corpus.
    In \ac{pi}, sentences are often treated as pseudo-documents to identify paraphrases using similarity in the latent space \cite{zhou_paraphrase_2025}.
\end{definition}

\begin{definition}
    [Linear discriminant analysis (LDA)]
    LDA uses SVD to perform factorization on the co-occurrence matrix with a non-negativity constraint in the latent representation based on non-orthogonal basis \cite{zhou_paraphrase_2025}.
\end{definition}

\begin{definition}
    [Subsequence]
    A sequence $Z=[z_1, z_2, ..., z_n]$ is a subsequence from another sequence $X=[x_1, x_2, ..., x_m]$ if there exists a strictly increasing sequence of indices $i_1, i_2, ..., i_k$ of $X$ such that $z_j = x_{i_j}$ for all $j=1, 2, ..., k$ \cite{lin_rouge_2004}.
\end{definition}

\begin{definition}
    [Longest Subsequence (LCS)]
    Given two sequences $X$ and $Y$, the longest common subsequence (LCS) of $X$ and $Y$ is the common subsequence with maximum length \cite{lin_rouge_2004}.
\end{definition}
\section{Definitions}
\label{sec:definitions}


\begin{definition}
    [Author profiling]
    Task of inferring an extensive set of (sensitive) personal information.
    This includes age, gender, education, socio-economic status and mental health issues \cite{emmery_adversarial_2021,stamatatos_survey_2009}.
    The task is also referred to as author characterization \cite{stamatatos_survey_2009}.
\end{definition}

\begin{definition}
    [Stylometry]
    The construction of models for the quantification of writing style, text complexity, and grading level assessment.
    Stylometric features include lexical, syntactic and structural features \cite{stein_intrinsic_2011}.
    In other words, stylometry is the statistical analysis of literary style between one writer or genre and another \cite{tyo_state_2022}.
\end{definition}

\begin{definition}
    [Style markers]
    Taxonomies of features to quantify the writing style \cite{stamatatos_survey_2009}.
\end{definition}

\begin{definition}
    [Lexical features]
    Lexical features are common features used in stylometry.
    They are character- (i.e. character (n-gram) frequency, ...) 
    or word-based (i.e. average word/ sentence length, ...) features \cite{stein_intrinsic_2011}. 
    These features consider text as a mere sequence of word-tokens or characters, respectively \cite{stamatatos_survey_2009}.
    Word features are more complex than character features \cite{stamatatos_survey_2009}.
\end{definition}

\begin{definition}
    [Adversarial Stylometry]
    Attack models that automatically infer a variety of potentially sensitive author information.
    These attacks are not to be confused with adversarial learning \cite{emmery_adversarial_2021}.
\end{definition}

\begin{definition}
    [Closed world]
    In the realm of plagiarism detection, closed world refers to the assumption that a reference collection $D$ of documents, that are supposed to be compared to the possibly plagiarized text, is given \cite{stein_intrinsic_2011}.
\end{definition}

\begin{definition}
    [Plagiarism]
    In the context of texts, plagiarism is the usage of another author's information, language, or writing without properly acknowledging the original source \cite{stein_intrinsic_2011}.
\end{definition}

\begin{definition}
    [Plagiarism detection]
    The task of identifying plagiarized text \cite{stein_intrinsic_2011}, i.e. finding similarities between two texts \cite{stamatatos_survey_2009}.
\end{definition}

\begin{definition}
    [Intrinsic plagiarism detection]
    This task can be understood as a more general form of \ac{av}.
    By analysing undeclared changes in writing style, potential plagiarism can be detected.
    Opposed to \ac{av}, where the decision is made based on the whole text, 
    intrinsic plagiarism detects plagiarism on a section level \cite{stein_intrinsic_2011}.
    Intrinsic analysis does not use any information on authorship from external sources \cite{zangerle_overview_2024}.
\end{definition}

\begin{definition}
    [Authorship analysing]
    This problem devises into two \textcolor{orange}{(i.e. first two acc. to \cite{stein_intrinsic_2011}, all acc. to \cite{stamatatos_survey_2009})} subproblems \cite{stein_intrinsic_2011}:
    \begin{itemize}
        \item \ac{aa} \cite{stein_intrinsic_2011}
        \item \ac{av} \cite{stein_intrinsic_2011,stamatatos_survey_2009}
        \item Plagiarism detection \cite{stamatatos_survey_2009}
        \item Author profiling \cite{stamatatos_survey_2009}
        \item Detection of stylistic inconsistencies (i.e. in collaborative writing) \cite{stamatatos_survey_2009}
    \end{itemize}
\end{definition}

\begin{definition}
    [\ac{aa}]   % authorship attribution
    The task of determining the author of a text based on textual features 
    given a (canonical) set of candidate authors with undisputed writing samples \cite{stein_intrinsic_2011,stamatatos_survey_2009,tyo_state_2022}.
    In terms of machine learning, this is a multiclass, single-label text categorization task \cite{stamatatos_survey_2009}.
    The task is also referred to as author(ship) identification \cite{stamatatos_survey_2009}.
\end{definition}

\begin{definition}
    [\ac{av}]   % authorship verification
    Given a set of writing samples of author $A$ and a text $t$,    % tyo_state_2022: only one wrinting sample
    the task is to determine whether $t$ was written by $A$ \cite{stein_intrinsic_2011,stamatatos_survey_2009,koppel_authorship_2011,tyo_state_2022}.
    Related research areas include \cite{stein_intrinsic_2011}:
    \begin{itemize}
        \item stylometry
        \item outlier analysis and meta learning
        \item symbolic knowledge representation, i.e. \todo{knownledge representation, deduction, heuristic inference}
    \end{itemize}
    \citet{tyo_state_2022} state that \ac{av} is the fundamental problem of \ac{aa}.
\end{definition}

\begin{table}[]
    \centering
    \caption{Building blocks for \ac{av} from \cite{stein_intrinsic_2011}.}
    \label{tab:authorship_verification_blocks}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|lll|l|}
    \hline
    \rowcolor[HTML]{EFEFEF} 
    Pre-analysis & \multicolumn{3}{l|}{\cellcolor[HTML]{EFEFEF}Modeling and classifier methods} & Post-processing \\ \hline
    \rowcolor[HTML]{EFEFEF} 
    Impurity assessment & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}Decomposition strategy} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}Style model construction} & Outlier identification & Outlier post-processing \\ \hline
    Document length analysis & \multicolumn{1}{l|}{Uniform length} & \multicolumn{1}{l|}{Lexical character features} & One-class density estimation & Heuristric voting \\
    Genre Analysis & \multicolumn{1}{l|}{Structural boundaries} & \multicolumn{1}{l|}{Lexical word features} & One-class boundary estimation & Citation analysis \\
    Analysis of issuing institution & \multicolumn{1}{l|}{Text element boundaries} & \multicolumn{1}{l|}{Syntactical features} & One-class reconstruction & Human inspection \\
     & \multicolumn{1}{l|}{Topical boundaries} & \multicolumn{1}{l|}{Structural features} & Two-class discrimination & Unmasking \\
     & \multicolumn{1}{l|}{Stylistic boundaries} & \multicolumn{1}{l|}{Language modeling} &  & Qsum \\
     & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  & Batch means
    \end{tabular}%
    }
\end{table}
Post-processing to avoid false positives, c.f. \citet{stein_intrinsic_2011} for approaches.

\begin{definition}
    [Meta learning]
    \todo{Based on learning successes and failures, the system learns to learn. }
    Approaches include:
    \begin{itemize}
        \item Unmasking: Measurement of reconstruction errors starting from a good reconstruction and iteratively impairing the reconstruction. % example on page 9 of Benno's paper
        \item Qsum heuristic: Compares the growth rates of two cumulative sums over a sequence of sentences. The sums are calculated via the deviations from the mean sentence length and the deviations of the function words.
        \item Batch means: \todo{For a series of values the variance development of the sample mean is measured while the sample size is successively increased.}
    \end{itemize}
\end{definition}

\begin{definition}
    [Unmasking]
    The idea of this meta learning approach is that with progressively omitting more and more frequent words, 
    topic specific words are excluded and thus, leaving only writing style specific words \cite{stein_intrinsic_2011}.
    After several iterations, remaining features which are not powerful enough to discriminate two documents indicate that 
    these documents originate from the same author \cite{stein_intrinsic_2011,tyo_state_2022}.
    In other words: 
    Two texts are probably written by different authors if the differences between are robust to changes in the underlying feature set used to represent the documents.
    Differences can be measured using instance-based classification via cross-validation accuracy \cite{koppel_authorship_2011}, 
    creating a performance degradation curve \cite{tyo_state_2022}.
    An \ac{svm} is trained to classify the degradation curve to determine whether two text originated from the same author \cite{tyo_state_2022}.
    Provided that the unseen text is very large, this method can handle small open candidate sets \cite{koppel_authorship_2011}.
    % koppel_determining_2014, pg. 1: based on text chunks
    \citet{koppel_determining_2014} claim that effective unmasking requires input documents to be large (i.e. > 10000 words).
\end{definition}

\begin{definition}
    [Stop words vs. function words]
    Function words are the most common words (articles, prepositions, pronouns, etc.) \cite{stamatatos_survey_2009}.
    Most function words are stop words, but not all stop words are function words \cite{stein_intrinsic_2011}.
\end{definition}

\begin{definition}
    [One-class classification]
    A classification problem where the classifier is trained on samples of a single class.
    If counterexamples, i.e. so-called outliers, are available, they are usually not considered to be representative of \textit{non-target class}.
    Hence, the classifier has to learn the concept of the target class in the absence of discriminating features.
    Examples of one-class classification are intrinsic plagiarism analysis and \ac{av}.
    Approaches to one-class classification fall into the following categories \cite{stein_intrinsic_2011}:
    \begin{itemize}
        \item One-class density estimation, e.g., Naive Bayes
        \item One-class boundary estimation
        \item One-class reconstruction
    \end{itemize}
\end{definition}

\begin{definition}
    [Open-set classification]
    The true author is not necessarily included in the set of candidate authors \cite{stamatatos_survey_2009}.
\end{definition}

\begin{definition}
    [Closed-set classification]
    The true author should be one of the candidate authors \cite{stamatatos_survey_2009,koppel_authorship_2011}.
\end{definition}

\begin{definition}
    [n-gram]
    $n$ contiguous words also known as word collocations.
\end{definition}

\begin{definition}
    [Space free n-gram]
    Removing spaces from the $n$-gram reduces the number of $n$-grams.
    \citet{koppel_authorship_2011} use these definitions:
    \begin{enumerate}
        \item a string of $n$ characters that not include spaces
        \item a string of less than $n$ characters that is surrounded by spaces
    \end{enumerate}
\end{definition}

\begin{definition}
    [Domain shift]
    Systematic statistical differences between the training and test sets \cite{tyo_state_2022}.
    These differences include:
    \begin{itemize}
        \item datasets are not identically distributed
        \item test set contains novel topics $\times_t$
        \item test set contains novel authors $\times_a$
        \item test set contains novel genres $\times_g$
    \end{itemize}
\end{definition}

\begin{definition}
    [Topic-confusion]
    In this setting, all topics appear in both training and test set. 
    However, the topics of the texts for each author changes in the test set.
    For example, in the training set, the author $A_1$ writes about topic $T_1$, author $A_2$ writes about topic $T_2$ 
    and in the test set, 
    the author $A_1$ writes about topic $T_2$ while author $A_2$ writes about topic $T_1$ \cite{tyo_state_2022}.
\end{definition}

\begin{definition}
    [text distortion]
    This domain-adversial method substitutes out-of-vocaublary items with asterisks $*$ \cite{tyo_state_2022}.
\end{definition}

\begin{definition}
    [Imposter method]
    This method extends the ngram-unmasking method, i.e. iteratively omitting most influencely features (repeated feature subsampling \cite{koppel_determining_2014})
    from a trained classifier and classifying the accuracy drop.
    It takes score of how often an author is predicted after each feature-elimination step.
    The final prediction is made based on this score \cite{tyo_state_2022}.
\end{definition}


\begin{definition}
    [Hard Negative Mining]
    This method updates the model during training only with the most difficult examples in each batch.
    In the \ac{aa} context, difficult is defined as the most similar two texts from different authors, 
    which makes the decision the most difficult.
    \citet{tyo_state_2022} claim that the \ac{av} setting is strictly easier since 
    it most compare to only a single text.
    Due to the fact, that the most difficult example is model-dependent, \ac{av} problems can be made harder 
    but they can not exist of exactly the hardest negatives.
\end{definition}
  
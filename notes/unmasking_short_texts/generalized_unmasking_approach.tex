\section{Generalised Unmasking Approach}
\label{sec:generalized_unmasking_approach}

\citet{bevendorff_generalizing_2019} prioritise precision, 
i.e.\ high confidence verification of authorship and thus, rejecting low confidence predictions.
As an example use-case where high precision is required, they mention wrong convictions.
They propose an \todo{open-source, customizable unmasking framework}. % cf.~bevendorff_generalizing_2019 for url
Moreover, they claim good performance on short text which inherently contain high amount of uncertainty since they have low stylistic information content.

\citet{bevendorff_generalizing_2019,bevendorff_divergence_based_2020} propose creating chunks by 
oversampling words in a bootstrap aggregating manner. 
Each text is a pool of words, from which words are sampled without replacement.
The pool is replenished if it is exhausted before the chunk has sufficiently many words.
Since the random sampling of unmasking features introduces variance, unmasking is performed multiple times and the curves are averaged.
The algorithm is displayed in \autoref{alg:generalized_unmasking}.
The content of the while loop is, except the number of removed features (\citep{koppel_authorship_2004}: 6 total), 
similar to the original unmasking algorithm \citep{koppel_authorship_2004}.

% prediction
Another linear \ac{svm} classifier is trained on the accuracy curve, its central-difference gradients (first- and second order), 
and its gradients sorted by magnitude.
This classifier is used to predict the whether the texts originate from the same author.

\begin{algorithm}
    \caption{Generalised Unmasking Algorithm \citep{bevendorff_generalizing_2019,bevendorff_divergence_based_2020}}
    \label{alg:generalized_unmasking}
    \begin{algorithmic}[1]
    \Procedure{Unmasking}{$A$, $B$}
        \Comment{$A$, $B$: input documents}
    
        \State $\mathcal{C}_A \gets \text{RandomChunks}(A, 30, 700)$ \Comment{30 chunks, 700 words each}
        \State $\mathcal{C}_B \gets \text{RandomChunks}(B, 30, 700)$
        \State $\mathcal{F} \gets \text{TopFreqWords}(A, B, 250)$
        \State $\mathcal{C} \gets \mathcal{C}_A \cup \mathcal{C}_B$

        
        \While{$|\mathcal{F}| \geq 0$}
        \State $a \gets \text{CVAcc}(\mathcal{C}_A, \mathcal{C}_B, \mathcal{F}, linSVM)$ \Comment{Append $10$-fold cross-validation accuracy}
        \State $\mathcal{F} \gets \mathcal{F} \setminus \mathcal{F}_{\text{top}}^{\pm}$ \Comment{Remove top $5$ most significant positive and negative features}
    
        \EndWhile
    
        \State \Return List of recorded accuracies $a$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}

% hyperparameters
The most important hyperparameters are the number of chunks, the number of words per chunk, the size of feature vectors, 
the number of word removals per round, and the number of averaged unmasking runs.
More chunks result in generally shallower curves while shorter features vectors or more removals produce steep curves.
Ideally, curves are not too steep and granular enough to allow distinguishing between different same and different author pairs.
\citet{bevendorff_bias_2019} recommend 25 to 50 chunks, vector sizes of 250 to 400 features, not fewer than 5, yet not more than 20 removals per round, 
between 500 and 700 words per chunk and about 10 runs to average for a curve.
They increase the minimal distance between the \ac{svm} hyperplane and the decision boundary, i.e.\ their confidence parameter $c$, to increase precision.
In a medium- to high-assurance scenario (where \acp{fp} should be avoided, but are not entirely critical), they recommend $c \geq 0.6$.
If \acp{fp} should be avoided at all costs, they recommend $c \geq 0.7$.
\citet{bevendorff_bias_2019} claim that, for this approach, hyperparameter tuning is simpler than for black box approaches.

% metric results
\citet{bevendorff_bias_2019} report that the generalised unmasking approach heavily prioritises precision 
opposed to compression-based approaches that balance precision and recall.
    
\chapter{Related Work}
\label{chap:related_work}

This work is different to the work of \cite{koppel_determining_2014} and \cite{kocher_unine_2015} 
in that it uses \acp{llm} to generate imposter texts.

% AV

% LLM detection using generative models
%% AA against LLMs
With the recent advances of \ac{nlg} come new challenges in text authorship.
The new technologies may be misused for fraudulent activities to scam naive or inexperienced users~\cite{uchendu_authorship_2020,bhattacharjee_fighting_2024}.
\citet{uchendu_authorship_2020} identified three authorship tasks essential for fighting fraudulent activities:
(1) Given two texts $t_1$ and $t_2$, determine whether they were produced by the same method (i.e. human author or a specific \ac{nlg} method).
(2) Given a text $t$, determine whether it was human authored or machine generated (Turing Test).
(3) Given a text $t$, find its author among $k+1$ candidates, which consists of one human and $k$ machines.
They compare classical \ac{ml} models, neural models and state-of-the-art \ac{aa} models as classifiers 
for these single- (Problem 1 and 2) and multi-class (Problem 3) tasks.
Their findings include, that as of 2020, most \ac{nlg} methods were distinguishable from human authors, 
but some \acp{llm} proved difficult to detect.
%%% compared to our work
In the following, we consider (1) \ac{av}, (2) classical \ac{llm} detection, and (3) closed-set \ac{aa}.
Our approach differs from the work of \citet{uchendu_authorship_2020} in that our candidates (i.e. imposters) do not include a human author (3), 
but only \acp{llm}.
Moreover, we use different classifiers originally designed for \ac{av}, rather than \ac{aa}.

%% LLM (gpt-3.5, GPT-4) as detector
\citet{bhattacharjee_fighting_2024} evaluate using an \ac{llm} as classifier for \ac{llm} detection.
They use \ac{gpt}-3.5 and \ac{gpt}-4 to classify texts as human or machine generated.
They find that \ac{gpt}-3.5 performs better when being fed simple instructions, rather than constrained prompts.
They find that \ac{gpt}-4 predicts almost exclusively \ac{ai} generated texts, while \ac{gpt}-3.5 predictions are more reliable (especially for actually human authored texts).
%%% compared to our work
Our work differs from theirs in that we use \acp{llm} to generate imposter texts specific to the candidate text, 
rather than using the publicly available dataset TuringBench with previously generated texts.

%% Perturb (Mask), score, compare: DetectGPT
\citet{mitchell_detectgpt_2023} propose DetectGPT, a method that is threefold:
(1) They perturb the input text by (1.1) masking out random 2-word spans until 15 \% of the text is masked. 
Masked spans are replaced (1.2) with words from an off-the-shelf (i.e. not finetuned to target domain) \ac{llm} (e.g. T5-3B). 
These perturbations are semantically similar rephrasings of the original text.
(2) They score (in terms of log probability) each perturbed text using a scoring \ac{llm} 
(ideally their candidate \ac{llm}, but it works also with any \ac{llm}, though scores deteriorate). 
(3) The difference of the score of the original text and the average score of the perturbed texts is denoted perturbation discrepancy $d$. 
(4) Normalize $d$ by the standard deviation of the scores of the perturbed texts.
(5) Based on a threshold $\epsilon=0.1$, classify the original text as human authored or machine generated 
(formally Local Perturbation Discrepancy Gap hypothesis).
If $d$ is positive, the original text is likely machine generated.
If $d$ is near zero, i.e. $d < \epsilon$, the original text is likely human authored.
\citet{mitchell_detectgpt_2023} motivate their method by the observation that generated texts tend to occupy 
negative curvature regions of the model's log probability function (i.e. they lie on the local maximum of the manifold).
When the text is machine generated, it lies on a local maximum, 
and perturbing it will lead to lower log probabilities of perturbed texts.
When the text is human authored, it does not lie on a local maximum to begin with, 
rendering log probabilities of perturbed texts similar either bigger or smaller than the original text.
Averaging the log probabilities of perturbed human texts leads to a value that is 
close to the original text's log probability (i.e. a perturbation discrepancy $d$ near zero).
Even though, DetectGPT works best when the source (i.e. generating) \ac{llm} and the scoring \ac{llm} are the same 
(requires white-box access to the \ac{llm}), 
it works also with different \acp{llm} as surrogate for the source model when scoring (in a black-box case).
%%% compared to our work
We can not supply a white box setting, because we do know the source \ac{llm} that generated the imposter texts.
%%%% imposters and perturbations
However, this approach is similar to our approach, because perturbing texts can be seen as a 
form of imposter generation (especially as we use paraphrases). 
%%%% sample from the source model
Both approaches try to sample from the probability distribution of the source model either 
by using imposters (via prompting an \ac{llm}) or by perturbing the original text (using an \ac{llm}).
%%%% input
While the imposter approach is an \ac{av} task (i.e. input is a disputed and a candidate text), 
DetectGPT receives a disputed text and a candidate \ac{llm} as input.
%%%% similarity measure
While we use a similarity measure on traditional n-gram frequency vectors, 
\citet{mitchell_detectgpt_2023} require a scoring \ac{llm} to compute the perturbation discrepancy $d$.
Hence, our approach is easier in terms of computational resources and requirements.

%% LLM rewrite LLM texts less than human texts (no AA, but edit distance hypothesis)
RAIDAR~\cite{mao_raidar_2024} builds upon the invariance property of \acp{llm}, which states that prompting an \ac{llm} to rewrite a machine generated text will introduce little change.
They motivate this by the observation that (different) autoregressive models produce similar patterns and thus, 
consider texts generated by (different) \acp{llm} as high quality that do not require rewriting.
Change is measured by the edit distance between the original text and the rewritten text. 
\citet{mao_raidar_2024} propose using an edit distance based on the Levenshtein distance or \ac{bow} representations.
RAIDAR operates on character level rather than using deep neural network features, and it does not require the original generating model for classification. 
RAIDAR fails to detect \ac{llm} generated texts in out-of-distribution scenarios (i.e. different domains than training), 
or when \ac{llm} were explicitly instructed to produce text prone to heavy \ac{llm} modification when being asked to rewrite the text \cite{li_learning_2025}.
Based on RAIDAR (\cite{mao_raidar_2024}), \citet{li_learning_2025} propose fine-tuning an \ac{llm} to rewrite human authored texts more than machine generated text.
Classification is carried out by comparing the edit distance of the original text and the rewritten text to a threshold.
\citet{li_learning_2025} admit that their approach is slow in inference time, since a candidate text has to be rewritten multiple times (about 200 different prompts) to obtain a reliable score.
%%% compared to our work
%%%% generation of texts during inference
Both approaches are similar to our work in that they use \acp{llm} to generate texts during inference.
We do not fine-tune an \ac{llm} for paraphrasing but use off-the-shelf models (like RAIDAR).
%%%% similarity measure
All these approaches compute the similarity of the original text and the generated text.
However, we do not use edit distance (i.e. Levenshtein distance) as similarity measure.
%%%% limitations
This approach is unable to detect which \ac{llm} generated the text.
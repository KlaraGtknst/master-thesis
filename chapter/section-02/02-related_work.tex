\chapter{Related Work}
\label{chap:related_work}

This work is different to the work of \cite{koppel_determining_2014} and \cite{kocher_unine_2015} 
in that it uses \acp{llm} to generate imposter texts.

% AV

% LLM detection using generative models
%% AA against LLMs
With the recent advances of \ac{nlg} come new challenges in text authorship.
The new technologies may be misused for fraudulent activities to scam naive or inexperienced users~\cite{uchendu_authorship_2020,bhattacharjee_fighting_2024}.
\citet{uchendu_authorship_2020} identified three authorship tasks essential for fighting fraudulent activities:
(1) Given two texts $t_1$ and $t_2$, determine whether they were produced by the same method (i.e. human author or a specific \ac{nlg} method).
(2) Given a text $t$, determine whether it was human authored or machine generated (Turing Test).
(3) Given a text $t$, find its author among $k+1$ candidates, which consists of one human and $k$ machines.
They compare classical \ac{ml} models, neural models and state-of-the-art \ac{aa} models as classifiers 
for these single- (Problem 1 and 2) and multi-class (Problem 3) tasks.
Their findings include, that as of 2020, most \ac{nlg} methods were distinguishable from human authors, 
but some \acp{llm} proved difficult to detect.
%%% compared to our work
In the following, we consider (1) \ac{av}, (2) classical \ac{llm} detection, and (3) closed-set \ac{aa}.
Our approach differs from the work of \citet{uchendu_authorship_2020} in that our candidates (i.e. imposters) do not include a human author (3), 
but only \acp{llm}.
Moreover, we use different classifiers originally designed for \ac{av}, rather than \ac{aa}.

%% LLM (gpt-3.5, GPT-4) as detector
\citet{bhattacharjee_fighting_2024} evaluate using an \ac{llm} as classifier for \ac{llm} detection.
They use \ac{gpt}-3.5 and \ac{gpt}-4 to classify texts as human or machine generated.
They find that \ac{gpt}-3.5 performs better when being fed simple instructions, rather than constrained prompts.
They find that \ac{gpt}-4 predicts almost exclusively \ac{ai} generated texts, while \ac{gpt}-3.5 predictions are more reliable (especially for actually human authored texts).
%%% compared to our work
Our work differs from theirs in that we use \acp{llm} to generate imposter texts specific to the candidate text, 
rather than using the publicly available dataset TuringBench with previously generated texts.

%% Perturb (Mask), score, compare: DetectGPT
\citet{mitchell_detectgpt_2023} propose DetectGPT, a method that is threefold:
(1) They perturb the input text by (1.1) masking out random 2-word spans until 15 \% of the text is masked. 
Masked spans are replaced (1.2) with words from an off-the-shelf (i.e. not finetuned to target domain) \ac{llm} (e.g. T5-3B). 
These perturbations are semantically similar rephrasings of the original text.
(2) They score (in terms of log probability) each perturbed text using a scoring \ac{llm} 
(ideally their candidate \ac{llm}, but it works also with any \ac{llm}, though scores deteriorate). 
(3) The difference of the score of the original text and the average score of the perturbed texts is denoted perturbation discrepancy $d$. 
(4) Normalize $d$ by the standard deviation of the scores of the perturbed texts.
(5) Based on a threshold $\epsilon=0.1$, classify the original text as human authored or machine generated 
(formally Local Perturbation Discrepancy Gap hypothesis).
If $d$ is positive, the original text is likely machine generated.
If $d$ is near zero, i.e. $d < \epsilon$, the original text is likely human authored.
\citet{mitchell_detectgpt_2023} motivate their method by the observation that generated texts tend to occupy 
negative curvature regions of the model's log probability function (i.e. they lie on the local maximum of the manifold).
When the text is machine generated, it lies on a local maximum, 
and perturbing it will lead to lower log probabilities of perturbed texts.
When the text is human authored, it does not lie on a local maximum to begin with, 
rendering log probabilities of perturbed texts similar either bigger or smaller than the original text.
Averaging the log probabilities of perturbed human texts leads to a value that is 
close to the original text's log probability (i.e. a perturbation discrepancy $d$ near zero).
Even though, DetectGPT works best when the source (i.e. generating) \ac{llm} and the scoring \ac{llm} are the same 
(requires white-box access to the \ac{llm}), 
it works also with different \acp{llm} as surrogate for the source model when scoring (in a black-box case).
%%% compared to our work
We can not supply a white box setting, because we do know the source \ac{llm} that generated the imposter texts.
%%%% imposters and perturbations
However, this approach is similar to our approach, because perturbing texts can be seen as a 
form of imposter generation (especially as we use paraphrases). 
%%%% sample from the source model
Both approaches try to sample from the probability distribution of the source model either 
by using imposters (via prompting an \ac{llm}) or by perturbing the original text (using an \ac{llm}).
%%%% input
While the imposter approach is an \ac{av} task (i.e. input is a disputed and a candidate text), 
DetectGPT receives a disputed text and a candidate \ac{llm} as input.
%%%% similarity measure
While we use a similarity measure on traditional n-gram frequency vectors, 
\citet{mitchell_detectgpt_2023} require a scoring \ac{llm} to compute the perturbation discrepancy $d$.
Hence, our approach is easier in terms of computational resources and requirements.
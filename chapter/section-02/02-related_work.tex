\chapter{Related Work}
\label{chap:related_work}

% where does my work fit in literetaure?
% why does my work matter?
% what gap is addressed by my work?

% start with classical, then recent and then my own work

% paraphrasing, unmasking, one class


% cross-domain
Studies~\citep{stamatatos_survey_2009,barlas_cross_domain_2020} have shown that character, function word and \ac{pos} n-grams are generally more topic-invariant than other stylometric features.
Nevertheless, \citet{bischoff_importance_2020} argue that frequent function words still exhibit a strong correlation with topic.
More broadly, many traditional stylometric features remain sensitive to domain variation, and even neural authorship models have been found to perform poorly in \ac{ood} settings~\citep{rivera_soto_learning_2021}.

Compression-based \ac{av} methods determine authorship by comparing the compression of a text in isolation with its compression alongside a partner text. 
Pairs exhibiting only small differences are identified as originating from the same author.
Various distance-based metrics are applied within this framework~\citep{elmanarelbouanani_authorship_2014,bevendorff_divergence_based_2020,bevendorff_overview_2024}, supported by different compression algorithms such as PPM, LZW, and GZIP. 
While conceptually elegant, compression methods tend to be computationally expensive, slow and less effective on large datasets~\citep{tyo_state_2022,neal_surveying_2018}. 

%% Meta learning and Unmasking
The unmasking approach reframes \ac{av} as a meta-learning problem. 
Unmasking iteratively removes the most discriminative features between two documents and observes how quickly classification accuracy degrades. 
If the texts are by the same author, performance collapses rapidly under feature removal and if not, accuracy remains relatively stable~\citep{koppel_authorship_2004}. 
Unmasking has been empirically effective, particularly on long documents such as novels~\citep{koppel_authorship_2011}. 
Its main drawback, however, is its dependence on large text volumes~\citep{koppel_determining_2014,bevendorff_generalizing_2019}.

Subsequent work~\citep{bevendorff_generalizing_2019,bevendorff_divergence_based_2020} has attempted to generalize unmasking to shorter texts by creating chunks by oversampling words in a bootstrap aggregating manner. 
Real-world applications such as plagiarism detection~\citep{stein_intrinsic_2011}, or pre-filtering hyperpartisan news from mainstream news~\citep{potthast_stylometric_2018} incorporate the unmasking approach.

% PAN metrics: F0.5u, Brier, C@1: tyo_state_2022

%% Impostor Method and Open-Set Identification
In the \impAppr{}, \citet{koppel_determining_2014} reduce the open-set \ac{av} problem to an open-set \ac{aa} problem.
They denote the latter many-candidates problem. 
Unlike closed-set attribution, where the true author is guaranteed to be among the candidates, open-set scenarios require distinguishing not only between multiple authors but also between "in-set" and "out-of-set" cases. 
The \ac{av} problem is reduced to the many-candidates problem via adding so-called impostor texts to the candidates pool. 
Instead of relying on fixed feature sets, it repeatedly samples random subsets of features and compares the target text against both candidate author and the set of \imps{}. 
By aggregating many such trials, the method estimates whether the candidate consistently outperforms the \imps{} across varying conditions~\citep{koppel_determining_2014}. 
The method's effectiveness depends heavily on the relevance of available impostor texts. 
This dependency motivates the use of synthetic \imps{} generated by \acp{llm}, which allow for better control over confounders.

% CLEF
With combining shared tasks such as \acs{pan} or ELOQUENT at CLEF into an adversarial Generative \ac{ai} \ac{av} task called Voight-Kampff inspired by the movie Blade Runner.
\acs{pan} participants built a detector, while ELOQUENT built systems that should fool the detectors into believing the machine generated texts were human authored.
\acs{pan} 2024 baselines included estimating text perplexities (for perturbated texts), or self-similarity assessments via Unmasking or PPMD approaches for chunks of the text.
\acs{pan} 2024 submissions included BERT based models with classifiers or generation probability prediction (similar to the baseline), as well as direct discriminators or LoRA \acp{llm}.
Most approaches proved unable to generalize across domains indicating that their approaches encoded domain-specific information.
They built an evaluation dataset using a two-step approach extracting bullet points and metadata from the original text via GPT-4 Turbo and subsequently generating artificial texts using different \acp{llm} based on this information.
Unlike this approach, we do not specific classes to choose from during metadata extraction, nor do we have only one text genre (here: news articles).
We were inspired by this approach, especially since one-step paraphrasing has been reported as not as difficult by \citet{bevendorff_overview_2024}.
They measured difficulty by the inverse mean detection score.


%% LLM-Based Authorship and Detection
% Recent advances in \acp{llm} have reshaped the landscape of \ac{av}. 
% The emergence of generative models introduces both a novel adversary and a valuable methodological resource.
% Detecting \ac{llm} generated text can be regarded as a special case of \ac{av}, where the author is a generative model~\citep{bevendorff_overview_2024}. 
% Reflecting this shift, the \ac{pan} 2024 shared task "Voight-Kampff" focused on generative \ac{ai} detection, challenging participants to develop systems capable of distinguishing between human and machine-authored texts~\citep{bevendorff_overview_2024,ayele_overview_2024}.
\citet{llm_detection_av_2025} observe that, although current \ac{llm} generated texts still exhibit detectable differences from human writing, such differences are expected to diminish as models continue to improve.
At the same time, \acp{llm} can serve as discriminators~\citep{futrzynski_pairwise_2021} or tools for augmenting \ac{av} methods, for instance through text generation, or paraphrasing~\citep{mao_raidar_2024,baradia_mirror_2025}.
Since text distortion has previously proven effective in cross-domain scenarios~\citep{bischoff_importance_2020}, \acp{llm} appear particularly promising as generators of controlled distortions in \ac{av} research.

% BERT: uitility for AV?
Neural \ac{av} models include the usage of transformer based models such as BERT.
Though BERT models are well-known for encoding topic information~\citep{sawatphol_cross_topic_av_24}, they have proven useful to distinguish direct speech of characters in novels from each other~\citep{michel_fictional_2024}.
One might argue that those characters inherently voice the style of the novel's author and thus, the only point separating them is the semantic difference between their speeches.
It becomes clear that different verification tasks require different tools.

\Ac{llm} detection methods can be grouped into metric-based approaches, fine-tuned classifiers, and watermarking. 
Among these, DetectGPT is an influential example for a metric-based approach~\citep{wang_stumbling_2024}.
DetectGPT assumes that artificial generated text corresponds to regions of high model log probability. 
By perturbing candidate texts and rescoring them, the method measures the difference of their respective probabilities.
If it is near zero, the text is likely human authored~\citep{mitchell_detectgpt_2023}. 
While powerful, DetectGPT is limited to white-box scenarios %(access to model probabilities) 
and is vulnerable to paraphrases or detecting unseen models~\citep{Wu_ODD_challenges_2025}.

\citet{schmidt_llm_av_latin_24} have evaluated the zero-shot performance of \acl{sota} \acp{llm} for \ac{av} and \ac{aa} for low-resource languages.
Even though they consider Latin high resource historic language, the authors state that the number of available data is limited.
Models included GPT-4o, Gemini-1.5, Mistral-Large, and Claude.
They found that more detailed prompts improved precision and identification of syntactic features, but let to \acp{llm} assuming a deterministic relationship between the presence of features and the final prediction, and thus, deteriorated performance.
This was visible especially for larger models such as GPT-4o and Claude, since they use their extensive intrinsic knowledge more freely with less constrained prompts.
Even though the performance was good, \acp{llm} struggle with discerning writing style independently of the content~\citep{schmidt_llm_av_latin_24}.
\citet{nguyen_bert_topic_av_2023} also found that transformer based methods relied on topical information rather than authorship characteristics.

There are also initiatives to use \acp{llm} for improving explainability of \ac{av} methods.
\citet{hung_xai_av_llm_2023} compare different prompts for GPT-4 in zero- and few-shot scenarios.
They note that requesting continuous confidence scores rather than binary outputs reduces a bias towards "no".
While the accuracy of their approaches ranges from $0.51$ to $0.67$, they observed that the explanation of \acp{llm} sometimes contains hallucinations.
Besides hallucinations, \citet{ramnath_cave_xai_llm_2025} disclose also issues of propagated bias and incompleteness with their post-hoc explanations.
Due to the lack of human written explanations for \ac{av} decisions, they create a silver dataset with GPT-4-Turbo as an oracle and subsequently use it to train their CAVE model on.
All biases present in GPT-4-Turbo therefore propagate to CAVE.
 
\citet{nogueira_doc2query_2019} propose Doc2query, a document expansion strategy to mitigate the effects of missing relevant documents during keyword based information retrieval due to term mismatched.
Their method consists of a sequence-to-sequence transformer model that predicts a query given an input document.
Though their approach applies to information retrieval rather than our use case, the usage of a pre-trained model to extract a query a human wants to have answered by the document aligns with our \pextractor{} in the two-step paraphrasing approach.
This approach, however, does not rely on the predicted query capturing a comprehensive image of the document but only few relevant points.
Hence, our approach needs to enhance this idea.

Other detection methods also make use of rewriting. 
Paraphrases can be generated through rule-based or thesaurus-driven substitutions, by means of monolingual machine translation~\citep{zhou_paraphrase_2021}, or by explicitly prompting generative T5 or GPT models~\citep{kurt_pehlivanoglu_comparative_2024}.
RAIDAR, for instance, generates perturbed versions of a text using paraphrasing and compares edit distances~\citep{mao_raidar_2024}, while Mirror Minds employs paraphrasing combined with syntactic similarity metrics such as BLEU and METEOR~\citep{baradia_mirror_2025}. 
Based on RAIDAR, \citet{li_learning_2025} propose fine-tuning an \ac{llm} to rewrite human authored texts more than machine generated text.
%%% compared to our work: 
All of these approaches are similar to ours in that they leverage \acp{llm} to generate texts at inference time. 
However, unlike \citet{li_learning_2025}, we do not fine-tune an \ac{llm} for paraphrasing, instead relying on off-the-shelf models.
These methods also generally compute the similarity between the original and generated texts. 
Unlike them, we do not use conventional metrics such as edit distance, BLEU, or METEOR, nor do we perform comparisons at the paraphrase level (e.g., BLEU measures n-gram overlap). 
Instead, we construct frequency-based n-gram vectors and apply similarity metrics.
Finally, a limitation of these approaches is that they cannot identify which \ac{llm} produced a given text.

% Paraphrasing and Evaluation Metrics
Paraphrasing undermines detectors while also offering methodological utility. 
Systems like DIPPER~\citep{Krishna_dipper_2023} demonstrate how adversarial rewriting can bypass detection. 
At the same time, paraphrasing has been leveraged as a tool for generating stylistic variants~\citep{mao_raidar_2024,baradia_mirror_2025}.

% Evaluating paraphrases remains an open challenge. 
% Paraphrases evaluation can include dimensions of readability, complexity and grade level~\citep{Thomas_cross_topic_24}.
% They use Textstat, a Python library that helps extract statistics from text.
% Traditional metrics such as BLEU~\citep{papineni_bleu_2001}, ROUGE~\citep{lin_rouge_2004}, METEOR~\citep{banerjee_METEOR_2005}, and GLEU~\citep{kurt_pehlivanoglu_comparative_2024} capture n-gram overlap but often fail to reflect semantic adequacy. 
% Embedding-based metrics such as BERTScore~\citep{hanna_fine_grained_2021} and Word Mover's Distance~\citep{gohsen_captions_2023} offer more robust semantic similarity estimates, while human evaluation remains the gold standard~\citep{zhou_paraphrase_2021}. 
% Despite this, no consensus exists on the best metric for evaluating paraphrasing. 
% This uncertainty motivates further exploration of similarity measures for evaluating paraphrases.


% % LLM detection using generative models
% %% AA against LLMs

% \citet{uchendu_authorship_2020} identified three authorship tasks essential for fighting fraudulent activities:
% (1) Given two texts $t_1$ and $t_2$, determine whether they were produced by the same method (i.e. human author or a specific \ac{nlg} method).
% (2) Given a text $t$, determine whether it was human authored or machine generated (Turing Test).
% (3) Given a text $t$, find its author among $k+1$ candidates, which consists of one human and $k$ machines.

% %%% compared to our work
% In the following, we consider (1) \ac{av}, (2) classical \ac{llm} detection, and (3) closed-set \ac{aa}.
% Our approach differs from the work of \citet{uchendu_authorship_2020} in that our candidates (i.e. \imps{}) do not include a human author (3), 
% but only \acp{llm}.
% Moreover, we use different classifiers originally designed for \ac{av}, rather than \ac{aa}.



% %% LLM rewrite LLM texts less than human texts (no AA, but edit distance hypothesis)
% RAIDAR~\citep{mao_raidar_2024} builds upon the invariance property of \acp{llm}, 
% which states that prompting an \ac{llm} to rewrite a machine generated text will introduce little change.
% They motivate this by the observation that (different) autoregressive models produce similar patterns and thus, 
% consider texts generated by (different) \acp{llm} as high quality that do not require rewriting.
% Change is measured by the edit distance between the original text and the rewritten text. 
% \citet{mao_raidar_2024} propose using an edit distance based on the Levenshtein distance or \ac{bow} representations.
% RAIDAR operates on character level rather than using deep neural network features, and it does not require the original generating model for classification. 
% Classification is carried out by comparing the edit distance of the original text and the rewritten text to a threshold.


% %% LLMDet: Proxy to perplexity (problem: requires access to the LLM to build the dictionary)
% Perplexity is a reliable statistical metric for attributing texts to \acp{llm}~\citep{zhang_llmdet_2023}.
% Unfortunately, perplexity requires access to \acp{llm}' parameters (i.e., white-box detection).
% \citet{wu_llmdet_2023} propose LLMDet, a method that uses a proxy to perplexity, 
% where a dictionary of frequent n-gram (frequent among $n$ randomly prompted generated texts per \ac{llm}) 
% next token probabilities is pre-computed (i.e. requiring access to the \ac{llm}), 
% and is subsequently used during inference to approximate perplexity by replacing $x_{<i}$ in $p(x_i | x_{<i})$ with an n-gram.
% Since the construction of the dictionary requires access to the \ac{llm}, LLMDet requires contribution of the closed-source model owners.
% The disputed text is tokenized and the proxy perplexity is calculated for each model and thus, constructing a proxy perplexity vector.
% This vector is input to a trained classifier.
% %%% compared to our work
% Proxy perplexity could be used as a baseline for our approach, though it requires access to the \ac{llm} and is thus not applicable in our case.

%% AV/ AA via LLMs

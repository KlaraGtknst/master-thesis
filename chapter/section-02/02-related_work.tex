\chapter{Related Work}
\label{chap:related_work}

We distinguish traditional \ac{av} scenarios where both texts of an input pair were human authored from more recently advancing scenarios where at least one of the texts was artificially generated by an \ac{llm}.
Aligned with \citet{llm_detection_av_2025}, we consider \acp{llm} authors due to their fast advances in writing like an human.
Therefore, when reviewing prior work, we considered both \ac{av} approaches in the traditional human-human sceanrio and those, which propose ideas to tackle to \ac{llm} detection which can be framed as an \ac{av} task.
Since this work focuses on evaluation of a \ac{llm}-based extension to the traditional \impAppr{}~\citep{koppel_determining_2014}, we focus on the traditional \ac{av} scenario for evaluation but still consider advancements in the more recent \ac{av} scenario to enhance our methodology.

% CLEF
As a response to bespoken \ac{gai} advances, combined shared tasks from \acs{pan} and \acs{eloquent} at \acs{clef} such as Voight-Kampff have emerged.
This adversarial \ac{gai} \ac{av} task requires \acs{pan} participants built a detector, while \acs{eloquent} participants built systems that should fool the detectors into believing the machine generated texts were human authored.
This track showcases that traditional approaches remain relevant since the \acs{pan} 2024 baselines included \unmasking{}~\citep{koppel_authorship_2004,koppel_authorship_2011,bevendorff_generalizing_2019,bevendorff_divergence_based_2020} and \ac{ppmd} approaches~\citep{tyo_state_2022,neal_surveying_2018}.
It becomes also clear that participants have adapted to the trend of using neural models for \ac{av} tasks, as \acs{pan} 2024 submissions included \acs{bert}-based models with classifiers or generation probability prediction, as well as direct discriminators or LoRA \acp{llm}.
However, most approaches proved unable to generalize across domains indicating that their approaches encoded domain-specific information~\citep{bevendorff_overview_2024}.

This limitation of \ac{llm}-based approaches in an \ac{ood} setting is also apparent in other work~\citep{rivera_soto_learning_2021}.
\citet{schmidt_llm_av_latin_24}\ evaluated the zero-shot effectiveness of \acl{sota} \acp{llm} for \ac{av} and \ac{aa} in low-resource languages, using models such as GPT-4o, Gemini-1.5, Mistral-Large, and Claude. 
Although overall effectiveness was reasonable, \acp{llm} still struggle to distinguish writing style independently of content~\citep{schmidt_llm_av_latin_24}. 
Similarly, both \citet{nguyen_bert_topic_av_2023}\ and \citet{sawatphol_cross_topic_av_24}\ found that transformer-based methods, such as\ac{bert}, strongly encode topical information and thus, often rely on topical information rather than genuine authorship characteristics.
However, neural \ac{av} models can still be effective in specific applications. 
For example, transformer-based models have successfully distinguished the direct speech of different characters in novels~\citep{michel_fictional_2024}. 
In this setting, the characters inherently reflect the style of the novelâ€™s author, meaning that the primary distinguishing factor between texts is semantic rather than stylistic.

Again, prioir reserach underlines the relevance of tarditional non-nerual approaches, since they have proven to be domain-invariant.
Studies assessing the robustness of existing features and approaches~\citep{stamatatos_survey_2009,barlas_cross_domain_2020} have shown that character, function word and \ac{pos} n-grams are generally more topic-invariant than other stylometric features.
Nevertheless, \citet{bischoff_importance_2020}\ argue that frequent function words still exhibit a strong correlation with topic.
Many traditional stylometric features remain sensitive to domain variation~\citep{rivera_soto_learning_2021}.

This may suggest that \acp{llm} alone may not be equiepped yet as standalone discriminators in \ac{av} tasks, but their strengths may be valuable to make tarditional approaches more robust to \ac{ood} settings or other improvements.
% XAI
There are also initiatives to use \acp{llm} for improving explainability of \ac{av} methods.
\citet{hung_xai_av_llm_2023}\ compare different prompts for GPT-4 in zero- and few-shot scenarios.
They note that requesting continuous confidence scores rather than binary outputs reduces a bias towards different author predicitions.
While the accuracy of their approaches ranges from $0.51$ to $0.67$, they observed that the explanation of \acp{llm} sometimes contains hallucinations.
Similarly, \citet{ramnath_cave_xai_llm_2025}\ report hallucinations and issues of propagated bias and incompleteness with their post-hoc explanations.
Due to the lack of human written explanations for \ac{av} decisions, they created a silver dataset with GPT-4-Turbo as an oracle and subsequently using it to train their CAVE model on.
All biases present in GPT-4-Turbo therefore propagate to CAVE.

\acp{llm} can serve as discriminators for \ac{llm} detection~\citep{futrzynski_pairwise_2021} or tools for augmenting \ac{av} methods, for instance through text generation, or paraphrasing~\citep{mao_raidar_2024,baradia_mirror_2025}.
Since text distortion has previously proven effective in cross-domain scenarios~\citep{bischoff_importance_2020}, \acp{llm} appear particularly promising as generators of controlled distortions in \ac{av} research.

Another very promising application of \acp{llm} is to leverage just what makes them an author in more current \ac{av} scenarios: Their abaility to imitate human writing style.
DetectGPT assumes that artificial generated text corresponds to regions of high model log probability. 
By perturbing candidate texts and rescoring them, the method measures the difference of their respective probabilities.
If it is near zero, the text is likely human authored~\citep{mitchell_detectgpt_2023}. 
While powerful, DetectGPT is limited to white-box scenarios %(access to model probabilities) 
and is vulnerable to paraphrases or detecting unseen models~\citep{Wu_ODD_challenges_2025}.
Notably, its perturbation strategy bears similarity to our paraphrasing approach.

We also found prioir work informing our two-step paraphraser.
\citet{nogueira_doc2query_2019}\ introduce Doc2query, a document expansion method addressing term mismatch in keyword-based information retrieval by predicting queries from documents using a sequence-to-sequence transformer. 
Instead of producing a full document representation, Doc2query generates a single query that captures only a few salient points. 
In a related direction, \citet{lee_gecko_2024}\ propose Gecko, a transformer-based \ac{lm} distilled from \acp{llm}, which uses query and task generation for finetuning. 
Unlike our approach, their generated queries are not constrained to the input text but are designed to retrieve documents that better answer the query than the original text itself. 
While both methods focus on information retrieval, they share with our framework the idea of extracting plausible human queries from documents. 
Our two-step paraphrase generation component \pextractor{} extends this idea by aiming for a more complete and document-specific representation.

Even better analysis and subsequent usage of \ac{llm} abilities for paraphrasing were showcased by
\acs{pan} for the construction of an evaluation dataset of news articles using a two-step procedure.
First, bullet points and metadata were extracted from the original texts using GPT-4 Turbo.
Second, artificial texts were generated by various \acp{llm} based on this extracted information. 
This methodology is particularly relevant to our work, as it motivated using a two-step approach to paraphrasing. 
In \ac{llm} detection, one-step paraphrasing is considered less challenging, with \citet{bevendorff_overview_2024} quantifying difficulty as the inverse of the mean detection score.
Unlike \acs{pan}, our approach does not restrict metadata extraction to a predefined set of classes, nor does it focus on a single text genre. 

Other work already used \ac{llm} rewriting and combined it with tarditonal syntactic scores:
Other \ac{llm} detection methods also make use of rewriting. 
Paraphrases can be generated through rule-based or thesaurus-driven substitutions, by means of monolingual machine translation~\citep{zhou_paraphrase_2021}, or by explicitly prompting generative T5 or GPT models~\citep{kurt_pehlivanoglu_comparative_2024}.
RAIDAR, for instance, generates perturbed versions of a text using paraphrasing and compares edit distances for predictions~\citep{mao_raidar_2024}, while \mirrorMinds{} employs paraphrasing combined with syntactic similarity metrics such as BLEU and METEOR~\citep{baradia_mirror_2025}. 
While their idea conceptually is similar to ours, we found that the \mirrorMinds{} approach produced bad results due to extracting questions that were anserwed by one word rather than a paraphrase during their two-step paraphrasing process.
Based on RAIDAR, \citet{li_learning_2025}\ propose fine-tuning an \ac{llm} to rewrite human authored texts more than machine generated text.
%%% compared to our work: 
All of these approaches are similar to ours in that they leverage \acp{llm} to generate texts at inference time. 
However, unlike \citet{li_learning_2025}, we do not fine-tune an \ac{llm} for paraphrasing, instead relying on off-the-shelf models.
These methods also generally compute the similarity between the original and generated texts. 
Unlike them, we do not use conventional metrics such as edit distance, BLEU, or METEOR, nor do we perform comparisons at the paraphrase level (e.g., BLEU measures n-gram overlap). 
Instead, we construct frequency-based n-gram vectors and apply similarity metrics.


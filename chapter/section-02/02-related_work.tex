\chapter{Related Work}
\label{chap:related_work}

This work is different to the work of \citet{koppel_determining_2014} and \citet{kocher_unine_2015} 
in that it uses \acp{llm} to generate \imp{} texts.

\textcolor{red}{TODO: Aufschl√ºsseln was hier behandelt wird}
% \section{Traditional \acs{llm} Detection}
% \label{sec:llm_detection}
\textcolor{red}{TODO: LLM Detection chapter here or in Ausblick: Falls wenig Experimente, dann nicht hier}
\input{chapter/section-02/perplexity.tex}

% AV
% LLM detection using generative models
%% AA against LLMs
With the recent advances of \ac{nlg} come new challenges in text authorship.
The new technologies may be misused for fraudulent activities to scam naive or inexperienced users~\citep{uchendu_authorship_2020,bhattacharjee_fighting_2024}.
\citet{uchendu_authorship_2020} identified three authorship tasks essential for fighting fraudulent activities:
(1) Given two texts $t_1$ and $t_2$, determine whether they were produced by the same method (i.e. human author or a specific \ac{nlg} method).
(2) Given a text $t$, determine whether it was human authored or machine generated (Turing Test).
(3) Given a text $t$, find its author among $k+1$ candidates, which consists of one human and $k$ machines.
They compare classical \ac{ml} models, neural models and state-of-the-art \ac{aa} models as classifiers 
for these single- (Problem 1 and 2) and multi-class (Problem 3) tasks.
Their findings include, that as of 2020, most \ac{nlg} methods were distinguishable from human authors, 
but some \acp{llm} proved difficult to detect.
%%% compared to our work
In the following, we consider (1) \ac{av}, (2) classical \ac{llm} detection, and (3) closed-set \ac{aa}.
Our approach differs from the work of \citet{uchendu_authorship_2020} in that our candidates (i.e. \imps{}) do not include a human author (3), 
but only \acp{llm}.
Moreover, we use different classifiers originally designed for \ac{av}, rather than \ac{aa}.

There are different categories of \ac{llm} detectors-
Metric-based detectors classify based on a threshold and the inferred log-probability from the generator \ac{llm}.
Examples inlcude GLTR, Range, LogRank and DetectGPT.
Fine-tuned detectors are pretrained \acp{lm} in a supervised scenario.
Examples include the OpenAI Detector.
Watermark-based detectors add algorithmically detectable signatures into the text during generation~\citep{wang_stumbling_2024}.

%% LLM (gpt-3.5, GPT-4) as detector
\citet{bhattacharjee_fighting_2024} evaluate using an \ac{llm} as classifier for \ac{llm} detection.
They use \ac{gpt}-3.5 and \ac{gpt}-4 to classify texts as human or machine generated.
They find that \ac{gpt}-3.5 performs better when being fed simple instructions, rather than constrained prompts.
They find that \ac{gpt}-4 predicts almost exclusively \ac{ai} generated texts, 
while \ac{gpt}-3.5 predictions are more reliable (especially for actually human authored texts).
%%% compared to our work
Our work differs from theirs in that we use \acp{llm} to generate \imp{} texts specific to the candidate text, 
rather than using the publicly available dataset TuringBench with previously generated texts.

%% DetectGPT: Perturb (Mask), score, compare (unsupervised)
\citet{mitchell_detectgpt_2023} propose DetectGPT, a method that is threefold:
(1) They perturb the input text by (1.1) masking out random 2-word spans until 15 \% of the text is masked. 
Masked spans are replaced (1.2) with words from an off-the-shelf (i.e. not finetuned to target domain) \ac{llm} (e.g. T5-3B). 
These perturbations are semantically similar paraphrases of the original text.
(2) They score (in terms of log probability) each perturbed text using a scoring \ac{llm} 
(ideally their candidate \ac{llm}, but it works also with any \ac{llm}, though scores deteriorate). 
(3) The difference of the score of the original text and the average score of the perturbed texts is denoted perturbation discrepancy $d$. 
(4) Normalize $d$ by the standard deviation of the scores of the perturbed texts.
(5) Based on a threshold $\epsilon=0.1$, classify the original text as human authored or machine generated 
(formally Local Perturbation Discrepancy Gap hypothesis).
If $d$ is positive, the original text is likely machine generated.
If $d$ is near zero, i.e. $d < \epsilon$, the original text is likely human authored.
\citet{mitchell_detectgpt_2023} motivate their method by the observation that generated texts tend to occupy 
negative curvature regions of the model's log probability function (i.e. they lie on the local maximum of the manifold).
When the text is machine generated, it lies on a local maximum, 
and perturbing it will lead to lower log probabilities of perturbed texts.
When the text is human authored, it does not lie on a local maximum to begin with, 
rendering log probabilities of perturbed texts similar either bigger or smaller than the original text.
Averaging the log probabilities of perturbed human texts leads to a value that is 
close to the original text's log probability (i.e. a perturbation discrepancy $d$ near zero).
Even though, DetectGPT works best when the source (i.e. generating) \ac{llm} and the scoring \ac{llm} are the same 
(requires white-box access to the \ac{llm}), 
it works also with different \acp{llm} as surrogate for the source model when scoring (in a black-box case).
%%% compared to our work
We can not supply a white box setting, because we do know the source \ac{llm} that generated the \imp{} texts.
%%%% \imps{} and perturbations
However, this approach is similar to our approach, because perturbing texts can be seen as a 
form of \imp{} generation (especially as we use paraphrases). 
%%%% sample from the source model
Both approaches try to sample from the probability distribution of the source model either 
by using \imps{} (via prompting an \ac{llm}) or by perturbing the original text (using an \ac{llm}).
%%%% input
While the \impAppr{} is an \ac{av} task (i.e. input is a disputed and a candidate text), 
DetectGPT receives a disputed text and a candidate \ac{llm} as input.
%%%% similarity measure
While we use a similarity measure on traditional n-gram frequency vectors, 
\citet{mitchell_detectgpt_2023} require a scoring \ac{llm} to compute the perturbation discrepancy $d$.
Hence, our approach is easier in terms of computational resources and requirements.
Research building on DetectGPt finds that the whitebox approach is vulnerable to unknown models, especiall GPT-3.5-Turbo~\citep{Wu_ODD_challenges_2025}.

%% LLM rewrite LLM texts less than human texts (no AA, but edit distance hypothesis)
RAIDAR~\citep{mao_raidar_2024} builds upon the invariance property of \acp{llm}, 
which states that prompting an \ac{llm} to rewrite a machine generated text will introduce little change.
They motivate this by the observation that (different) autoregressive models produce similar patterns and thus, 
consider texts generated by (different) \acp{llm} as high quality that do not require rewriting.
Change is measured by the edit distance between the original text and the rewritten text. 
\citet{mao_raidar_2024} propose using an edit distance based on the Levenshtein distance or \ac{bow} representations.
RAIDAR operates on character level rather than using deep neural network features, and it does not require the original generating model for classification. 
RAIDAR fails to detect \ac{llm} generated texts in out-of-distribution scenarios (i.e. different domains than training), 
or when \ac{llm} were explicitly instructed to produce text prone to heavy \ac{llm} modification when being asked to rewrite the text \citep{li_learning_2025}.
Based on RAIDAR (\citep{mao_raidar_2024}), \citet{li_learning_2025} propose fine-tuning an \ac{llm} to rewrite human authored texts more than machine generated text.
Classification is carried out by comparing the edit distance of the original text and the rewritten text to a threshold.
\citet{li_learning_2025} admit that their approach is slow in inference time, 
since a candidate text has to be rewritten multiple times (about 200 different prompts) to obtain a reliable score.
\citet{mao_raidar_2024} find that the quality of perturbation based models (i.e. rewriting) for \ac{llm} detection correlates with the perturbation model size.
\citet{mitchell_detectgpt_2023} find a negative correlation (\textcolor{red}{TODO: chapter 2 vorletzter Absatz}) between the size of the perturbation model and the performance of DetectGPT.
%%% compared to our work
%%%% generation of texts during inference
Both approaches are similar to our work in that they use \acp{llm} to generate texts during inference.
We do not fine-tune an \ac{llm} for paraphrasing but use off-the-shelf models (like RAIDAR).
%%%% similarity measure
All these approaches compute the similarity of the original text and the generated text.
However, we do not use edit distance (i.e. Levenshtein distance) as similarity measure.
%%%% limitations
This approach is unable to detect which \ac{llm} generated the text.

%% LLMDet: Proxy to perplexity (problem: requires access to the LLM to build the dictionary)
Perplexity is a reliable statistical metric for attributing texts to \acp{llm}~\citep{zhang_llmdet_2023}.
Unfortunately, perplexity requires access to \acp{llm}' parameters (i.e., white-box detection).
\citet{wu_llmdet_2023} propose LLMDet, a method that uses a proxy to perplexity, 
where a dictionary of frequent n-gram (frequent among $n$ randomly prompted generated texts per \ac{llm}) 
next token probabilities is pre-computed (i.e. requiring access to the \ac{llm}), 
and is subsequently used during inference to approximate perplexity by replacing $x_{<i}$ in $p(x_i | x_{<i})$ with an n-gram.
Since the construction of the dictionary requires access to the \ac{llm}, LLMDet requires contribution of the closed-source model owners.
The disputed text is tokenized and the proxy perplexity is calculated for each model and thus, constructing a proxy perplexity vector.
This vector is input to a trained classifier.
%%% compared to our work
Proxy perplexity could be used as a baseline for our approach, though it requires access to the \ac{llm} and is thus not applicable in our case.

%% Mirror Minds: extract query, genrate two paraphrases, compare & classify via threshold (very similar to our work)
\citet{baradia_mirror_2025} propose (1) extracting a query from the disputed text, which captures the essence of the text, 
(2) generating two paraphrases of the original text using the query as input prompt to two \acp{llm}, 
and (3) comparing the paraphrases to the original text via the BLEU and the METEOR score.
Both score capture syntactic similarity, even though \citet{baradia_mirror_2025} argue they also capture semantic similarity.
They use the maximum across the two models per similarity measure as a final score pair.
Classification of the resemblance to \ac{ai} generated content requires a threshold.
%%% compared to our work
%%%% same approach
This approach is similar to our approach in that it uses \acp{llm} to generate paraphrases of the original text.
Moreover, it compares the original text to the generated paraphrases as in a \ac{aa} problem. % rather AI detection?????
%%%% similarity measure
We do not use BLEU or METEOR as similarity measure, nor do we compare directly on paraphrase-level (i.e. BLEU calculates n-gram overlap) 
but construct our own frequency based n-gram vectors input vector similarity metrics.
%%%% they discard information, solve another problem
However, this approach discards the information which \ac{llm} produced the most similar paraphrase. 
While our goal is to solve an \ac{aa} problem (i.e. multiclass classification), 
\citet{baradia_mirror_2025} solve a binary classification problem (i.e. human vs. \ac{ai} generated text).


% Paraphrasing
\citet{Krishna_dipper_2023} have found that \acl{sota} \ac{llm} detectors are vulnerable to paraphrasing attacks.
These adversarial attacks paraphrase the text such that the semantic content remains the same, but the syntactic and lexical features distributions change~\citep{wang_stumbling_2024}.
The 11B parameter paraphrase generation model (DIPPER) manages to evade several detectors, including DetectGPT and OpenAI's text classifier.
DIPPER is explicitely trained to be a paraphraser~\citep{Krishna_dipper_2023}.
\citet{Sadasivan_t5_2023} employ DIPPER, LLaMA-2-7B-Chat with 7B parameters and T5-based paraphraser with 222M parameters as paraphrasers.
The first two provide one-shot paraphrases while the T5 generates paraphrases in a sentence by sentence fashion.
LLaMA-2-7B-Chat is an instruction-tuned model for chat purposes which is instructed to be a paraphraser via a system prompt.
This system prompt instructs the model to generate diverse outputs while maintaining text length, text quality and grammar~\citep{Sadasivan_t5_2023}.
We opted to omit explicit specification of diversity in our instruction, because for future application of our method to \ac{llm} detection, we want to capture the \ac{llm}'s style properties.
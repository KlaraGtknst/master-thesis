\chapter{Related Work}
\label{chap:related_work}

% where does my work fit in literetaure?
% why does my work matter?
% what gap is addressed by my work?

% start with classical, then recent and then my own work

% paraphrasing, unmasking, one class

Classical \ac{av} methods include character-based compression methods~\citep{tyo_state_2022,neal_surveying_2018}, and meta-learning techniques such as traditional~\citep{koppel_authorship_2004,koppel_authorship_2011} or generalized \unmasking{}~\citep{bevendorff_generalizing_2019,bevendorff_divergence_based_2020}.
The \impAppr{} utilizes hard negative texts to distinguish same- and different author pairs~\citep{koppel_determining_2014}.

% CLEF
Reacting to advances in \ac{gai}, more advanced methodas emerge from Voight-Kampff, a shared task combining \acs{pan} or ELOQUENT at CLEF into an adversarial \ac{gai} \ac{av} task .
\acs{pan} participants built a detector, while ELOQUENT built systems that should fool the detectors into believing the machine generated texts were human authored.
\acs{pan} 2024 baselines included estimating text perplexities (for perturbated texts), and self-similarity assessments via \unmasking{} or PPMD approaches for chunks of the text.
\acs{pan} 2024 submissions included BERT based models with classifiers or generation probability prediction (similar to the baseline), as well as direct discriminators or LoRA \acp{llm}.
Most approaches proved unable to generalize across domains indicating that their approaches encoded domain-specific information~\citep{bevendorff_overview_2024}.

\Ac{llm} detection methods can be grouped into metric-based approaches, fine-tuned classifiers, and watermarking. 
Among these, DetectGPT is an influential example for a metric-based approach~\citep{wang_stumbling_2024}.
DetectGPT assumes that artificial generated text corresponds to regions of high model log probability. 
By perturbing candidate texts and rescoring them, the method measures the difference of their respective probabilities.
If it is near zero, the text is likely human authored~\citep{mitchell_detectgpt_2023}. 
While powerful, DetectGPT is limited to white-box scenarios %(access to model probabilities) 
and is vulnerable to paraphrases or detecting unseen models~\citep{Wu_ODD_challenges_2025}.
\textcolor{red}{There are parallels to our approach.}

\citet{schmidt_llm_av_latin_24} evaluated the zero-shot performance of \acl{sota} \acp{llm} for \ac{av} and \ac{aa} in low-resource languages, using models such as GPT-4o, Gemini-1.5, Mistral-Large, and Claude. 
They observed that more detailed prompts improved precision and enhanced the identification of syntactic features. 
However, such prompts also led the models to assume a deterministic relationship between specific features and the final prediction, which degraded overall performance. 
This effect was particularly pronounced for larger models like GPT-4o and Claude, which can leverage their extensive intrinsic knowledge more freely under less constrained prompts. 
Although overall performance was reasonable, \acp{llm} still struggle to distinguish writing style independently of content~\citep{schmidt_llm_av_latin_24}. 
Similarly, \citet{nguyen_bert_topic_av_2023} found that transformer-based methods often rely on topical information rather than genuine authorship characteristics.

\ac{pan} constructed an evaluation dataset of news articles using a two-step procedure.
First, bullet points and metadata were extracted from the original texts using GPT-4 Turbo.
Second, artificial texts were generated by various \acp{llm} based on this extracted information. 
This methodology is particularly relevant to our work, as it motivated using a two-step approach to paraphrasing. 
The one-step paraphrasing has been reported as less challenging by \citet{bevendorff_overview_2024}, who quantified difficulty as the inverse of the mean detection score.
Unlike \ac{pan}, our approach does not restrict metadata extraction to a predefined set of classes, nor does it focus on a single text genre. 

% cross-domain
Studies assessing the robustness of existing features and approaches~\citep{stamatatos_survey_2009,barlas_cross_domain_2020} have shown that character, function word and \ac{pos} n-grams are generally more topic-invariant than other stylometric features.
Nevertheless, \citet{bischoff_importance_2020} argue that frequent function words still exhibit a strong correlation with topic.
Many traditional stylometric features remain sensitive to domain variation, and even neural authorship models have been found to perform poorly in \ac{ood} settings~\citep{rivera_soto_learning_2021}.

\acp{llm} can serve as discriminators for \ac{llm} detection~\citep{futrzynski_pairwise_2021} or tools for augmenting \ac{av} methods, for instance through text generation, or paraphrasing~\citep{mao_raidar_2024,baradia_mirror_2025}.
Since text distortion has previously proven effective in cross-domain scenarios~\citep{bischoff_importance_2020}, \acp{llm} appear particularly promising as generators of controlled distortions in \ac{av} research.

% BERT: uitility for AV?
Neural \ac{av} models often employ transformer-based architectures such as BERT. 
Although BERT is known to strongly encode topical information~\citep{sawatphol_cross-topic_av_24}, it can still be effective in specific applications. 
For example, transformer-based models have successfully distinguished the direct speech of different characters in novels~\citep{michel_fictional_2024}. 
In this setting, the characters inherently reflect the style of the novelâ€™s author, meaning that the primary distinguishing factor between texts is semantic rather than stylistic.

% XAI
There are also initiatives to use \acp{llm} for improving explainability of \ac{av} methods.
\citet{hung_xai_av_llm_2023} compare different prompts for GPT-4 in zero- and few-shot scenarios.
They note that requesting continuous confidence scores rather than binary outputs reduces a bias towards "no".
While the accuracy of their approaches ranges from $0.51$ to $0.67$, they observed that the explanation of \acp{llm} sometimes contains hallucinations.
Similarly, \citet{ramnath_cave_xai_llm_2025} report hallucinations and issues of propagated bias and incompleteness with their post-hoc explanations.
Due to the lack of human written explanations for \ac{av} decisions, they created a silver dataset with GPT-4-Turbo as an oracle and subsequently using it to train their CAVE model on.
All biases present in GPT-4-Turbo therefore propagate to CAVE.

\citet{nogueira_doc2query_2019} propose Doc2query, a document expansion strategy designed to mitigate the effects of missing relevant documents in keyword-based information retrieval caused by term mismatch. 
Their method employs a sequence-to-sequence transformer model that predicts a query given an input document. 
Although their approach targets information retrieval rather than our application, the use of a pre-trained model to extract the query a human might ask based on a document is conceptually similar to our \pextractor{} in the two-step paraphrasing framework. 
However, Doc2query focuses only on generating a few relevant points rather than capturing a comprehensive representation of the document. 
Therefore, our approach extends this idea by aiming to extract a more complete and informative representation suitable for paraphrase generation.

Other detection methods also make use of rewriting. 
Paraphrases can be generated through rule-based or thesaurus-driven substitutions, by means of monolingual machine translation~\citep{zhou_paraphrase_2021}, or by explicitly prompting generative T5 or GPT models~\citep{kurt_pehlivanoglu_comparative_2024}.
RAIDAR, for instance, generates perturbed versions of a text using paraphrasing and compares edit distances for predictions~\citep{mao_raidar_2024}, while \mirrorMinds{} employs paraphrasing combined with syntactic similarity metrics such as BLEU and METEOR~\citep{baradia_mirror_2025}. 
Based on RAIDAR, \citet{li_learning_2025} propose fine-tuning an \ac{llm} to rewrite human authored texts more than machine generated text.
%%% compared to our work: 
All of these approaches are similar to ours in that they leverage \acp{llm} to generate texts at inference time. 
However, unlike \citet{li_learning_2025}, we do not fine-tune an \ac{llm} for paraphrasing, instead relying on off-the-shelf models.
These methods also generally compute the similarity between the original and generated texts. 
Unlike them, we do not use conventional metrics such as edit distance, BLEU, or METEOR, nor do we perform comparisons at the paraphrase level (e.g., BLEU measures n-gram overlap). 
Instead, we construct frequency-based n-gram vectors and apply similarity metrics.
Finally, a limitation of these approaches is that they cannot distinguish \ac{llm} produced a given text.

% Paraphrasing 
Paraphrasing undermines detectors while also offering methodological utility. 
Systems like DIPPER~\citep{Krishna_dipper_2023} demonstrate how adversarial rewriting can bypass detection. 
At the same time, paraphrasing has been leveraged as a tool for generating stylistic variants~\citep{mao_raidar_2024,baradia_mirror_2025}.

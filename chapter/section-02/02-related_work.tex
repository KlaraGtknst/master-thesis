\label{chap:related_work}
\chapter{Related Work}

% where does my work fit in literetaure?
% why does my work matter?
% what gap is addressed by my work?

% start with classical, then recent and then my own work

% AV, AA & established approaches
\Ac{aa} and \ac{av} are persistent problems in computational linguistics, with roots in stylometry dating back to the \nth{19} century. 
Early approaches were based on word length frequencies, with the aim of resolving disputes over literary authorship~\citep{neal_surveying_2018,stamatatos_survey_2009}.
Today, \ac{aa} and \ac{av} are applied in a wide range of contexts from plagiarism detection~\citep{stein_intrinsic_2011} to author profiling~\citep{emmery_adversarial_2021,stamatatos_survey_2009,elmanarelbouanani_authorship_2014}. 
Despite decades of progress, several core challenges persist, including the application of models in cross-domain settings, on short texts, on texts with temporal style shift, large candidate sets where the true author may not even be present. 
These challenges have motivated successive methodological innovations, which this section reviews.

% Stylometry and Classical Approaches
Since the seminal work of Mosteller and Wallace (1964), numerous stylometric features have been proposed.
These range from writeprints, a set of lexical, syntactic and structural text features~\citep{abbasi_writeprints_2008} to function words~\citep{abbasi_writeprints_2008} and character n-grams.
Function words and n-grams have consistently proven to be among the most effective features for authorship~\citep{tyo_state_2022,altakrori_topic_2021,koppel_authorship_2011}, while writeprints has provided a versatile framework that has served as the basis for several subsequent approaches~\citep{weerasinghe_feature_vector_difference_2021}. 

% cross-domain
Studies~\citep{stamatatos_survey_2009,barlas_cross_domain_2020} have shown that character, function word and \ac{pos} n-grams are generally more topic-invariant than other stylometric features.
Nevertheless, \citet{bischoff_importance_2020} argue that frequent function words still exhibit a strong correlation with topic.
More broadly, many traditional stylometric features remain sensitive to domain variation, and even neural authorship models have been found to perform poorly in \ac{ood} settings~\citep{rivera_soto_learning_2021}.

Compression-based \ac{av} methods determine authorship by comparing the compression of a text in isolation with its compression alongside a partner text. 
Pairs exhibiting only small differences are identified as originating from the same author.
Various distance-based metrics are applied within this framework~\citep{elmanarelbouanani_authorship_2014,bevendorff_divergence_based_2020,bevendorff_overview_2024}, supported by different compression algorithms such as PPM, LZW, and GZIP. 
While conceptually elegant, compression methods tend to be computationally expensive, slow and less effective on large datasets~\citep{tyo_state_2022,neal_surveying_2018}. 

%% Meta learning and Unmasking
The unmasking approach reframes \ac{av} as a meta-learning problem. 
Unmasking iteratively removes the most discriminative features between two documents and observes how quickly classification accuracy degrades. 
If the texts are by the same author, performance collapses rapidly under feature removal and if not, accuracy remains relatively stable~\citep{koppel_authorship_2004}. 
Unmasking has been empirically effective, particularly on long documents such as novels~\citep{koppel_authorship_2011}. 
Its main drawback, however, is its dependence on large text volumes~\citep{koppel_determining_2014,bevendorff_generalizing_2019}.

Subsequent work~\citep{bevendorff_generalizing_2019,bevendorff_divergence_based_2020} has attempted to generalize unmasking to shorter texts by creating chunks by oversampling words in a bootstrap aggregating manner. 
Real-world applications such as plagiarism detection~\citep{stein_intrinsic_2011}, or pre-filtering hyperpartisan news from mainstream news~\citep{potthast_stylometric_2018} incorporate the unmasking approach.

% PAN metrics: F0.5u, Brier, C@1: tyo_state_2022

%% Impostor Method and Open-Set Identification
In the \impAppr{}, \citet{koppel_determining_2014} reduce the open-set \ac{av} problem to an open-set \ac{aa} problem.
They denote the latter many-candidates problem. 
Unlike closed-set attribution, where the true author is guaranteed to be among the candidates, open-set scenarios require distinguishing not only between multiple authors but also between "in-set" and "out-of-set" cases. 
The \ac{av} problem is reduced to the many-candidates problem via adding so-called impostor texts to the candidates pool. 
Instead of relying on fixed feature sets, it repeatedly samples random subsets of features and compares the target text against both candidate author and the set of \imps{}. 
By aggregating many such trials, the method estimates whether the candidate consistently outperforms the \imps{} across varying conditions~\citep{koppel_determining_2014}. 
The method's effectiveness depends heavily on the relevance of available impostor texts. 
This dependency motivates the use of synthetic \imps{} generated by \acp{llm}, which allow for better control over confounders.

%% LLM-Based Authorship and Detection
Recent advances in \acp{llm} have reshaped the landscape of \ac{av}. 
The emergence of generative models introduces both a novel adversary and a valuable methodological resource.
Detecting \ac{llm} generated text can be regarded as a special case of \ac{av}, where the author is a generative model~\citep{bevendorff_overview_2024}. 
Reflecting this shift, the \ac{pan} 2024 shared task "Voight-Kampff" focused on generative \ac{ai} detection, challenging participants to develop systems capable of distinguishing between human and machine-authored texts~\citep{bevendorff_overview_2024,ayele_overview_2024}.
\citet{llm_detection_av_2025} observe that, although current \ac{llm} generated texts still exhibit detectable differences from human writing, such differences are expected to diminish as models continue to improve.
At the same time, \acp{llm} can serve as discriminators~\citep{futrzynski_pairwise_2021} or tools for augmenting \ac{av} methods, for instance through text generation, or paraphrasing~\citep{mao_raidar_2024,baradia_mirror_2025}.
Since text distortion has previously proven effective in cross-domain scenarios~\citep{bischoff_importance_2020}, \acp{llm} appear particularly promising as generators of controlled distortions in \ac{av} research.

\Ac{llm} detection methods can be grouped into metric-based approaches, fine-tuned classifiers, and watermarking. 
Among these, DetectGPT is an influential example for a metric-based approach~\citep{wang_stumbling_2024}.
DetectGPT assumes that artificial generated text corresponds to regions of high model log probability. 
By perturbing candidate texts and rescoring them, the method measures the difference of their respective probabilities.
If it is near zero, the text is likely human authored~\citep{mitchell_detectgpt_2023}. 
While powerful, DetectGPT is limited to white-box scenarios %(access to model probabilities) 
and is vulnerable to paraphrases or detecting unseen models~\citep{Wu_ODD_challenges_2025}.

\citet{nogueira_doc2query_2019} propose Doc2query, a document expansion strategy to mitigate the effects of missing relevant documents during keyword based information retrieval due to term mismatched.
Their method consists of a sequence-to-sequence transformer model that predicts a query given an input document.
Though their approach applies to information retrieval rather than our use case, the usage of a pre-trained model to extract a query a human wants to have answered by the document aligns with our \pextractor{} in the two-step paraphrasing approach.
This approach, however, does not rely on the predicted query capturing a comprehensive image of the document but only few relevant points.
Hence, our approach needs to enhance this idea.

Other detection methods also make use of rewriting. 
Paraphrases can be generated through rule-based or thesaurus-driven substitutions, by means of monolingual machine translation~\citep{zhou_paraphrase_2021}, or by explicitly prompting generative T5 or GPT models~\citep{kurt_pehlivanoglu_comparative_2024}.
RAIDAR, for instance, generates perturbed versions of a text using paraphrasing and compares edit distances~\citep{mao_raidar_2024}, while Mirror Minds employs paraphrasing combined with syntactic similarity metrics such as BLEU and METEOR~\citep{baradia_mirror_2025}. 
Based on RAIDAR, \citet{li_learning_2025} propose fine-tuning an \ac{llm} to rewrite human authored texts more than machine generated text.
%%% compared to our work: 
All of these approaches are similar to ours in that they leverage \acp{llm} to generate texts at inference time. 
However, unlike \citet{li_learning_2025}, we do not fine-tune an \ac{llm} for paraphrasing, instead relying on off-the-shelf models.
These methods also generally compute the similarity between the original and generated texts. 
Unlike them, we do not use conventional metrics such as edit distance, BLEU, or METEOR, nor do we perform comparisons at the paraphrase level (e.g., BLEU measures n-gram overlap). 
Instead, we construct frequency-based n-gram vectors and apply similarity metrics.
Finally, a limitation of these approaches is that they cannot identify which \ac{llm} produced a given text.

% Paraphrasing and Evaluation Metrics
Paraphrasing undermines detectors while also offering methodological utility. 
Systems like DIPPER~\citep{Krishna_dipper_2023} demonstrate how adversarial rewriting can bypass detection. 
At the same time, paraphrasing has been leveraged as a tool for generating stylistic variants~\citep{mao_raidar_2024,baradia_mirror_2025}.

Evaluating paraphrases remains an open challenge. 
Paraphrases evaluation can include dimensions of readability, complexity and grade level~\citep{Thomas_cross_topic_24}.
% They use Textstat, a Python library that helps extract statistics from text.
Traditional metrics such as BLEU~\citep{papineni_bleu_2001}, ROUGE~\citep{lin_rouge_2004}, METEOR~\citep{banerjee_METEOR_2005}, and GLEU~\citep{kurt_pehlivanoglu_comparative_2024} capture n-gram overlap but often fail to reflect semantic adequacy. 
Embedding-based metrics such as BERTScore~\citep{hanna_fine_grained_2021} and Word Mover's Distance~\citep{gohsen_captions_2023} offer more robust semantic similarity estimates, while human evaluation remains the gold standard~\citep{zhou_paraphrase_2021}. 
Despite this, no consensus exists on the best metric for evaluating paraphrasing. 
This uncertainty motivates further exploration of similarity measures for evaluating paraphrases.


% % LLM detection using generative models
% %% AA against LLMs

% \citet{uchendu_authorship_2020} identified three authorship tasks essential for fighting fraudulent activities:
% (1) Given two texts $t_1$ and $t_2$, determine whether they were produced by the same method (i.e. human author or a specific \ac{nlg} method).
% (2) Given a text $t$, determine whether it was human authored or machine generated (Turing Test).
% (3) Given a text $t$, find its author among $k+1$ candidates, which consists of one human and $k$ machines.

% %%% compared to our work
% In the following, we consider (1) \ac{av}, (2) classical \ac{llm} detection, and (3) closed-set \ac{aa}.
% Our approach differs from the work of \citet{uchendu_authorship_2020} in that our candidates (i.e. \imps{}) do not include a human author (3), 
% but only \acp{llm}.
% Moreover, we use different classifiers originally designed for \ac{av}, rather than \ac{aa}.



% %% LLM rewrite LLM texts less than human texts (no AA, but edit distance hypothesis)
% RAIDAR~\citep{mao_raidar_2024} builds upon the invariance property of \acp{llm}, 
% which states that prompting an \ac{llm} to rewrite a machine generated text will introduce little change.
% They motivate this by the observation that (different) autoregressive models produce similar patterns and thus, 
% consider texts generated by (different) \acp{llm} as high quality that do not require rewriting.
% Change is measured by the edit distance between the original text and the rewritten text. 
% \citet{mao_raidar_2024} propose using an edit distance based on the Levenshtein distance or \ac{bow} representations.
% RAIDAR operates on character level rather than using deep neural network features, and it does not require the original generating model for classification. 
% Classification is carried out by comparing the edit distance of the original text and the rewritten text to a threshold.


% %% LLMDet: Proxy to perplexity (problem: requires access to the LLM to build the dictionary)
% Perplexity is a reliable statistical metric for attributing texts to \acp{llm}~\citep{zhang_llmdet_2023}.
% Unfortunately, perplexity requires access to \acp{llm}' parameters (i.e., white-box detection).
% \citet{wu_llmdet_2023} propose LLMDet, a method that uses a proxy to perplexity, 
% where a dictionary of frequent n-gram (frequent among $n$ randomly prompted generated texts per \ac{llm}) 
% next token probabilities is pre-computed (i.e. requiring access to the \ac{llm}), 
% and is subsequently used during inference to approximate perplexity by replacing $x_{<i}$ in $p(x_i | x_{<i})$ with an n-gram.
% Since the construction of the dictionary requires access to the \ac{llm}, LLMDet requires contribution of the closed-source model owners.
% The disputed text is tokenized and the proxy perplexity is calculated for each model and thus, constructing a proxy perplexity vector.
% This vector is input to a trained classifier.
% %%% compared to our work
% Proxy perplexity could be used as a baseline for our approach, though it requires access to the \ac{llm} and is thus not applicable in our case.

%% AV/ AA via LLMs

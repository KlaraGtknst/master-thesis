\chapter{Related Work}
\label{chap:related_work}

In this work, we distinguish between the traditional \ac{av} scenario, where both texts in a pair are human-authored, and emerging scenarios in which at least one text is artificially generated by an \ac{llm}. 
Following \citet{llm_detection_av_2025}, we treat \acp{llm} as authors due to their increasing ability to emulate human writing. 
Accordingly, our review covers both traditional human-human \ac{av} approaches and recent methods addressing \ac{llm} detection, which can be framed as an \ac{av} task. 
While our evaluation focuses on the traditional \ac{av} scenario, we also consider advances in \ac{llm}-involved scenarios to inform methodological improvements, particularly in enhancing robustness in cross-domain settings.

Recent shared tasks at \acs{clef}, such as Voight-Kampff, have responded to advances in \ac{gai} by combining the \acs{pan} and \acs{eloquent} tracks. 
In this adversarial \ac{llm} detection setting, \acs{pan} participants build detectors, while \acs{eloquent} participants generate texts designed to fool them. 
Despite the increasing use of neural models, traditional approaches remain relevant. 
The \acs{pan} 2024 baselines included \unmasking{}~\citep{koppel_authorship_2004,koppel_authorship_2011,bevendorff_generalizing_2019,bevendorff_divergence_based_2020} and \ac{ppmd}-based methods~\citep{tyo_state_2022,neal_surveying_2018}.
In contrast, many submissions incorporated neural approaches. 
\acs{bert}-based classifiers, generation probability predictors, and LoRA \acp{llm} were among the submissions. 
However, most approaches struggled to generalise across domains, indicating reliance on domain-specific cues~\citep{bevendorff_overview_2024,rivera_soto_learning_2021}. 
Transformer-based methods often encode topical rather than stylistic information~\citep{nguyen_bert_topic_av_2023,sawatphol_cross_topic_av_24}, though they can be effective in applications where semantic content reflects authorship, such as distinguishing characters' speech in novels~\citep{michel_fictional_2024}.

Traditional non-neural approaches retain relevance for their strength in cross-domain settings. 
Studies of feature robustness~\citep{stamatatos_survey_2009,barlas_cross_domain_2020} highlight that character, function word, and \ac{pos} n-grams are generally more topic-independent than other stylometric features, though frequent function words can still correlate with topic~\citep{bischoff_importance_2020}. 
These findings suggest that \acp{llm} alone may not yet be reliable as standalone discriminators but can enhance traditional approaches or improve robustness in cross-domain settings.

\acp{llm} have been explored both as discriminators for \ac{llm} detection~\citep{futrzynski_pairwise_2021} and as tools to augment \ac{av} methods through text generation or paraphrasing~\citep{mao_raidar_2024,baradia_mirror_2025}. 
Generating controlled distortions has proven effective in cross-domain scenarios~\citep{bischoff_importance_2020}, positioning \acp{llm} as promising additions to \ac{av} research.

\acp{llm} also support explainability. 
\citet{hung_xai_av_llm_2023} employed GPT-4 for generating \ac{av} predictions with explanations in zero- and few-shot scenarios, noting that requesting continuous confidence scores reduces bias towards different-author predictions. 
However, generated explanations can contain hallucinations. 
Similarly, \citet{ramnath_cave_xai_llm_2025} observed hallucinations, bias propagation and incompleteness when using GPT-4-Turbo as an oracle to create a silver dataset for training their CAVE model, a strategy motivated by the lack of human-written explanations for \ac{av} decisions.

Furthermore, certain approaches build on the capacity of \acp{llm} to closely approximate human writing style.
DetectGPT, for instance, identifies machine-generated text by perturbing candidate texts and rescoring them based on model log probabilities~\citep{mitchell_detectgpt_2023}. 
While effective, DetectGPT is limited to white-box settings and is vulnerable to paraphrases or unseen models~\citep{Wu_ODD_challenges_2025}. 
Its perturbation strategy shares conceptual similarity with our paraphrasing approach.

Prior work also informs our two-step paraphraser. 
\citet{nogueira_doc2query_2019} introduce Doc2query, generating queries from documents to address term mismatch in retrieval, while \citet{lee_gecko_2024} propose Gecko, a transformer distilled from \acp{llm} that generates queries for document retrieval. 
Both share the goal with our \pextractor{} of extracting plausible human queries, though our approach yields more complete, document-specific information.

A similar two-step approach is reported by \citet{bevendorff_overview_2024} for constructing an evaluation dataset for \acs{pan}.
They first extract bullet points and metadata from news articles using GPT-4 Turbo, and then generate artificial texts based on this extracted information. 
This motivated our decision to work on a two-step paraphrasing strategy. 
Unlike prior work, we do not restrict metadata to predefined classes nor focus on a single text genre.

Other approaches combine \ac{llm} rewriting with traditional syntactic metrics. 
Paraphrases can be generated via rule-based methods, monolingual machine translation~\citep{zhou_paraphrase_2021}, or generative models~\citep{kurt_pehlivanoglu_comparative_2024}. 
RAIDAR, for example, generates perturbed versions of text and compares edit distances for predictions~\citep{mao_raidar_2024}, while \mirrorMinds{} scores paraphrases using \acs{bleu} and METEOR~\citep{baradia_mirror_2025}. 
However, when using \mirrorMinds{}, we found it produced low-quality paraphrases, limiting its potential for \ac{av} tasks. % only one word answers to text
\citet{li_learning_2025} fine-tune \acp{llm} to rewrite human-authored texts more extensively than machine-generated ones. 
All of these approaches are similar to ours in that they leverage \acp{llm} to generate texts at inference time. 
But unlike them, our method employs off-the-shelf models, constructs frequency-based n-gram vectors, and compares them via vectors similarity measures.
Moreover, for paraphrase evaluation we leverage more than simple n-gram overlap-based similarity measures.

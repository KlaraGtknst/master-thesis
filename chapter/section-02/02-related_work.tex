\label{chap:related_work}
\chapter{Related Work}

% where does my work fit in literetaure?
% why does my work matter?
% what gap is addressed by my work?

% start with classical, then recent and then my own work

% AV, AA & established approaches
\ac{aa} and \ac{av} are persistent problems in computational linguistics, with roots in stylometry dating back to the \nth{19} century. 
Early approaches were based on word length frequencies, with the aim of resolving disputes over literary authorship~\citep{neal_surveying_2018,stamatatos_survey_2009}.
Today, \ac{aa} and \ac{av} are applied in a wide range of contexts. 
Despite decades of progress, several core challenges persist: 
Application of models in cross-domain settings, short or inconsistent texts, large candidate sets where the true author may not even be present. 
These challenges have motivated successive methodological innovations, which this section reviews.

% Stylometry and Classical Approaches
Starting with Mosteller and Wallace (1964), stylometry has developed numerous feature-based approaches for \ac{av}. 
Character n-grams have proven especially effective~\citep{tyo_state_2022,altakrori_topic_2021}. 
Unfortunately, many stylometric features are sensitive to domain and topic variation.

Compression-based methods estimate similarity through compressibility, bypassing the need for explicit features. 
Notable approaches include the Normalized Compression Distance (NCD)~\citep{elmanarelbouanani_authorship_2014}, Compression-Based Classification (CBC)~\citep{bevendorff_divergence_based_2020,bevendorff_overview_2024}, and compression algorithms such as PPM, LZW, and GZIP. 
The central intuition is that if two texts are authored by the same individual, their concatenation will compress more efficiently than unrelated texts. 
While conceptually elegant, compression methods tend to be computationally expensive and less effective on large datasets~\citep{tyo_state_2022}. 

%% Meta learning and Unmasking
The unmasking approach reframes \ac{av} as a meta-learning problem. 
Instead of focusing on absolute similarity, unmasking iteratively removes the most discriminative features between two documents and observes how quickly classification accuracy degrades. 
If the texts are by the same author, performance collapses rapidly under feature removal and if not, accuracy remains relatively stable~\citep{koppel_authorship_2004}. 
Unmasking has been empirically effective, particularly on long documents such as novels~\citep{koppel_authorship_2011}. 
Its main drawback, however, is its dependence on large text volumes~\citep{koppel_determining_2014,bevendorff_generalizing_2019}.

Subsequent work~\citep{bevendorff_generalizing_2019,bevendorff_divergence_based_2020} has attempted to generalize unmasking to shorter texts. 
Creating chunks by oversampling words in a bootstrap aggregating manner aims to allow application of the unmasking approach on smaller samples. 

%% Impostor Method and Open-Set Identification
A central challenge in \ac{aa} is the many-candidates problem, often framed as open-set identification. 
Unlike closed-set attribution, where the true author is guaranteed to be among the candidates, open-set scenarios require distinguishing not only between multiple authors but also between "in-set" and "out-of-set" cases. 
This is particularly relevant in forensic or real-world applications, where the actual author may not appear in the candidate pool.

The \impAppr{} represents reduction from the \ac{av} problem to the many-candidates  problem via adding artificial impostor texts to the candidates pool. 
Instead of relying on fixed feature sets, it repeatedly samples random subsets of features and compares the target text against both candidate authors and a set of unrelated \imps{}. 
By aggregating many such trials, the method estimates whether the candidate consistently outperforms \imps{} across varying conditions~\citep{koppel_determining_2014}. 
The method's effectiveness depends heavily on the relevance of available impostor texts. 
This dependency motivates the use of synthetic \imps{} generated by \acp{llm}, which allow for better control over confounders.

%% LLM-Based Authorship and Detection
Recent advances in \acp{llm} have reshaped the landscape of \ac{av}. 
The rise of \ac{ai}-generated text has introduced both a new adversary and a new methodological resource. 
On the one hand, detecting \ac{ai}-authored text can be viewed as a special case of \ac{av}, where the author is a generative model~\citep{bevendorff_overview_2024}. 
On the other hand, \ac{llm} themselves can serve as tools for augmenting \ac{av} methods, for example through text generation or paraphrasing.

\ac{llm} detection methods can be grouped into metric-based approaches, fine-tuned classifiers, and watermarking. 
Among these, DetectGPT is influential example for metric-based approach~\citep{wang_stumbling_2024}.
DetectGPT assumes that model-generated text corresponds to regions of high model log probability. 
By perturbing candidate texts and rescoring them, the method measures the difference of their respective probabilities.
If it is near zero, the text is likely human authored~\citep{mitchell_detectgpt_2023}. 
While powerful, DetectGPT is limited by its white-box scenarios (access to model probabilities) and is vulnerable to paraphrasing or distributional shifts when tested against unseen models~\citep{Wu_ODD_challenges_2025}.

Other detection methods similarly exploit rewriting. 
RAIDAR, for instance, generates perturbed versions of a text using paraphrasing and compares edit distances~\citep{mao_raidar_2024}, while Mirror Minds employs paraphrasing combined with syntactic similarity metrics such as BLEU and METEOR~\citep{baradia_mirror_2025}. 
Both approaches reduce the problem to binary classification, human versus AI, which contrasts with our multiclass framing???.
%%% compared to our work: RAIDAR
%%%% generation of texts during inference
Both approaches are similar to our work in that they use \acp{llm} to generate texts during inference.
We do not fine-tune an \ac{llm} for paraphrasing but use off-the-shelf models (like RAIDAR).
%%%% similarity measure
All these approaches compute the similarity of the original text and the generated text.
However, we do not use edit distance (i.e. Levenshtein distance) as similarity measure.
%%%% limitations
This approach is unable to detect which \ac{llm} generated the text.
%%% compared to our work: Mirror minds
%%%% same approach
This approach is similar to our approach in that it uses \acp{llm} to generate paraphrases of the original text.
Moreover, it compares the original text to the generated paraphrases as in a \ac{aa} problem. % rather   detection?????
%%%% similarity measure
We do not use BLEU or METEOR as similarity measure, nor do we compare directly on paraphrase-level (i.e. BLEU calculates n-gram overlap) 
but construct our own frequency based n-gram vectors input vector similarity metrics.
%%%% they discard information, solve another problem
However, this approach discards the information which \ac{llm} produced the most similar paraphrase. 
While our goal is to solve an \ac{aa} problem (i.e. multiclass classification), 
\citet{baradia_mirror_2025} solve a binary classification problem (i.e. human vs. \ac{ai} generated text).

% Paraphrasing and Evaluation Metrics

Paraphrasing undermines detectors while also offering methodological utility. 
Systems like DIPPER~\citep{Krishna_dipper_2023} demonstrate how adversarial rewriting can bypass detection. 
At the same time, paraphrasing has been leveraged as a tool for generating stylistic variants~\citep{mao_raidar_2024,baradia_mirror_2025}.

Evaluating paraphrases remains an open challenge. 
Paraphrases evaluation can include dimensions of readability, complexity and grade level~\citep{Thomas_cross_topic_24}.
% They use Textstat, a Python library that helps extract statistics from text.
Traditional metrics such as BLEU~\citep{papineni_bleu_2001}, ROUGE~\citep{lin_rouge_2004}, METEOR, and GLEU~\citep{kurt_pehlivanoglu_comparative_2024} capture n-gram overlap but often fail to reflect semantic adequacy. 
Embedding-based metrics such as BERTScore and Word Mover's Distance~\citep{gohsen_captions_2023} offer more robust semantic similarity estimates, while human evaluation remains the gold standard~\citep{zhou_paraphrase_2021}. 
Despite this, no consensus exists on the best metric for evaluating paraphrasing. 
This uncertainty motivates further exploration of similarity measures for evaluating paraphrases.



% % LLM detection using generative models
% %% AA against LLMs

% \citet{uchendu_authorship_2020} identified three authorship tasks essential for fighting fraudulent activities:
% (1) Given two texts $t_1$ and $t_2$, determine whether they were produced by the same method (i.e. human author or a specific \ac{nlg} method).
% (2) Given a text $t$, determine whether it was human authored or machine generated (Turing Test).
% (3) Given a text $t$, find its author among $k+1$ candidates, which consists of one human and $k$ machines.

% %%% compared to our work
% In the following, we consider (1) \ac{av}, (2) classical \ac{llm} detection, and (3) closed-set \ac{aa}.
% Our approach differs from the work of \citet{uchendu_authorship_2020} in that our candidates (i.e. \imps{}) do not include a human author (3), 
% but only \acp{llm}.
% Moreover, we use different classifiers originally designed for \ac{av}, rather than \ac{aa}.



% %% LLM rewrite LLM texts less than human texts (no AA, but edit distance hypothesis)
% RAIDAR~\citep{mao_raidar_2024} builds upon the invariance property of \acp{llm}, 
% which states that prompting an \ac{llm} to rewrite a machine generated text will introduce little change.
% They motivate this by the observation that (different) autoregressive models produce similar patterns and thus, 
% consider texts generated by (different) \acp{llm} as high quality that do not require rewriting.
% Change is measured by the edit distance between the original text and the rewritten text. 
% \citet{mao_raidar_2024} propose using an edit distance based on the Levenshtein distance or \ac{bow} representations.
% RAIDAR operates on character level rather than using deep neural network features, and it does not require the original generating model for classification. 
% Classification is carried out by comparing the edit distance of the original text and the rewritten text to a threshold.


% %% LLMDet: Proxy to perplexity (problem: requires access to the LLM to build the dictionary)
% Perplexity is a reliable statistical metric for attributing texts to \acp{llm}~\citep{zhang_llmdet_2023}.
% Unfortunately, perplexity requires access to \acp{llm}' parameters (i.e., white-box detection).
% \citet{wu_llmdet_2023} propose LLMDet, a method that uses a proxy to perplexity, 
% where a dictionary of frequent n-gram (frequent among $n$ randomly prompted generated texts per \ac{llm}) 
% next token probabilities is pre-computed (i.e. requiring access to the \ac{llm}), 
% and is subsequently used during inference to approximate perplexity by replacing $x_{<i}$ in $p(x_i | x_{<i})$ with an n-gram.
% Since the construction of the dictionary requires access to the \ac{llm}, LLMDet requires contribution of the closed-source model owners.
% The disputed text is tokenized and the proxy perplexity is calculated for each model and thus, constructing a proxy perplexity vector.
% This vector is input to a trained classifier.
% %%% compared to our work
% Proxy perplexity could be used as a baseline for our approach, though it requires access to the \ac{llm} and is thus not applicable in our case.

%% AV/ AA via LLMs

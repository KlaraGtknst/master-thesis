\chapter{Related Work}
\label{chap:related_work}

% where does my work fit in literetaure?
% why does my work matter?
% what gap is addressed by my work?

% start with classical, then recent and then my own work

% paraphrasing, unmasking, one class

Classical \ac{av} methods include character-based compression methods~\citep{tyo_state_2022,neal_surveying_2018}, and meta-learning techniques such as traditional~\citep{koppel_authorship_2004,koppel_authorship_2011} or generalized \unmasking{}~\citep{bevendorff_generalizing_2019,bevendorff_divergence_based_2020}.
The \impAppr{} utilizes hard negative mining for prediction~\citep{koppel_determining_2014}.

% CLEF
Reacting to advances in \ac{gai}, more advanced methods emerge such as Voight-Kampff, a shared task combining \acs{pan} and \acs{eloquent} at \acs{clef} into an adversarial \ac{gai} \ac{av} task.
\acs{pan} participants built a detector, while \acs{eloquent} participants built systems that should fool the detectors into believing the machine generated texts were human authored.
\acs{pan} 2024 baselines included estimating text perplexities (for perturbated texts) and self-similarity assessments via \unmasking{} or \ac{ppmd} approaches for chunks of the text.
\acs{pan} 2024 submissions included \acs{bert}-based models with classifiers or generation probability prediction (similar to one of the baselines), as well as direct discriminators or LoRA \acp{llm}.
Most approaches proved unable to generalize across domains indicating that their approaches encoded domain-specific information~\citep{bevendorff_overview_2024}.

\Ac{llm} detection methods can be grouped into metric-based approaches, fine-tuned classifiers, and watermarking. 
Among these, DetectGPT is an influential example for a metric-based approach~\citep{wang_stumbling_2024}.
DetectGPT assumes that artificial generated text corresponds to regions of high model log probability. 
By perturbing candidate texts and rescoring them, the method measures the difference of their respective probabilities.
If it is near zero, the text is likely human authored~\citep{mitchell_detectgpt_2023}. 
While powerful, DetectGPT is limited to white-box scenarios %(access to model probabilities) 
and is vulnerable to paraphrases or detecting unseen models~\citep{Wu_ODD_challenges_2025}.
Notably, its perturbation strategy bears similarity to our paraphrasing approach.

\citet{schmidt_llm_av_latin_24}\ evaluated the zero-shot effectiveness of \acl{sota} \acp{llm} for \ac{av} and \ac{aa} in low-resource languages, using models such as GPT-4o, Gemini-1.5, Mistral-Large, and Claude. 
They observed that more detailed prompts improved precision and enhanced the identification of syntactic features. 
However, such prompts also led the models to assume a deterministic relationship between specific features and the final prediction, which degraded overall effectiveness. 
This effect was particularly pronounced for larger models like GPT-4o and Claude, which can leverage their extensive intrinsic knowledge more freely under less constrained prompts. 
Although overall effectiveness was reasonable, \acp{llm} still struggle to distinguish writing style independently of content~\citep{schmidt_llm_av_latin_24}. 
Similarly, \citet{nguyen_bert_topic_av_2023}\ found that transformer-based methods often rely on topical information rather than genuine authorship characteristics.

\acs{pan} constructed an evaluation dataset of news articles using a two-step procedure.
First, bullet points and metadata were extracted from the original texts using GPT-4 Turbo.
Second, artificial texts were generated by various \acp{llm} based on this extracted information. 
This methodology is particularly relevant to our work, as it motivated using a two-step approach to paraphrasing. 
In \ac{llm} detection, one-step paraphrasing is considered less challenging, with \citet{bevendorff_overview_2024} quantifying difficulty as the inverse of the mean detection score.
Unlike \acs{pan}, our approach does not restrict metadata extraction to a predefined set of classes, nor does it focus on a single text genre. 

% cross-domain
Studies assessing the robustness of existing features and approaches~\citep{stamatatos_survey_2009,barlas_cross_domain_2020} have shown that character, function word and \ac{pos} n-grams are generally more topic-invariant than other stylometric features.
Nevertheless, \citet{bischoff_importance_2020}\ argue that frequent function words still exhibit a strong correlation with topic.
Many traditional stylometric features remain sensitive to domain variation, and even neural authorship models have been found to perform poorly in \ac{ood} settings~\citep{rivera_soto_learning_2021}.

\acp{llm} can serve as discriminators for \ac{llm} detection~\citep{futrzynski_pairwise_2021} or tools for augmenting \ac{av} methods, for instance through text generation, or paraphrasing~\citep{mao_raidar_2024,baradia_mirror_2025}.
Since text distortion has previously proven effective in cross-domain scenarios~\citep{bischoff_importance_2020}, \acp{llm} appear particularly promising as generators of controlled distortions in \ac{av} research.

% \ac{bert}: uitility for AV?
Neural \ac{av} models often employ transformer-based architectures such as \ac{bert}. 
Although \ac{bert} is known to strongly encode topical information~\citep{sawatphol_cross_topic_av_24}, it can still be effective in specific applications. 
For example, transformer-based models have successfully distinguished the direct speech of different characters in novels~\citep{michel_fictional_2024}. 
In this setting, the characters inherently reflect the style of the novelâ€™s author, meaning that the primary distinguishing factor between texts is semantic rather than stylistic.

% XAI
There are also initiatives to use \acp{llm} for improving explainability of \ac{av} methods.
\citet{hung_xai_av_llm_2023}\ compare different prompts for GPT-4 in zero- and few-shot scenarios.
They note that requesting continuous confidence scores rather than binary outputs reduces a bias towards different author predicitions.
While the accuracy of their approaches ranges from $0.51$ to $0.67$, they observed that the explanation of \acp{llm} sometimes contains hallucinations.
Similarly, \citet{ramnath_cave_xai_llm_2025}\ report hallucinations and issues of propagated bias and incompleteness with their post-hoc explanations.
Due to the lack of human written explanations for \ac{av} decisions, they created a silver dataset with GPT-4-Turbo as an oracle and subsequently using it to train their CAVE model on.
All biases present in GPT-4-Turbo therefore propagate to CAVE.

\citet{nogueira_doc2query_2019}\ introduce Doc2query, a document expansion method addressing term mismatch in keyword-based information retrieval by predicting queries from documents using a sequence-to-sequence transformer. 
Instead of producing a full document representation, Doc2query generates a single query that captures only a few salient points. 
In a related direction, \citet{lee_gecko_2024}\ propose Gecko, a transformer-based \ac{lm} distilled from \acp{llm}, which uses query and task generation for finetuning. 
Unlike our approach, their generated queries are not constrained to the input text but are designed to retrieve documents that better answer the query than the original text itself. 
While both methods focus on information retrieval, they share with our framework the idea of extracting plausible human queries from documents. 
Our two-step paraphrase generation component \pextractor{} extends this idea by aiming for a more complete and document-specific representation.

Other detection methods also make use of rewriting. 
Paraphrases can be generated through rule-based or thesaurus-driven substitutions, by means of monolingual machine translation~\citep{zhou_paraphrase_2021}, or by explicitly prompting generative T5 or GPT models~\citep{kurt_pehlivanoglu_comparative_2024}.
RAIDAR, for instance, generates perturbed versions of a text using paraphrasing and compares edit distances for predictions~\citep{mao_raidar_2024}, while \mirrorMinds{} employs paraphrasing combined with syntactic similarity metrics such as BLEU and METEOR~\citep{baradia_mirror_2025}. 
Based on RAIDAR, \citet{li_learning_2025}\ propose fine-tuning an \ac{llm} to rewrite human authored texts more than machine generated text.
%%% compared to our work: 
All of these approaches are similar to ours in that they leverage \acp{llm} to generate texts at inference time. 
However, unlike \citet{li_learning_2025}, we do not fine-tune an \ac{llm} for paraphrasing, instead relying on off-the-shelf models.
These methods also generally compute the similarity between the original and generated texts. 
Unlike them, we do not use conventional metrics such as edit distance, BLEU, or METEOR, nor do we perform comparisons at the paraphrase level (e.g., BLEU measures n-gram overlap). 
Instead, we construct frequency-based n-gram vectors and apply similarity metrics.
% Finally, a limitation of these approaches is their inability to identify which \ac{llm} generated a given text.

% Paraphrasing 
Paraphrasing undermines detectors while also offering methodological utility. 
Systems like DIPPER~\citep{Krishna_dipper_2023} demonstrate how adversarial rewriting can bypass detection. 
At the same time, paraphrasing has been leveraged as a tool for generating stylistic variants~\citep{mao_raidar_2024,baradia_mirror_2025}.

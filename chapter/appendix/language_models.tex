\chapter{Large Language Models}
\label{app:language_models}

We used four autoregressive \acp{llm} for paraphrasing and instruction-based information extraction, with details in \autoref{tab:llm_paraphrasers} and the subsequent subsections.

\begin{table}[h]
\centering
\caption{Collection of multilingual \acp{llm} used for paraphrasing\protect\footnotemark.}
\label{tab:llm_paraphrasers}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llrl}
\toprule
\textbf{Organization} & \textbf{Model} & \textbf{Context Window Size (\# token)} & \textbf{Knowledge cutoff} \\
\midrule
Mistral       & Mistral Large Instruct & 128k & July 2024 \\
Meta          & Llama 3.1 8B Instruct  & 128k & December 2023 \\
OpenAI        & GPT OSS 120B           & 128k & June 2024 \\
Alibaba Cloud & Qwen 3 32B             & 32k  & September 2024 \\
\bottomrule
\end{tabular}%
}
\end{table}
\footnotetext{\url{https://docs.hpc.gwdg.de/services/chat-ai/models/index.html} (14.09.2025)}

\section{Llama 3.1}

\texttt{Meta Llama 3.1} is an autoregressive \ac{llm} with a transformer architecture.
Its pretrained model components  (8B, 70B, 405B) are optimized using different fine-tuning strategies.
% They are further listed in \autoref{tab:llama31}.
The model was released on July 23, 2024 and has a knowledge cut-off of December 2023.
It supports eight languages including English, French, German, Hindi, Italian, Portuguese, Spanish, and Thai\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct} (14.09.2025)}.

% \begin{table}[h]
% \centering
% \caption{Meta Llama 3.1 collection of multilingual \acp{llm} is a collection of pretrained and instruction-tuned generative models which are optimised for multilingual dialogue use cases.}
% \label{tab:llama31}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{llrll}
% \toprule
% \textbf{Model}             & \textbf{Training Data}                & \textbf{\# Parameters} & \textbf{Input Modalities} & \textbf{Output Modalities} \\
% \midrule
% Llama 3.1 (text only) & Mix of publicly available online data & 8B                     & Multilingual text         & Multilingual text and code \\
%  &  & 70B  & Multilingual text & Multilingual text and code \\
%  &  & 405B & Multilingual text & Multilingual text and code \\
% \bottomrule
% \end{tabular}%
% }
% \end{table}


\section{Mistral-Large-Instruct}

We use \texttt{Mistral-Large-Instruct-2407} (also \texttt{Mistral Large Instruct-2}\footnote{\url{https://mistral.ai/news/mistral-large-2407} (14.09.2025)}).
It is a multilingual dense transformer-based \ac{llm}\footnote{\url{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}, \url{https://dataloop.ai/library/model/alpindale_mistral-large-instruct-2407-fp8/} (14.09.2025)}.
Its context window length is 128K.

\section{Openai-gpt-oss-120b}

\texttt{gpt-oss} models are autoregressive mixture-of-experts transformers~\citep{openai2025gptoss120bgptoss20bmodel}.
The 120b variant consists of 36 layers and 5.1B active parameters per token per forward pass.
It contains 128 experts and has a context length of around 128K.
A detailed review of \texttt{gpt-oss} architecture and training is provided in \citep{openai2025gptoss120bgptoss20bmodel}.


\section{Qwen3:32b}
% there are also mixture-of-experts, but not thsi one
% causal (=autoregressive, != masked) language model with 32K context length: https://huggingface.co/Qwen/Qwen3-32B
\texttt{Qwen3:32b} is a dense autoregressive transformer model.
The model supports more than 100 languages and dialects\footnote{\url{https://qwenlm.github.io/blog/qwen3/} (14.09.2025)}.
Qwen3 models can operate in non-thinking and thinking mode~\citep{qwen3_technical_report}.
% It has 64 layers and a context length of 128K~\citep{qwen3_technical_report}.
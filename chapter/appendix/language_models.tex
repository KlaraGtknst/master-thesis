\chapter{Large Language Models}
\label{app:language_models}

We initially also worked with T5-based models\footnote{\url{https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base}, \url{https://huggingface.co/Vamsi/T5_Paraphrase_Paws} (12.06.2025)}. However, these models proved insufficient for our tasks in terms of prompt adherence and output quality. 
Consequently, we focused on the four autoregressive \acp{llm} for paraphrasing and instruction-based information extraction presented in \Cref{tab:llm_paraphrasers}.

\begin{table}[h]
\centering
\caption{Collection of multilingual \acp{llm} used for paraphrasing\protect\footnotemark.}
\label{tab:llm_paraphrasers}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llrl@{}}
\toprule
\textbf{Organization} & \textbf{Model} & \textbf{Context Window Size (\# token)} & \textbf{Knowledge cutoff} \\
\midrule
Mistral       & Mistral Large Instruct & 128k & July 2024 \\
Meta          & Llama 3.1 8B Instruct  & 128k & December 2023 \\
OpenAI        & GPT OSS 120B           & 128k & June 2024 \\
Alibaba Cloud & Qwen 3 32B             & 32k  & September 2024 \\
\bottomrule
\end{tabular}%
}
\end{table}
\footnotetext{\url{https://docs.hpc.gwdg.de/services/chat-ai/models/index.html} (14.09.2025)}

To perform paraphrasing, we configure both a system prompt and a user message to guide the model's output. 
The system prompt defines the assistant’s behaviour, instructing it to produce only the final paraphrased text, while the user message provides the input text and any additional instructions. 
\Cref{lst:paraphrase_prompt} shows this setup using the chat-based API.
We did not compare different system prompts in this study.


\begin{listing}[H]
\centering
\begin{minted}{python}
body = {
    "model": self.model_id,
    "messages": [
        {
            "role": "system",
            "content": "You are a paraphrasing assistant. "
                    "Output only the final paraphrased text.",
        },
        {
            "role": "user",
            "content": f"{text[:30000]}\n{prompt.strip()}"
        },
    ],
    "temperature": temperature
}

response = self.client.chat.completions.create(**body)
\end{minted}
\caption{
System prompt and user message configuration for paraphrasing using models hosted by \ac{gwdg}/\ac{saia}. 
\texttt{model\_id} specifies the model to be used. 
The input variables \texttt{temperature}, \texttt{text}, and \texttt{prompt} correspond, respectively, to the temperature for the \ac{llm}, the text to be paraphrased, and the instruction provided to the \ac{llm}.
For very large inputs, the text is truncated to avoid errors if it exceeds the model’s maximum context window.
}
\label{lst:paraphrase_prompt}
\end{listing}



\section{Llama 3.1}

\texttt{Meta Llama 3.1} is an autoregressive \ac{llm} with a transformer architecture.
Its pretrained model components  (8B, 70B, 405B) are optimised using different fine-tuning strategies.
% They are further listed in \Cref{tab:llama31}.
The model was released on July 23, 2024 and has a knowledge cut-off of December 2023.
It supports eight languages including English, French, German, Hindi, Italian, Portuguese, Spanish, and Thai\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct} (14.09.2025)}.

% \begin{table}[h]
% \centering
% \caption{Meta Llama 3.1 collection of multilingual \acp{llm} is a collection of pretrained and instruction-tuned generative models which are optimised for multilingual dialogue use cases.}
% \label{tab:llama31}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}llrll@{}}
% \toprule
% \textbf{Model}             & \textbf{Training Data}                & \textbf{\# Parameters} & \textbf{Input Modalities} & \textbf{Output Modalities} \\
% \midrule
% Llama 3.1 (text only) & Mix of publicly available online data & 8B                     & Multilingual text         & Multilingual text and code \\
%  &  & 70B  & Multilingual text & Multilingual text and code \\
%  &  & 405B & Multilingual text & Multilingual text and code \\
% \bottomrule
% \end{tabular}%
% }
% \end{table}


\section{Mistral-Large-Instruct}

We use \texttt{Mistral-Large-Instruct-2407} (also \texttt{Mistral Large Instruct-2}\footnote{\url{https://mistral.ai/news/mistral-large-2407} (14.09.2025)}).
It is a multilingual dense transformer-based \ac{llm}\footnote{\url{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}, \url{https://dataloop.ai/library/model/alpindale_mistral-large-instruct-2407-fp8/}
(14.09.2025); unlike the version at this URL, our imodel is full-precision}.
Its context window length is 128K.

\section{Openai-gpt-oss-120b}

\texttt{gpt-oss} models are autoregressive mixture-of-experts transformers~\citep{openai2025gptoss120bgptoss20bmodel}.
The 120b variant consists of 36 layers and 5.1B active parameters per token per forward pass.
It contains 128 experts and has a context length of around 128K.
A detailed review of \texttt{gpt-oss} architecture and training is provided in \citep{openai2025gptoss120bgptoss20bmodel}.


\section{Qwen3:32b}
% there are also mixture-of-experts, but not thsi one
% causal (=autoregressive, != masked) language model with 32K context length: https://huggingface.co/Qwen/Qwen3-32B
\texttt{Qwen3:32b} is a dense autoregressive transformer model.
The model supports more than 100 languages and dialects\footnote{\url{https://qwenlm.github.io/blog/qwen3/} (14.09.2025)}.
Qwen3 models can operate in non-thinking and thinking mode~\citep{qwen3_technical_report}.
% It has 64 layers and a context length of 128K~\citep{qwen3_technical_report}.
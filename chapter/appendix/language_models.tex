\chapter{\acp{lm}}
\label{app:language_models}

\section{Llama 3.1}

\texttt{Meta Llama 3.1} is an autoregressive \ac{lm} that using a transformer architecture.
Its pretrained model components  (8B, 70B, 405B) are optimized using different fine-tuning strategies.
% They are further listed in \autoref{tab:llama31}.
The model was released on July 23, 2024 and has a knowledge cut-off of December 2023.
It supports eight languages including English, French, German, Hindi, Italian, Portuguese, Spanish, and Thai\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct} (14.09.20025)}.

% \begin{table}[h]
% \centering
% \caption{Meta Llama 3.1 collection of multilingual \acp{llm} is a collection of pretrained and instruction-tuned generative models which are optimised for multilingual dialogue use cases.}
% \label{tab:llama31}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{llrll}
% \toprule
% \textbf{Model}             & \textbf{Training Data}                & \textbf{\# Parameters} & \textbf{Input Modalities} & \textbf{Output Modalities} \\
% \midrule
% Llama 3.1 (text only) & Mix of publicly available online data & 8B                     & Multilingual text         & Multilingual text and code \\
%  &  & 70B  & Multilingual text & Multilingual text and code \\
%  &  & 405B & Multilingual text & Multilingual text and code \\
% \bottomrule
% \end{tabular}%
% }
% \end{table}


\section{Mistral-Large-Instruct}

\texttt{Mistral-Large-Instruct} is a multilingual dense \ac{llm}\footnote{\url{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407} (14.09.2025)}.


\section{Openai-gpt-oss-120b}

\texttt{gpt-oss} models are autoregressive mixture-of-experts transformers~\citep{openai2025gptoss120bgptoss20bmodel}.
The 120b variant consists of 36 layers and 5.1B active parameters per token per forward pass.
It contains 128 experts and has a context length of around 130K.
A detailed review of \texttt{gpt-oss} architecture and training is provided in \citep{openai2025gptoss120bgptoss20bmodel}.


\section{Qwen3:32b}
% there are also mixture-of-experts, but not thsi one
% causal (=autoregressive, != masked) language model with 32K context length: https://huggingface.co/Qwen/Qwen3-32B
\texttt{Qwen3:32b} is a dense autoregressive transformer model.
The model supports more than 100 languages and dialects\footnote{\url{https://qwenlm.github.io/blog/qwen3/} (14.09.2025)}.
Qwen3 models can operate in non- and thinking mode.
It has 64 layers and a context length of 128K~\citep{qwen3_technical_report}.
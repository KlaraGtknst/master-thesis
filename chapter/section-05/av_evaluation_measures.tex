\subsection{\ac{av} Quality Measures}
\label{subsec:av_quality_measures}

Since \ac{av} forms the core of \ac{aa}, and because every \ac{aa} metric can be reduced to \ac{av}, this section focuses primarily on \ac{av}. 
Evaluation relies on standard classification metrics, each with distinct advantages and limitations.

The most straightforward metric is accuracy (\autoref{eq:accuracy}), which measures the proportion of correctly classified cases across all samples. 
While intuitive, accuracy can be misleading in scenarios with class imbalance. 

\begin{equation}\label{eq:accuracy}
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

Precision and recall address this shortcoming by emphasizing the performance on the positive class, i.e., the correct identification of same-author cases. 
Precision quantifies how often the systemâ€™s positive predictions are correct, while recall measures how often the true positive cases are successfully detected. 
Since these metrics often trade off against each other, the harmonic mean of precision and recall, known as the F1 score, is frequently adopted to provide a balanced assessment~\citep{neal_surveying_2018}.

Beyond these basic measures, \ac{av} research has introduced metrics that explicitly account for difficul borderline cases. 
The modified $F_{0.5u}$ score penalizes non-answers by treating them as \acp{fn}~\citep{bevendorff_overview_2024}. 
This places additional weight on correctly deciding same-author cases~\citep{weerasinghe_feature_vector_difference_2021}, thereby evaluating the ability of \ac{av} methods to abstain from hard samples~\citep{tyo_state_2022}. 
The $c@1$ metric, by contrast, was designed to reward abstention from particularly ambiguous cases. 
It does so by granting unanswered problems partial credit, equal to the average accuracy on the remaining cases~\citep{bevendorff_overview_2024}, thus encouraging systems to remain silent when uncertain.

However, in the present work, $c@1$ and $F_{0.5u}$ are not appropriate measures. 
The metrics assume that models can explicitly abstain by assigning a score of exactly 0.5, a convention used in \ac{pan} shared tasks~\citep{tyo_state_2022,bevendorff_overview_2024,kocher_unine_2015}. 
In our setting, the model produces only the binary outcomes "same author" or "don't know". 
Since there is no natural mechanism for producing a calibrated 0.5 score, abstention cannot be meaningfully represented. 
One could introduce a second threshold to create an artificial abstention region, but this would not align with the open-set nature of \ac{av}. 
The "different author" class is inherently ill-defined, as the set of possible authors is unbounded and cannot be exhaustively represented. 
Moreover, we do not report average precision across different recall values, since we do not need good results across different thresholds, but only one threshold which produces a high precision value.
For this reason, the focus in this thesis remains on precision, recall, and F1-based measures. 

% A well-known metric is accuracy which measures the percentage of classified correctly over all test cases~\citep{neal_surveying_2018}.
% The computation is given in \autoref{eq:accuracy}.
% Accuracy can produce misleading results if there is class imbalance.
% \begin{equation}\label{eq:accuracy}
%     $Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$
% \end{equation}

% If the focus is on getting positive class prediction correct, a better metric to use is precision as computed in 
% It measures how often a system gets positive classification correctly~\citep{neal_surveying_2018}.
% \begin{equation}\label{eq:precision}
%     $Precision = \frac{TP}{TP + FP}$
% \end{equation}

% Recall measures how often a system gets positive classification correctly~\citep{neal_surveying_2018}
% \begin{equation}\label{eq:recall}
%     $Recall = \frac{TP}{TP + FN}$
% \end{equation}


% F1~\citep{neal_surveying_2018}: Harmonic mean of precision and recall \citep{bevendorff_overview_2024,weerasinghe_feature_vector_difference_2021}.
%     A higher value indicates a better performance \citep{neal_surveying_2018}.
%     $F_{0.5u}$: Modified version of the $F_{0.5}$ score, where the non-answers are considered \acp{fn} \citep{bevendorff_overview_2024}. 
%     A measure that puts more emphasis on deciding same-author cases correctly \citep{weerasinghe_feature_vector_difference_2021}.
%     Used .
% \begin{equation}\label{eq:f1}
%     $F_1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}$
% \end{equation}



% c@1 where $np$ is the number of problems, $nc$ the number of correct answers, 
%     and $nu$ the number of unanswered problems \citep{kocher_unine_2015}. 
%     Modified version of the accuracy \citep{bevendorff_overview_2024}/ F1-score \citep{weerasinghe_feature_vector_difference_2021} score, 
%     where the non-answers (abstained) \citep{llm_detection_av_2025} are assigned the average accuracy of the remaining cases \citep{bevendorff_overview_2024}. 
%     It rewards systems that leave difficult problems unanswered \citep{weerasinghe_feature_vector_difference_2021}.
% \begin{equation}\label{eq:cat1}
%     $C@1 = \frac{nc}{np}(1+\frac{nu}{np})$
% \end{equation}

% $TP$ is the number of \aclp{tp}, $FP$ is the number of \aclp{fp} 
% and $FN$ is the number of \aclp{fn}~\citep{chen_web_2008}.



% % PAN AA & AV metrics: Abstaining from hard samples
% The F0.5u, C@1 metrics are used to evaluate the ability of \ac{av} methods 
% to abstain from hard samples \citep{tyo_state_2022}.
% For each sample, a score $\in [0, 1]$ is assigned to the sample.
% A score of exactly 0.5 means the model abstains from the sample \citep{tyo_state_2022,bevendorff_overview_2024,kocher_unine_2015}.

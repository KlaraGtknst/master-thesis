
We will present state-of-the-art quantitative evaluation measures for \ac{aa}, \ac{av} and paraphrase generation.
Ideally, quantitative measures are comparable and reproducible since they do not arise from human biased judgment.

\section{\ac{av} Quality Measures}
\label{sec:av_quality_measures}

Common metrics for evaluating the \ac{av} performance are:
\begin{itemize}
    \item $Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$ \citep{elmanarelbouanani_authorship_2014,neal_surveying_2018} 
    measures the percentage of classified correctly over all test cases \citep{neal_surveying_2018}.

    \item $Precision = \frac{TP}{TP + FP}$ \citep{elmanarelbouanani_authorship_2014,neal_surveying_2018,chen_web_2008} 
    measures how often a system gets positive classification correctly \citep{neal_surveying_2018}.

    \item $Recall = \frac{TP}{TP + FN}$ \citep{elmanarelbouanani_authorship_2014,neal_surveying_2018,chen_web_2008} 
    measures how often a system correctly classifies positive samples when it encounters them \citep{neal_surveying_2018}.
    Recall is also called sensitivity or \acl{tp} rate \citep{palivela_optimization_2021}

    \item $F-measure = \frac{2 \cdot precision \cdot recall}{precision + recall}$~\citep{chen_web_2008,abbasi_writeprints_2008}

    % PAN
    \item \ac{roc-auc} \citep{bevendorff_overview_2024,weerasinghe_feature_vector_difference_2021,kocher_unine_2015}
    where \ac{roc} plots \ac{fpr} $= \frac{FP}{FP+TN}$ on the x-axis against the \ac{tpr} $=\frac{TP}{TP+FN}$ on the y-axis 
    for varying thresholds \citep{kocher_unine_2015,neal_surveying_2018}.
    The maximum \ac{roc} value of 1.0 indicated a perfect performance \citep{kocher_unine_2015}.
    Greater \ac{auc} indicates a better performance \citep{neal_surveying_2018}.
    % The \ac{auc} of the \ac{roc} is biased since the \ac{roc} gives more emphasis 
    % on the first position and therefore increases the total \ac{auc}.
    % A misclassification with a lower probability is less penalized with \ac{roc-auc} \citep{kocher_unine_2015}.
    \citet{kocher_unine_2015} claim that both \ac{roc} and \ac{auc} are difficult to interpret.
    % LLM detection as AV task
    \citet{llm_detection_av_2025} argue that the reduction from the \ac{fpr}-\ac{tpr} curve of \ac{roc} to a single \ac{roc-auc} number 
    comes with information loss due to the absence of a fixed threshold and trade-off.
    Moreover, \ac{roc}'s \ac{fpr} and \ac{tpr} are independent of class prevalence, which is desirable.
    However, in highly imbalanced class scenarios \ac{roc} can be misleading (overly optimistic or pessimistic).
    
    \item BRIER: Complement of the Brier score \citep{bevendorff_overview_2024,weerasinghe_feature_vector_difference_2021}, 
    in \citet{bevendorff_overview_2024}'s case equivalent to the mean squared loss.
    The Brier score is used to evaluate the ability of \ac{av} methods to abstain from hard samples \citep{tyo_state_2022}.
    
    \item $C@1 = \frac{nc}{np}(1+\frac{nu}{np})$ where $np$ is the number of problems, $nc$ the number of correct answers, 
    and $nu$ the number of unanswered problems \citep{kocher_unine_2015}. 
    Modified version of the accuracy \citep{bevendorff_overview_2024}/ F1-score \citep{weerasinghe_feature_vector_difference_2021} score, 
    where the non-answers (abstained) \citep{llm_detection_av_2025} are assigned the average accuracy of the remaining cases \citep{bevendorff_overview_2024}. 
    It rewards systems that leave difficult problems unanswered \citep{weerasinghe_feature_vector_difference_2021}.
    
    \item $F_1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}$~\citep{neal_surveying_2018}: Harmonic mean of precision and recall \citep{bevendorff_overview_2024,weerasinghe_feature_vector_difference_2021}.
    A higher value indicates a better performance \citep{neal_surveying_2018}.
    
    \item $F_{0.5u}$: Modified version of the $F_{0.5}$ score, where the non-answers are considered \acp{fn} \citep{bevendorff_overview_2024}. 
    A measure that puts more emphasis on deciding same-author cases correctly \citep{weerasinghe_feature_vector_difference_2021}.
    Used to evaluate the ability of \ac{av} methods to abstain from hard samples \citep{tyo_state_2022}.
\end{itemize}
$TP$ is the number of \aclp{tp}, $FP$ is the number of \aclp{fp} 
and $FN$ is the number of \aclp{fn}~\citep{chen_web_2008}.



% PAN AA & AV metrics: Abstaining from hard samples
% The \todo{F0.5u, C@1, and Brier Score metrics} are used to evaluate the ability of \ac{av} methods 
% to abstain from hard samples \citep{tyo_state_2022}.
% For each sample, a score $\in [0, 1]$ is assigned to the sample.
% A score of exactly 0.5 means the model abstains from the sample \citep{tyo_state_2022,bevendorff_overview_2024,kocher_unine_2015}.

% The AUC metric is used to evaluate the ability of methods to rank predictions.
% No threshold is required.
% \ac{pan} ignores any abstained samples when calculating the AUC metric \citep{tyo_state_2022}.

% != PAN
% \citet{tyo_state_2022} chose to adopt the macro-averaged accuracy metric, so-called macro-accuracy, for \ac{aa}, 
% and AUC \ac{av} tasks.
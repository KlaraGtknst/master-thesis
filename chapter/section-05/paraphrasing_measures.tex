
\subsection{Paraphrase evaluation}
\label{subsec:paraphrase_evaluation}

Evaluating paraphrases can be reduced to central \ac{nlp} problems such as summarization or translation evaluation.
Yet, there is no universal definition of what constitutes a paraphrase. 
Definitions vary in degree of semantic equivalence required. 
This conceptual ambiguity makes the task of evaluation especially challenging, since different applications may prioritize different aspects such as fidelity to meaning, stylistic variation, or grammatical well-formedness.
Because of this, paraphrase evaluation cannot be reduced to the syntactic dimension alone. 
A meaningful assessment must account both for syntactic diversity and for the extent to which semantic content is preserved. 

Existing approaches can broadly be grouped into automatic and human-based methods. 
Automatic measures attempt to quantify the similarity or equivalence between a candidate and a reference paraphrase using algorithmic techniques. 
These methods can be further distinguished by the linguistic level at which they operate. 
Some focus on syntactic structure and word overlap, while others evaluate whether the meaning preservation. 
Human evaluation, in contrast, remains the gold standard, as it naturally incorporates all of these dimensions.

% Syntactic (\ac{bleu}, \ac{rouge}-1, \ac{rouge}-L), semantic (BERTScore, cosine similarity of SBERT vectors, WMS), human evaluation (TODO)

% There is manual (by humans) evaluation and automatic evaluation for paraphrase generation \citep{fu_learning_2024,zhou_paraphrase_2021}.
% According to \citet{zhou_paraphrase_2021}, automatic evaluation metrics mainly focus on the n-gram overlaps instead of meaning, 
% and hence, human evaluation is more accurate and has a higher quality.
% In the following, we focus on automatic evaluation.

In the following, we examine both automatic and human evaluation strategies. 
The automatic measures are divided into syntactic and semantic approaches, reflecting the different dimensions along which paraphrases can be compared. 
This is followed by a discussion of human evaluation, which complements automatic measures by providing a more comprehensive assessment of paraphrase quality.


\input{chapter/section-05/quantitative_evaluation_metrics.tex}


\subsubsection{Qualitative Evaluation}
\label{subsec:qualitative_evaluation}

Human qualitative evaluation can combine syntactic and semantic dimensions more reliable than any automatic metric proposed.
Naturally, when being asked to evaluate the quality of a paraphrase, individuals will score syntactic difference from the reference text, the readability from the paraphrase and semantic similarity to the reference text.
Evaluation is usually formalized via a Likert scale~\citep{gohsen_captions_2023}.

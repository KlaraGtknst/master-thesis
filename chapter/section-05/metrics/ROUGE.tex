
\paragraph{\ac{rouge}.}
\ac{rouge}, initially developed for summarisation evaluation, is recall-oriented and emphasises coverage of reference content in the candidate text~\citep{lin_rouge_2004}. 
Lower \ac{rouge} scores indicate greater diversity \citep{kurt_pehlivanoglu_comparative_2024}.
Several variants exist, including \ac{rouge}-N, which computes word n-gram recall, and \ac{rouge}-L, which measures the \ac{lcs}~\citep{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}. 

% ROUGE-N
\ac{rouge}-N is an n-gram recall between the candidate text $c$ and the reference text $r$~\citep{lin_rouge_2004}.
Since we only consider single reference scenarios, we use the simpler version for one reference in \Cref{eq:rouge_n}.
\begin{equation}
    \operatorname{ROUGE-N} = \sum_{n \text{-} gram \in r} \frac{\operatorname{Count_{match}}(n \text{-} gram)}{\operatorname{Count}(n \text{-} gram)}
\label{eq:rouge_n}
\end{equation}
Both the nominator and the denominator iterate over all n-grams in the reference text $r$.
$\operatorname{Count_{match}}(n \text{-} gram)$ is the number of co-occurrences of this n-gram in the reference text $r$ and the candidate text $c$.
The nominator sums $\operatorname{Count_{match}}(n \text{-} gram)$ over all n-grams in the reference text $r$, and is naturally capped by the total number of occurrences in the reference.

The denominator ignores matches and instead sums the total occurrences of each n-gram in the reference $r$~\citep{lin_rouge_2004}. 
This sum serves as a normalizer which ensures that \ac{rouge}-N values range between 0 and 1~\citep{kurt_pehlivanoglu_comparative_2024}.
%
If every n-gram from the reference $r$ would appear equally often in the candidate $c$, the \ac{rouge}-N would be one since it measures the n-gram overlap between reference and candidate from a reference or recall perspective.


The computation of \ac{rouge}-L for a candidate $c$ of length $n$ and a reference $r$ of length $m$ is defined in \Cref{eq:rouge_l}, where $\beta$ is defined as $\frac{\mathrm{P_{lcs}}}{\mathrm{R_{lcs}}}$.
The length is measured in number of words.
This F-measure incorporates precision $\mathrm{P_{lcs}}$, defined in \Cref{eq:rouge_l_precision}, and recall $\mathrm{R_{lcs}}$, defined in \Cref{eq:rouge_l_recall}.
The intuition is that the length of the \ac{lcs} between the candidate text $c$ and reference text $r$ correlates with their similarity.
\ac{rouge}-L does not include shorter sequences or alternative \ac{lcs} in the final score~\citep{lin_rouge_2004}.

\begin{equation}
\mathrm{P_{lcs}} = \frac{\operatorname{LCS}(r,c)}{n}
\label{eq:rouge_l_precision}
\end{equation}

\begin{equation}
\mathrm{R_{lcs}} = \frac{\operatorname{LCS}(r,c)}{m}
\label{eq:rouge_l_recall}
\end{equation}

\begin{equation}
\operatorname{ROUGE-L} = \frac{(1 + \beta^2)  \mathrm{R_{lcs}}  \mathrm{P_{lcs}}}{\mathrm{R_{lcs}} + \beta^2  \mathrm{P_{lcs}}}
\label{eq:rouge_l}
\end{equation}

\ac{rouge}-Lsum is a \ac{rouge}-L variant for evaluating multiple candidate texts $\mathcal{C}$.
While it follows the computation in \Cref{eq:rouge_l}, precision $\mathrm{P_{lcs}}$ and recall $\mathrm{R_{lcs}}$ replace $\operatorname{LCS}(r,c)$ with $\operatorname{LCS}_\cup(r,\mathcal{C})$.
$\operatorname{LCS}_\cup(r,C)$ unites $\operatorname{LCS}(r,c)$ for different candidate texts $c \in \mathcal{C}$.
For example, given reference $r=w_1 w_2 w_3 w_4 w_5$, and candidate texts $c_1 = w_1 w_2 w_6 w_7 w_8$ and $c_2 = w_1 w_3 w_8 w_9 w_5$, then $\operatorname{LCS}(r,c_1)=2$, because the \ac{lcs} of $r$ and $c_1$ is $w_1 w_2$.
Moreover, $\operatorname{LCS}(r,c_2)=3$, since $w_1 w_3 w_5$ is the \ac{lcs} of $r$ and $c_2$.
Hence, $\operatorname{LCS}_\cup(r,\mathcal{C})=4$, due to the union of \ac{lcs} containing $w_1 w_2 w_3 w_5$~\citep{lin_rouge_2004}.


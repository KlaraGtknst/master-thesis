
\paragraph{\ac{rouge}}
\ac{rouge} \citep{lin_rouge_2004}, initially developed for summarization, is recall-oriented and emphasizes coverage of reference content in the candidate text. 
Lower \ac{rouge} scores indicate greater diversity \citep{kurt_pehlivanoglu_comparative_2024}.
Several variants exist, including \ac{rouge}-N, which computes n-gram recall, and \ac{rouge}-L, which measures the \ac{lcs}~\citep{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}. 

% ROUGE-N
\ac{rouge}-N is an n-gram recall between the candidate text $c$ and the reference text $r$~\citep{lin_rouge_2004}.
Since we only consider single reference scenarios we use the simpler version for one reference in \autoref{eq:rouge_n}.
\begin{equation}
    \text{ROUGE-N} = \sum_{s \in r}\sum_{n-gram \in s} \frac{Count_{match}(n-gram)}{Count(n-gram)}
\label{eq:rouge_n}
\end{equation}
Both the nominator and the denominator iterate over all n-grams in all sentences of the reference, i.e. ground truth, text.
The nominator sums up $Count_{match}(n-gram)$, i.e. the maximum number of occurrences of this reference sentence n-gram in any of the candidate texts.
Since this sum measures the number of co-occurrences of that n-gram in both candidate and reference text, it is naturally capped to the number of occurrences in the reference.
This ensures that after normalizing with the denominator, the \ac{rouge}-N values range from 0 to 1~\citep{kurt_pehlivanoglu_comparative_2024}.
% The nominator sums over all references and thus, gives more weight to matching n-grams that occur in multiple references (i.e. a consensus between references) \citep{lin_rouge_2004}.    % do not use, bc we have only one reference
The denominator does not consider matches but only sums up the number of times a n-grams appears in the reference sentences~\citep{lin_rouge_2004}.
If every n-gram from the reference sentences would appear equally often in the candidate, the \ac{rouge}-N would be one since it measures the n-gram overlap between reference and candidate from a reference or recall perspective.

\ac{rouge}-L for a candidate $c$ of length $n$ and a reference $r$ of length $m$ is defined in \autoref{eq:rouge_l}.
The length is measured in number of words.
The intuition is that the length of the \ac{lcs} between the candidate and reference texts correlates with their similarity.
\ac{rouge}-L does not include shorter sequences or alternative \acp{lcs} in the final score~\citep{lin_rouge_2004}.
$\beta$ is defined as $\frac{P_{lcs}}{R_{lcs}}$.

\begin{equation}
P_{lcs} = \frac{LCS(r,c)}{n},
\label{eq:rouge_l_precision}
\end{equation}

\begin{equation}
R_{lcs} = \frac{LCS(r,c)}{m},
\label{eq:rouge_l_recall}
\end{equation}

\begin{equation}
\text{ROUGE-L} = \frac{(1 + \beta^2)R_{lcs}P_{lcs}}{R_{lcs} + \beta^2 P_{lcs}},
\label{eq:rouge_l}
\end{equation}

There is also a summary-level \ac{lcs} \ac{rouge} variant which computes \ac{rouge}-Lsum via summing over the union \ac{lcs} matches $LCS_\cup(r_i,C)$ between the reference summary sentences $r_i, i \in [1,u]$ and every candidate sentence $c_j, j \in [1,v]$.
The total number of words in the references and candidates is $m$ and $n$, respectively.
While the formula for the final score $F_{lcs}$ from \autoref{eq:rouge_l} remains the same, the computation of precision and recall replace $LCS(r,c)$ by $\sum_{i=1}^{u}LCS_\cup(r_i,C)$.

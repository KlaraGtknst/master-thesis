\paragraph{\ac{bleu}}
\ac{bleu}~\citep{papineni_bleu_2001} was originally developed for machine translation~\citep{zhou_paraphrase_2021,anantha_pearson_metrics_2021}. 
\ac{bleu}'s basic unit of evaluation is a sentence. 
\ac{bleu} is based on precision, i.e.\ computing the fraction of generated word n-grams $n\text{-}gram \in \mathcal{C}$ that appear in any reference text $r$~\citep{kurt_pehlivanoglu_comparative_2024,palivela_optimization_2021,papineni_bleu_2001,anantha_pearson_metrics_2021}. 
In our case, there is only one reference.
We consider the reference the ground truth text.
To prevent inflated precision scores $p_n$ due to repetition of frequent tokens (e.g.\ "the"), \ac{bleu} introduces a clipping mechanism $\operatorname{Count_{match}}(.)$ that caps n-gram counts at their maximum reference frequency~\citep{papineni_bleu_2001}. 
Precision $p_n$ for $n \in \mathbb{N}_{>0}$ is given by \Cref{eq:bleu}.

\begin{equation}
    p_n = \sum_{\mathcal{C} \in \left\{ Candidates \right\}}\sum_{n\text{-}gram \in\mathcal{C}} \frac{\operatorname{Count_{match}}(n\text{-}gram)}{\operatorname{Count}(n\text{-}gram)}
\label{eq:bleu}
\end{equation}

The choice of $n$ determines what syntactic characteristic is evaluated.
Uni-grams are used to test adequacy, while longer n-grams are used to test fluency~\citep{papineni_bleu_2001}. 
The brevity penalty $\operatorname{BP}$ from \Cref{eq:bleu_brevity_penalty} is applied to discourage excessively short candidates $c$~\citep{papineni_bleu_2001}.

\begin{equation}
    \operatorname{BP} = \begin{cases}
        1 & \text{if } \operatorname{len}(c) > \operatorname{len}(r) \\
        e^{1 - \frac{r}{c}} & \text{else}
    \end{cases}
\label{eq:bleu_brevity_penalty}
\end{equation}

In order to compute the \ac{bleu} score from \Cref{eq:bleu} for more than one sentence, 
one (1) computes the clipped n-gram matches sentence by sentence, 
then (2) adds them across all sentences, 
and finally (3) divides the total clipped n-gram matches by 
the total number of unclipped n-grams in all candidate sentences~\citep{papineni_bleu_2001,cordeiro_bleu_2007}.

Combined scores across different n-gram orders are computed via the geometric mean, weighted uniformly (i.e.\ $w_n$) across different $n$~\citep{papineni_bleu_2001,banerjee_METEOR_2005}.
Combining precision $p_n$ (\Cref{eq:bleu}) and brevity penalty $\operatorname{BP}$ (\Cref{eq:bleu_brevity_penalty}) leads to the final score in \Cref{eq:bleu_final}.

\begin{equation}
    \operatorname{BLEU} = \operatorname{BP}  \exp\left(\sum_{n=1}^{N} w_n  \log p_n\right)
\label{eq:bleu_final}
\end{equation}

\ac{bleu} disregards semantic similarity completely and therefore judges paraphrases only based on n-gram overlap. 
As such, it is generally recommended being supplemented with human evaluation~\citep{zhou_paraphrase_2021}.



% Its values range from 0 to 1 \citep{papineni_bleu_2001}.

% \ac{bleu} automatically penalises n-grams appearing in the candidate text but not in the reference text, 
% as well as n-grams appearing more often in the candidate than in the reference text \citep{papineni_bleu_2001}.

% For multiple sentences, they (1) add the best match (among the reference texts) length for each candidate sentence, 
% and (2) divide this sum $r$ by the total length of all candidate sentences $c$. 
% They cannot use recall for length-related problems here, 
% because \ac{bleu} uses multiple reference texts, which may have different lengths \citep{papineni_bleu_2001,banerjee_METEOR_2005}.
% If the generated candidate is significantly shorter than the reference text, the brevity penalty $\operatorname{BP}$ is applied.
% A \ac{bleu} score approaching 1 signifies the candidate matches one reference almost exactly \citep{papineni_bleu_2001}, 
% and thus, limited syntactic diversity (i.e.\ inadequate paraphrase) \citep{kurt_pehlivanoglu_comparative_2024}.
% Note that more reference texts lead to higher \ac{bleu} scores \citep{papineni_bleu_2001}.
\subsection{Traditional Quantitative Paraphrase Evaluation Measures}
\label{subsec:traditional_quantitative_evaluation_measures}

Evaluating paraphrases can be reduced to summarization or translation evaluation.
The evaluation of paraphrases can be divided into syntactic and semantic approaches. 
% \citet{gohsen_captions_2023} normalized all metrics and averaged the semantic and syntactic scores separately.

\subsubsection{Syntactic Measures}
Syntactic evaluation metrics mainly focus on the n-gram overlaps~\citep{zhou_paraphrase_2021}. 
Common syntactic evaluation metrics include \acs{bleu}, \acs{rouge}-1, and \acs{rouge}-L.

\input{chapter/section-05/metrics/BLEU.tex}
\input{chapter/section-05/metrics/ROUGE.tex}
\input{chapter/section-05/metrics/METEOR.tex}

Evaluating the Pearson correlation to human judgement on \num{10000} samples from web pages from the Wayback Machine and the Common Crawl dataset, \citet{anantha_pearson_metrics_2021} found that none of the metrics above has a higher Pearson correlation to human judgement than $0.64$.
% \citet{banerjee_METEOR_2005} publishes higher values on the Chinese portion of the Tides 2003 dataset.


\subsubsection{Semantic Measures}
Syntactic measures are inadequate when the goal is to evaluate paraphrases that prioritize semantic preservation over lexical similarity. 
To address this limitation, semantic metrics leverage distributed representations of words or sentences.
We compute semantic similarity between transformer based models~\citep{gohsen_captions_2023}.

\paragraph{BERTScore}
BERTScore~\citep{hanna_fine_grained_2021} computes similarity between contextual BERT embeddings of candidate and reference texts. 
For reference vectors $r$ and candidate vectors $c$, precision and recall are defined as \autoref{eq:bert_p} and \autoref{eq:bert_r}, respectively.

\begin{equation}
    P_{BERT} = \frac{1}{|c|} \sum_{c_i \in c} \max_{z_j \in r} r_j\top c_i
\label{eq:bert_p}
\end{equation}
\begin{equation}
    R_{BERT} = \frac{1}{|r|} \sum_{r_i \in r} \max_{c_j \in c} r_i\top c_j
\label{eq:bert_r}
\end{equation}

% \begin{equation}
%     F_1 = \frac{2 P_{BERT} R_{BERT}}{P_{BERT} + R_{BERT}} 
% \label{eq:bert_f1}
% \end{equation}
% Since $F_1 \in \left[-1,1\right]$ it can be rescaled to $[0,1]$ by modifying the precision and recall calculation 
% to $\hat{P}_{BERT} = \frac{P_{BERT} - a}{1 - a}$ ($R_{BERT}$ analogous), where $a$ is the empirical lower bound on the BERTScore \citep{hanna_fine_grained_2021}.

BERTScore correlates with human judgment at the semantic level \citep{kurt_pehlivanoglu_comparative_2024}, although it may struggle when lexically overlapping but semantically incorrect candidates are present \citep{hanna_fine_grained_2021}.

\paragraph{SBERT cosine similarity}
The cosine similarity between dense vector representations $\mathcal{v_a}, \mathcal{v_b}$ of two documents $a,b$ is computed in \autoref{eq:cosine_sim}~\citep{thongtan_cosine_sim_19}.
The vector representations are computed using a SBERT model~\citep{gohsen_captions_2023}.

\begin{equation}
    cos(\theta_{a,b})=sim(\mathcal{v_a},\mathcal{v_b})=\frac{\mathcal{v_a}^T\mathcal{v_b}}{\left\| \mathcal{v_a} \right\|\left\| \mathcal{v_b} \right\|}
    \label{eq:cosine_sim}
\end{equation}


\paragraph{\ac{wmd}}
\ac{wmd} measures the minimal transport cost of aligning word embeddings from one text to another~\citep{gohsen_captions_2023}. 
\citet{kusner_wmd_15} formalize this via a flow matrix $T \in \mathcal{R}^{n \times n}$ where $T_{ij} \geq 0$ denotes how much of word $i$ in a document $d$ must travel to a word $j$ in a document $d'$.
To transform document $d$ to document $d'$, (1) the outgoing flow from word $i$ equals $d_i$, i.e. $\sum_{j}T_{ij}=d_i$, and (2) the incoming flow to word $j$ must match $d'_j$, i.e. $\sum_{i}T_{ij}=d'_j$.
The distance between document $d$ and document $d'$ is the minimum cumulative cost required to move all words from $d$ to $d'$, i.e. $\sum_{i,j}T_{i,j}c(i,j)$, where $c(i,j)$ is the cost of travelling from word $i$ to word $j$~\citep{kusner_wmd_15}.


\subsubsection{Gohsen Delta $\Delta_{sem,syn}$}
First, all syntactic and semantic measures are normalized to a scale from zero to one.
Then, the average syntactic similarity $\diameter_{syn}$ and the average semantic similarity $\diameter_{sem}$ is calculated.
Syntactic metrics include \ac{rouge}-1, \ac{rouge}-L, and \ac{bleu}.
Semantic measures include \ac{wms}, BERT, and cosine similarity of the SBERT embeddings.
Finally, $\Delta_{sem,syn}$ is defined as in \autoref{eq:gohsen_delta}, i.e. the difference of semantic and syntactic average distance~\citep{gohsen_captions_2023}.
\begin{equation}
    \Delta_{sem,syn}=\diameter_{sem}-\diameter_{syn}
    \label{eq:gohsen_delta}
\end{equation}
Hence, high $\Delta_{sem,syn}$ values indicate structurally and lexically diverse and semantically similar text pairs.

% \subsection{Traditional Quantitative Paraphrase Evaluation}
% \label{subsec:traditional_quantitative_evaluation_measures}

% Evaluating paraphrases can be reduced to summarisation or translation evaluation.
% The evaluation of paraphrases can be divided into syntactic and semantic approaches. 
% \citet{gohsen_captions_2023} normalized all metrics and averaged the semantic and syntactic scores separately.

\subsubsection{Syntactic Measures}
Syntactic evaluation metrics mainly focus on the n-gram overlaps~\citep{zhou_paraphrase_2021}. 
Common syntactic evaluation metrics include \acs{bleu}, \acs{rouge}-1, \acs{rouge}-L, and METEOR.

\input{chapter/section-05/metrics/BLEU.tex}
\input{chapter/section-05/metrics/ROUGE.tex}
\input{chapter/section-05/metrics/METEOR.tex}

Evaluating the Pearson correlation to human judgement on \num{10000}~samples from web pages from the Wayback Machine and the Common Crawl dataset, \citet{anantha_pearson_metrics_2021}\ found that none of \ac{bleu}, \ac{rouge}, and METEOR has a higher Pearson correlation to human judgement than $0.64$.
% \citet{banerjee_METEOR_2005} publishes higher values on the Chinese portion of the Tides 2003 dataset.


\subsubsection{Semantic Measures}
Syntactic measures are inadequate when the goal is to evaluate paraphrases that prioritise semantic preservation over lexical similarity. 
To address this limitation, semantic metrics leverage distributed representations of words or sentences.
As per \citet{gohsen_captions_2023}, we compute semantic similarity between transformer based models.

\paragraph{\acs{bert}Score}
\acs{bert}Score computes similarity between contextual \ac{bert} embeddings of candidate and reference texts~\citep{hanna_fine_grained_2021}. 
Contextual embeddings allow for the same word having different embeddings depending on its context.
Even though cosine similarity calculation considers tokens in isolation, the contextual embeddings contain information of the rest of the sentence.
Sentences are tokenized by \ac{bert} and subsequently encoded.
For computing precision, each token embedding $c_i$ in the candidate $c$ is matched to a token embedding $r_j$ in the reference $r$.
Analogous, for recall calculation, each token embedding $r_i$ in the reference $r$ is matched to a token embedding $c_j$ in the candidate $c$.
Matching is carried out based on similarity in a greedy fashion~\citep{zhang_bertscore_2020}.
For reference vectors $r$ and candidate vectors $c$, precision $P_{\text{BERT}}$ and recall $R_{\text{BERT}}$ are defined in \Cref{eq:bert_p} and \Cref{eq:bert_r}, respectively.
The $F_1$ score from \Cref{eq:bert_f1} is computed based on precision and recall.

\begin{equation}
    \operatorname{P_{BERT}} = \frac{1}{|c|} \sum_{c_i \in c} \max_{z_j \in r} r_j^\top c_i
\label{eq:bert_p}
\end{equation}

\begin{equation}
    \operatorname{R_{BERT}} = \frac{1}{|r|} \sum_{r_i \in r} \max_{c_j \in c} r_i^\top c_j
\label{eq:bert_r}
\end{equation}

\begin{equation}
    \operatorname{F_1} = \frac{2  \operatorname{P_{BERT}}  \operatorname{R_{BERT}}}{\operatorname{P_{BERT}} + \operatorname{R_{BERT}}} 
\label{eq:bert_f1}
\end{equation}
Since $\operatorname{F_1} \in \left[\text{-}1,1\right]$, it can be rescaled to $[0,1]$ to improve score readability by modifying the precision and recall calculation 
to $\mathrm{\hat{P}_{BERT}} = \frac{\operatorname{P_{BERT}} - a}{1 - a}$ ($\operatorname{R_{BERT}}$ analogous), where $a$ is the empirical lower bound on the BERTScore \citep{zhang_bertscore_2020,hanna_fine_grained_2021}.

% BERTScore correlates with human judgment at the semantic level \citep{kurt_pehlivanoglu_comparative_2024}, although it may struggle when lexically overlapping but semantically incorrect candidates are present \citep{hanna_fine_grained_2021}.

\paragraph{\acs{sbert} cosine similarity}
The cosine similarity between vector representations $v_a$ and $v_b$ of two documents $a$ and $b$ is defined in \Cref{eq:cosine_sim}. 
Cosine similarity values range from $-1$ to $1$~\citep{thongtan_cosine_sim_19,zhang_bertscore_2020}, where $-1$ indicates that $v_a$ and $v_b$ point in opposite directions, $0$ indicates no correlation, and $1$ indicates that they point in the same direction. 
Following \citet{gohsen_captions_2023}, we compute our document vectors using a \ac{sbert} model.

\begin{equation}
    \cos(\theta_{a,b})=\mathrm{sim}(v_a,v_b)=\frac{v_a^Tv_b}{\left\| v_a \right\|\left\| v_b \right\|}
    \label{eq:cosine_sim}
\end{equation}


\paragraph{\ac{wmd}}
\ac{wmd} measures the minimal transport cost of aligning word embeddings from one text to another~\citep{gohsen_captions_2023}. 
By default, word embeddings are derived using \acs{glove}~\citep{glove_wmd_2014}.
\citet{kusner_wmd_15}\ formalise this via a flow matrix $T \in \mathcal{R}^{n \times n}$ where $T_{ij} \geq 0$ denotes how much of word $i$ in a document $d$ must travel to a word $j$ in a document $d'$.
To transform document $d$ to document $d'$, (1) the outgoing flow from word $i$ equals $d_i$, i.e.\ $\sum_{j}T_{ij}=d_i$, and (2) the incoming flow to word $j$ must match $d'_j$, i.e.\ $\sum_{i}T_{ij}=d'_j$.
The distance between document $d$ and document $d'$ is the minimum cumulative cost required to move all words from $d$ to $d'$, i.e.\ $\sum_{i,j}T_{i,j}c(i,j)$, where $c(i,j)$ is the cost of travelling from word $i$ to word $j$~\citep{kusner_wmd_15}.


\subsubsection{Semanticâ€“Syntactic Metric}

\citet{gohsen_captions_2023}\ introduce the metric $\Delta_{sem,syn}$ as a means to improve the interpretability of paraphrasing scores by jointly considering syntactic and semantic similarity.
First, all syntactic and semantic measures are normalised to a scale from zero to one.
Then, the average syntactic similarity $\diameter_{syn}$ and the average semantic similarity $\diameter_{sem}$ is calculated.
Syntactic metrics include \ac{rouge}-1, \ac{rouge}-L, and \ac{bleu}.
Semantic measures include \ac{wms}, \ac{bert}, and cosine similarity of the \ac{sbert} embeddings.
\ac{wms} is the inversion of \ac{wmd}.
Finally, $\Delta_{sem,syn}$ is defined as in \Cref{eq:gohsen_delta}, i.e.\ the difference of semantic and syntactic average distance~\citep{gohsen_captions_2023}.
\begin{equation}
    \Delta_{sem,syn}=\diameter_{sem}-\diameter_{syn}
    \label{eq:gohsen_delta}
\end{equation}
Hence, high $\Delta_{sem,syn}$ values indicate structurally and lexically diverse and semantically similar text pairs.

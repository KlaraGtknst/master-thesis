\subsection{Traditional Quantitative Paraphrase Evaluation Measures}
\label{subsec:traditional_quantitative_evaluation_measures}



There is syntactic and semantic evaluation of paraphrases \citep{gohsen_captions_2023}.
% Metrics for syntactic evaluation include \ac{bleu}, \ac{rouge}-1, \ac{rouge}-L, 
% while metrics for semantic similarity include BERTScore, 
% cosine similarity of dense vector representations derived from a BERT-based sentence transformer, 
% and \ac{wmd} \citep{gohsen_captions_2023}.
% The \ac{wmd} computes the minimum amount of distance that embedded words of a text need to travel 
% to reach the embedded words of another text \citep{gohsen_captions_2023}.
% \citet{gohsen_captions_2023} normalized all metrics and averaged the semantic and syntactic scores separately.
Syntactic evaluation metrics mainly focus on the n-gram overlaps~\citet{zhou_paraphrase_2021}. 
Common syntactic evaluation metrics include \ac{bleu}, \ac{meteor} \ac{rouge}-1, \ac{rouge}-L.
\textcolor{red}{\citet{kurt_pehlivanoglu_comparative_2024} claims \ac{meteor} is a semantic metric.}

\ac{bleu} (2002) was developed for machine translation \citep{palivela_optimization_2021,zhou_paraphrase_2021,papineni_bleu_2001}.
Its values range from 0 to 1 \citep{papineni_bleu_2001}.
\ac{bleu} is a precision measure \citep{kurt_pehlivanoglu_comparative_2024,papineni_bleu_2001}.
It counts the matching n-grams (unigrams) in the generated/candidate text that appear in any of the gold/ reference texts \citep{palivela_optimization_2021,papineni_bleu_2001}, 
and then divides them by the total number of n-grams (unigrams) in the candidate text \citep{papineni_bleu_2001}.
Since candidates consisting only of high-probability n-grams (e.g. "the") would receive a high score without deserving it, 
\citet{papineni_bleu_2001} introduced a clipping mechanism to limit the count of n-grams in the candidate text 
to the maximum count of that n-gram in any of the reference texts.
The clipped n-grams occurrences are added up and divided by the total number 
of unclipped n-grams in the candidate text \citep{papineni_bleu_2001}.
\citet{papineni_bleu_2001} state that unigrams are used to test adequacy, while longer n-grams are used to test fluency.
\ac{bleu}'s basic unit of evaluation is a sentence. 
In order to compute the \ac{bleu} score from \autoref{eq:bleu} for more than one sentence, 
one (1) computes the n-grams matches sentence by sentence, 
then (2) adds the clipped n-grams matches across all sentences, 
and finally (3) divides the total clipped n-grams matches by 
the total number of unclipped n-grams in all candidate sentences \citep{papineni_bleu_2001}.
\begin{equation}
    p_n = \frac{\sum_{\mathcal{C} \in \left\{ Candidates \right\}}\sum_{n-gram \in\mathcal{C}}Count_{clip}(n-gram)}{\sum_{\mathcal{C'} \in \left\{ Candidates \right\}}\sum_{n-gram' \in\mathcal{C'}}Count(n-gram')}
\label{eq:bleu}
\end{equation}
\ac{bleu} combines the scores for different n-grams (separately computed) using the average logarithm with uniform weights, 
which is equivalent to using the geometric mean of the scores \citep{papineni_bleu_2001,banerjee_METEOR_2005}.
\citet{gohsen_captions_2023} use up to 4-grams.
\ac{bleu} automatically penalizes n-grams appearing in the candidate text but not in the reference text, 
as well as n-grams appearing more often in the candidate than in the reference text \citep{papineni_bleu_2001}.
According to \citet{papineni_bleu_2001}, they need to add a brevity penalty to the \ac{bleu} score 
to enforce proper length of the candidate text. 
Hence, the brevity penalty $BP$ is defined as follows in \autoref{eq:bleu_brevity_penalty}:
\begin{equation}
    BP = \begin{cases}
        1 & \text{if } c > r \\
        e^{1 - \frac{r}{c}} & \text{if } c \leq r
    \end{cases}
\label{eq:bleu_brevity_penalty}
\end{equation}
Combining all these, the final \ac{bleu} score is computed as follows in \autoref{eq:bleu_final}:
\begin{equation}
    \text{BLEU} = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \cdot \log p_n\right)
\label{eq:bleu_final}
\end{equation}
For multiple sentences, they (1) add the best match (among the reference texts) length for each candidate sentence, 
and (2) divide this sum $r$ by the total length of all candidate sentences $c$. 
They cannot use recall for length-related problems here, 
because \ac{bleu} uses multiple reference texts, which may have different lengths \citep{papineni_bleu_2001,banerjee_METEOR_2005}.
If the generated candidate is significantly shorter than the reference text, the brevity penalty $BP$ is applied.
A \ac{bleu} score approaching 1 signifies the candidate matches one reference almost exactly \citep{papineni_bleu_2001}, 
and thus, limited syntactic diversity (i.e. inadequate paraphrase) \citep{kurt_pehlivanoglu_comparative_2024}.
Note that more reference texts lead to higher \ac{bleu} scores \citep{papineni_bleu_2001}.
Unigrams are token-wise and bi-grams are word-pairs \citet{palivela_optimization_2021}.
According to \citet{zhou_paraphrase_2021}'s survey, \ac{bleu} is the most frequently used metric for paraphrase generation.
\ac{bleu} is unable to measure semantic equivalents \citep{kurt_pehlivanoglu_comparative_2024,zhou_paraphrase_2021} 
when applied to low-resource languages \citep{zhou_paraphrase_2021}.
Moreover, \ac{bleu} fails to capture good paraphrases that are not similar to the reference text \citep{zhou_paraphrase_2021}.
\citet{kurt_pehlivanoglu_comparative_2024} found that \ac{bleu} tends to overestimate the quality of paraphrases.
\citet{zhou_paraphrase_2021} suggest combining \ac{bleu} with human evaluation to overcome its limitations.


\ac{meteor} (2014) aims to address \ac{bleu}'s shortcomings.
Its values range from 0 to 1 \citep{kurt_pehlivanoglu_comparative_2024}.
First a mapping, so-called alignment, between the unigrams in the candidate text and the reference texts is created \citep{banerjee_METEOR_2005}.
Each unigram has zero or one match.
This alignment is created incrementally in repeating two steps:
(1) List all possible unigram mappings derived from different modules (i.e. exact matches, Porter stemmed matches, synonym matches), 
and (2) select the largest subset of unigram mappings that constitute a valid alignment 
(matches obtained from different modules are treated the same).
(3) Choose the subset with the largest cardinality and if there are multiple, 
choose the one with the fewest unigram mapping crosses \citep{banerjee_METEOR_2005}.
\ac{meteor} computes a weighted F-score 
(unigram-precision, unigram-recall \citep{kurt_pehlivanoglu_comparative_2024,banerjee_METEOR_2005} 
and a measure of fragmentation \citep{banerjee_METEOR_2005,kurt_pehlivanoglu_comparative_2024})
with a penality function whenever an incorrect word is encountered \citep{palivela_optimization_2021} as displayed in \autoref{eq:meteor}.
\begin{equation}
    METEOR = F_{mean} = \frac{10 \cdot P \cdot R}{R + 9P} \cdot (1 - Penalty)
\label{eq:meteor}
\end{equation}
The penalty is designed to reduce the $F_{mean}$ score to $50\%$ if there are no bigram or longer matches \citep{banerjee_METEOR_2005}.
It has better correlation with human judgment at the sentence/segment level than \ac{bleu} \citep{zhou_paraphrase_2021}, 
because it not only consists of simple n-gram matching but also including synonymy and stemming \citep{kurt_pehlivanoglu_comparative_2024}.

\ac{rouge} (2004) is a recall-based metric developed for text summarization 
\citep{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024,lin_rouge_2004}.
Its values range from 0 to 1 \citep{kurt_pehlivanoglu_comparative_2024}.
\ac{rouge} can focus on the word variations and diversity.
It has multiple versions, the most popular ones include 
\ac{rouge}-N (computing the n-gram recall) \citep{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}, 
\ac{rouge}-L (computing the longest common subsequence) \citep{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}, 
\ac{rouge}-W (Weighted longest common subsequence) \citep{palivela_optimization_2021}, 
\ac{rouge}-S (skip-bigram co-occurrence statistics) \citep{palivela_optimization_2021}.
\ac{rouge}-1 computes the recall by analysing the matching unigrams between the generated paraphrase and the reference paraphrase \citep{palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}.
% ROUGE-N
\ac{rouge}-N is an n-gram recall between the candidate text and the reference texts \citep{lin_rouge_2004} as displayed in \autoref{eq:rouge_n}.
\begin{equation}
    ROUGE-N = \frac{\sum_{\mathcal{S} \in \left\{ References \right\}}\sum_{n-gram \in\mathcal{S}}Count_{match}(n-gram)}{\sum_{\mathcal{S'} \in \left\{ References \right\}}\sum_{n-gram' \in\mathcal{S'}}Count(n-gram')}
\label{eq:rouge_n}
\end{equation}
$Count_{match}(n-gram)$ is the maximum number of n-grams co-occuring in the candidate text and the set of reference texts \citep{lin_rouge_2004}.
The nominator sums over all references and thus, gives more weight to matching n-grams that occur in multiple references (i.e. a consensus between references) \citep{lin_rouge_2004}.
Refer to \citet{lin_rouge_2004} for more details on the work with multiple references (I do not understand that, because I thought we already use multiple).
% ROUGE-L
For \ac{rouge}-L, the intuition is that the longer the longest common subsequence (LCS) between the candidate and reference texts, the more similar they are \citep{lin_rouge_2004}.
For a candidate $Y$ of length $n$ and a reference $X$ of length $m$, the \ac{rouge}-L score is defined as follows in \autoref{eq:rouge_l}:
\begin{equation}
    ROUGE-L = F_{lcs} = \frac{(1 + \beta^2)R_{lcs}P_{lcs}}{R_{lcs} + \beta^2 P_{lcs}}
\label{eq:rouge_l}
\end{equation}
where $R_{lcs} = \frac{LCS(X,Y)}{m}$ and $P_{lcs} = \frac{LCS(X,Y)}{n}$ \citep{lin_rouge_2004}.
\ac{rouge}-L requires in-sequence matches that reflect the sentence level word order as n-grams \citep{lin_rouge_2004}.
Moreover, no predefined $n$ is necessary, because \ac{rouge}-L includes the longest in-sequence common n-grams \citep{lin_rouge_2004}.
However, \ac{rouge}-L does not include shorter sequences or alternative LCSes in the final score \citep{lin_rouge_2004}.
% ROUGE-S
A skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps \citep{lin_rouge_2004}.
\ac{rouge}-S measures the overlap of skip-bigrams between the candidate text and the reference texts \citep{lin_rouge_2004}.
Hence, if the candidate text is the reverse of the reference text, the \ac{rouge}-S score is 0 even though it is not as bad as completely unrelated candidates \citep{lin_rouge_2004}.
\ac{rouge}-SU extends \ac{rouge}-S with unigrams to solve this issue \citep{lin_rouge_2004}.
% ROUGE generally
\citet{kurt_pehlivanoglu_comparative_2024} claim that \ac{rouge} may not be adequate to assess semantic similarity and fluency.
Lower \ac{rouge} scores indicate greater diversity \citep{kurt_pehlivanoglu_comparative_2024}.

Common metrics for semantic similarity include BERTScore, 
cosine similarity of dense vector representations derived from a BERT-based sentence transformer, 
and \ac{wmd} \citep{gohsen_captions_2023}.

BERTScore calculates the cosine similarity between the contextual embeddings of the reference and generated texts. 
Hence, is assesses semantic equivalence and correlates well with human judgment \citep{kurt_pehlivanoglu_comparative_2024}.
First, token vector representations are computed for both the reference and generated texts using a pre-trained BERT model \citep{hanna_fine_grained_2021}.
Let reference $z$ and candidate $\hat{z}$ be the vector representations of the reference and candidate texts, respectively.
Then, the BERTScore precision, recall and $F_1$ score is computed as follows in \autoref{eq:bert_p}, \autoref{eq:bert_r}, and \autoref{eq:bert_f1}, respectively:
\begin{equation}
    P_{BERT} = \frac{1}{|\hat{z}|} \sum_{\hat{z}_j \in \hat{z}} \max_{z_j \in z} z_i\top \hat{z}_j
\label{eq:bert_p}
\end{equation}
\begin{equation}
    R_{BERT} = \frac{1}{|z|} \sum_{z_j \in z} \max_{\hat{z}_j \in \hat{z}} z_i\top \hat{z}_j
\label{eq:bert_r}
\end{equation}
\begin{equation}
    F_1 = \frac{2 P_{BERT} R_{BERT}}{P_{BERT} + R_{BERT}} 
\label{eq:bert_f1}
\end{equation}
Since $F_1 \in \left[-1,1\right]$ it can be rescaled to $[0,1]$ by modifying the precision and recall calculation 
to $\hat{P}_{BERT} = \frac{P_{BERT} - a}{1 - a}$ ($R_{BERT}$ analogous), where $a$ is the empirical lower bound on the BERTScore \citep{hanna_fine_grained_2021}.
% The BERTScore has difficulties on datasets with lexically similar (i.e. lexical overlap of content words) incorrect candidates 
% opposed to lexically different more correct candidates \citep{hanna_fine_grained_2021}.

The \ac{wmd} computes the minimum amount of distance that embedded words of a text need to travel 
to reach the embedded words of another text \citep{gohsen_captions_2023}.


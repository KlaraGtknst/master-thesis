\subsection{Traditional Quantitative Paraphrase Evaluation Measures}
\label{subsec:traditional_quantitative_evaluation_measures}

Evaluating paraphrases can be reduced to summarization or translation evaluation.
The evaluation of paraphrases can be divided into syntactic and semantic approaches. 
% \citet{gohsen_captions_2023} normalized all metrics and averaged the semantic and syntactic scores separately.

\subsubsection{Syntactic Measures}
Syntactic evaluation metrics mainly focus on the n-gram overlaps~\citep{zhou_paraphrase_2021}. 
Common syntactic evaluation metrics include \acs{bleu}, \acs{rouge}-1, and \acs{rouge}-L.

\ac{bleu}~\citep{papineni_bleu_2001} was originally developed for machine translation~\citep{zhou_paraphrase_2021}. 
\ac{bleu}'s basic unit of evaluation is a sentence. 
\ac{bleu} is based on precision, i.e. computing the fraction of generated n-grams that appear in any reference text~\citep{kurt_pehlivanoglu_comparative_2024,palivela_optimization_2021,papineni_bleu_2001}. 
To prevent inflated precison scores $p_n$ due to repetition of frequent tokens (e.g. "the"), \ac{bleu} introduces a clipping mechanism that caps the count of n-grams at their maximum reference frequency~\citep{papineni_bleu_2001}. 
Precision $p_n$ for $n \in \mathbb{N}_{>0}$ is given by \autoref{eq:bleu}.

In order to compute the \ac{bleu} score from \autoref{eq:bleu} for more than one sentence, 
one (1) computes the n-grams matches sentence by sentence, 
then (2) adds the clipped n-grams matches across all sentences, 
and finally (3) divides the total clipped n-grams matches by 
the total number of unclipped n-grams in all candidate sentences \citep{papineni_bleu_2001}.

\begin{equation}
    p_n = \frac{\sum_{\mathcal{C} \in \left\{ Candidates \right\}}\sum_{n-gram \in\mathcal{C}}Count_{clip}(n-gram)}{\sum_{\mathcal{C'} \in \left\{ Candidates \right\}}\sum_{n-gram' \in\mathcal{C'}}Count(n-gram')}
\label{eq:bleu}
\end{equation}

The choice of $n$ determines what syntactic charcteristic is evaluated.
Unigrams are used to test adequacy, while longer n-grams are used to test fluency~\citep{papineni_bleu_2001}. 
The brevity penalty $BP$ from \autoref{eq:bleu_brevity_penalty} is applied to discourage excessively short candidates~\citep{papineni_bleu_2001}.

\begin{equation}
    BP = \begin{cases}
        1 & \text{if } c > r \\
        e^{1 - \frac{r}{c}} & \text{if } c \leq r
    \end{cases}
\label{eq:bleu_brevity_penalty}
\end{equation}

Combined scores across different n-gram orders are computed via the geometric mean, weighted uniformly (i.e. $w_n$) across different $n$~\citep{papineni_bleu_2001,banerjee_METEOR_2005}.
Combining precision $p_n$ (\autoref{eq:bleu}) and brevity penalty $BP$ (\autoref{eq:bleu_brevity_penalty}) leads to the final score in \autoref{eq:bleu_final}.

\begin{equation}
    \text{BLEU} = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \cdot \log p_n\right)
\label{eq:bleu_final}
\end{equation}

\ac{bleu} disregards semantic similarity completely and therefore judges paraphrases only based on n-gram overlap. 
As such, it is generally recommended being supplemented with human evaluation~\citep{zhou_paraphrase_2021}.



% Its values range from 0 to 1 \citep{papineni_bleu_2001}.

% \ac{bleu} automatically penalizes n-grams appearing in the candidate text but not in the reference text, 
% as well as n-grams appearing more often in the candidate than in the reference text \citep{papineni_bleu_2001}.

% For multiple sentences, they (1) add the best match (among the reference texts) length for each candidate sentence, 
% and (2) divide this sum $r$ by the total length of all candidate sentences $c$. 
% They cannot use recall for length-related problems here, 
% because \ac{bleu} uses multiple reference texts, which may have different lengths \citep{papineni_bleu_2001,banerjee_METEOR_2005}.
% If the generated candidate is significantly shorter than the reference text, the brevity penalty $BP$ is applied.
% A \ac{bleu} score approaching 1 signifies the candidate matches one reference almost exactly \citep{papineni_bleu_2001}, 
% and thus, limited syntactic diversity (i.e. inadequate paraphrase) \citep{kurt_pehlivanoglu_comparative_2024}.
% Note that more reference texts lead to higher \ac{bleu} scores \citep{papineni_bleu_2001}.

\ac{rouge} \citep{lin_rouge_2004}, initially developed for summarization, is recall-oriented and emphasizes coverage of reference content in the candidate text. 
Several variants exist, including \ac{rouge}-N, which computes n-gram recall, \ac{rouge}-L, which measures the longest common subsequence, and \ac{rouge}-S, which evaluates skip-bigram co-occurrences~\citep{zhou_paraphrase_2021,palivela_optimization_2021,kurt_pehlivanoglu_comparative_2024}. 
\ac{rouge}-L for a candidate $Y$ of length $n$ and a reference $X$ of length $m$, for example, is defined in \autoref{eq:rouge_l}.
The intuition is that the length of the longest common subsequence (LCS) between the candidate and reference texts correlates with their similarity \citep{lin_rouge_2004}.
% \ac{rouge}-L requires in-sequence matches that reflect the sentence level word order as n-grams.
% Moreover, no predefined $n$ is necessary, because \ac{rouge}-L includes the longest in-sequence common n-grams.
% However, \ac{rouge}-L does not include shorter sequences or alternative LCSes in the final score~\citep{lin_rouge_2004}.

\begin{equation}
ROUGE-L = \frac{(1 + \beta^2)R_{lcs}P_{lcs}}{R_{lcs} + \beta^2 P_{lcs}},
\label{eq:rouge_l}
\end{equation}

$R_{lcs} = \frac{LCS(X,Y)}{m}$ and $P_{lcs} = \frac{LCS(X,Y)}{n}$ capture recall and precision based on subsequence length. 

% Its values range from 0 to 1 \citep{kurt_pehlivanoglu_comparative_2024}.
% ROUGE-N
\ac{rouge}-N is an n-gram recall between the candidate text and the reference texts \citep{lin_rouge_2004} as displayed in \autoref{eq:rouge_n}.
\begin{equation}
    ROUGE-N = \frac{\sum_{\mathcal{S} \in \left\{ References \right\}}\sum_{n-gram \in\mathcal{S}}Count_{match}(n-gram)}{\sum_{\mathcal{S'} \in \left\{ References \right\}}\sum_{n-gram' \in\mathcal{S'}}Count(n-gram')}
\label{eq:rouge_n}
\end{equation}
\textcolor{red}{$Count_{match}(n-gram)$ is the maximum number of n-grams co-occuring in the candidate text and the set of reference texts.
The nominator sums over all references and thus, gives more weight to matching n-grams that occur in multiple references (i.e. a consensus between references) \citep{lin_rouge_2004}.}

% ROUGE-S
A skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps.
\ac{rouge}-S measures the overlap of skip-bigrams between the candidate text and the reference texts.
Hence, if the candidate text is the reverse of the reference text, the \ac{rouge}-S score is 0 even though it is not as bad as completely unrelated candidates.
\ac{rouge}-SU extends \ac{rouge}-S with unigrams to solve this issue \citep{lin_rouge_2004}.
% ROUGE generally
% \citet{kurt_pehlivanoglu_comparative_2024} claim that \ac{rouge} may not be adequate to assess semantic similarity and fluency.
Lower \ac{rouge} scores indicate greater diversity \citep{kurt_pehlivanoglu_comparative_2024}.

METEOR was proposed to address \ac{bleu}'s deficiencies. 
Unlike \ac{bleu}, which is precision-oriented, METEOR explicitly incorporates recall.
We consider METEOR a syntactic metric due to its conceptual similarity to \ac{bleu}, but it also captures semantic characteristics on account of inclusion of stemming and synonym matching modules~\citep{kurt_pehlivanoglu_comparative_2024}. 
Candidate and reference unigrams are first aligned based on modules (exact match, Porter stemmed match, synonymy), after which the best subset of unigram mappings is selected according to cardinality and minimal crossing. 
From this alignment, METEOR computes the weighted $F$-score from \autoref{eq:meteor}~\citep{banerjee_METEOR_2005}.

\begin{equation}
    METEOR = F_{mean} = \frac{10 \cdot P \cdot R}{R + 9P} \cdot (1 - Penalty)
\label{eq:meteor}
\end{equation}

The penalty function discourages fragmented matches and reduces the score to $50\%$ bigram or longer matches are absent~\citep{banerjee_METEOR_2005}. 
METEOR has been shown to correlate more strongly with human judgments than \ac{bleu}, particularly at the sentence or segment level, due to its sensitivity to lexical and semantic variation \citep{zhou_paraphrase_2021,kurt_pehlivanoglu_comparative_2024}.
% Its values range from 0 to 1 \citep{kurt_pehlivanoglu_comparative_2024}.
$P$ and $R$ denote unigram-precision and unigram-recall, respectively~\citep{kurt_pehlivanoglu_comparative_2024,banerjee_METEOR_2005}.


\subsubsection{Semantic Measures}
Syntactic measures are inadequate when the goal is to evaluate paraphrases that prioritize semantic preservation over lexical similarity. 
To address this limitation, semantic metrics leverage distributed representations of words or sentences.

BERTScore~\citep{hanna_fine_grained_2021} computes similarity between contextual BERT embeddings of candidate and reference texts. 
For reference vectors $r$ and candidate vectors $c$, precision and recall are defined as \autoref{eq:bert_p} and \autoref{eq:bert_r}, respectively.

\begin{equation}
    P_{BERT} = \frac{1}{|c|} \sum_{c_i \in c} \max_{z_j \in r} r_j\top c_i
\label{eq:bert_p}
\end{equation}
\begin{equation}
    R_{BERT} = \frac{1}{|r|} \sum_{r_i \in r} \max_{c_j \in c} r_i\top c_j
\label{eq:bert_r}
\end{equation}

% \begin{equation}
%     F_1 = \frac{2 P_{BERT} R_{BERT}}{P_{BERT} + R_{BERT}} 
% \label{eq:bert_f1}
% \end{equation}
% Since $F_1 \in \left[-1,1\right]$ it can be rescaled to $[0,1]$ by modifying the precision and recall calculation 
% to $\hat{P}_{BERT} = \frac{P_{BERT} - a}{1 - a}$ ($R_{BERT}$ analogous), where $a$ is the empirical lower bound on the BERTScore \citep{hanna_fine_grained_2021}.


BERTScore correlates with human judgment at the semantic level \citep{kurt_pehlivanoglu_comparative_2024}, although it may struggle when lexically overlapping but semantically incorrect candidates are present \citep{hanna_fine_grained_2021}.

\ac{wmd} measures the minimal transport cost of aligning word embeddings from one text to another \citep{gohsen_captions_2023}. 
\textcolor{red}{TODO: Rechnung}

\textcolor{red}{TODO: SBERT cosine similarity}


\subsubsection{One Score for All}

\textcolor{red}{Gohsen $\Delta_{sem,syn}=Avg(Semantic Sim)-Avg(Syntactic Sim)$~\citep{gohsen_captions_2023}}

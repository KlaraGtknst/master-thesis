\subsection{Traditional Quantitative Paraphrase Evaluation}
\label{subsec:traditional_quantitative_evaluation_measures}

Evaluating paraphrases can be reduced to summarization or translation evaluation.
The evaluation of paraphrases can be divided into syntactic and semantic approaches. 
% \citet{gohsen_captions_2023} normalized all metrics and averaged the semantic and syntactic scores separately.

\subsubsection{Syntactic Measures}
Syntactic evaluation metrics mainly focus on the n-gram overlaps~\citep{zhou_paraphrase_2021}. 
Common syntactic evaluation metrics include \acs{bleu}, \acs{rouge}-1, and \acs{rouge}-L.

\input{chapter/section-05/metrics/BLEU.tex}
\input{chapter/section-05/metrics/ROUGE.tex}
\input{chapter/section-05/metrics/METEOR.tex}

Evaluating the Pearson correlation to human judgement on \num{10000} samples from web pages from the Wayback Machine and the Common Crawl dataset, \citet{anantha_pearson_metrics_2021} found that none of \ac{bleu}, \ac{rouge}, and METEOR has a higher Pearson correlation to human judgement than $0.64$.
% \citet{banerjee_METEOR_2005} publishes higher values on the Chinese portion of the Tides 2003 dataset.


\subsubsection{Semantic Measures}
Syntactic measures are inadequate when the goal is to evaluate paraphrases that prioritize semantic preservation over lexical similarity. 
To address this limitation, semantic metrics leverage distributed representations of words or sentences.
As per \citet{gohsen_captions_2023}, we compute semantic similarity between transformer based models.

\paragraph{BERTScore}
BERTScore~\citep{hanna_fine_grained_2021} computes similarity between contextual BERT embeddings of candidate and reference texts. 
Contextual embeddings allow for the same word having different embeddings depending on its context.
Even though cosine similarity calculation considers tokens in isolation, the contextual embeddings contain information of the rest of the sentence.
Sentences are tokenized by BERT and subsequently encoded.
For computing precision, each token embedding $c_i$ in the candidate $c$ is matched to a token embedding $r_j$ in the reference $r$.
Analogous, for recall calculation, each token embedding $r_i$ in the reference $r$ is matched to a token embedding $c_j$ in the candidate $c$.
Matching is carried out in a greedy fashion based on similarity.
~\citep{zhang_bertscore_2020}.
For reference vectors $r$ and candidate vectors $c$, precision $P_{\text{BERT}}$ and recall $R_{\text{BERT}}$ are defined as \autoref{eq:bert_p} and \autoref{eq:bert_r}, respectively.
The $F_1$ score from \autoref{eq:bert_f1} is computed based on precision and recall.

\begin{equation}
    P_{\text{BERT}} = \frac{1}{|c|} \sum_{c_i \in c} \max_{z_j \in r} \mathbf{r}_j^\top \mathbf{c}_i
\label{eq:bert_p}
\end{equation}

\begin{equation}
    R_{\text{BERT}} = \frac{1}{|r|} \sum_{r_i \in r} \max_{c_j \in c} \mathbf{r}_i^\top \mathbf{c}_j
\label{eq:bert_r}
\end{equation}

\begin{equation}
    F_1 = \frac{2 P_{BERT} R_{BERT}}{P_{BERT} + R_{BERT}} 
\label{eq:bert_f1}
\end{equation}
Since $F_1 \in \left[-1,1\right]$ it can be rescaled to $[0,1]$ to improve score readability by modifying the precision and recall calculation 
to $\hat{P}_{BERT} = \frac{P_{BERT} - a}{1 - a}$ ($R_{BERT}$ analogous), where $a$ is the empirical lower bound on the BERTScore \citep{zhang_bertscore_2020,hanna_fine_grained_2021}.

% BERTScore correlates with human judgment at the semantic level \citep{kurt_pehlivanoglu_comparative_2024}, although it may struggle when lexically overlapping but semantically incorrect candidates are present \citep{hanna_fine_grained_2021}.

\paragraph{SBERT cosine similarity}
The cosine similarity between vector representations $v_a$ and $v_b$ of two documents $a$ and $b$ is defined in \autoref{eq:cosine_sim}. 
Cosine similarity values range from $-1$ to $1$~\citep{thongtan_cosine_sim_19,zhang_bertscore_2020}, where $-1$ indicates that $v_a$ and $v_b$ point in opposite directions, $0$ indicates no correlation, and $1$ indicates that they point in the same direction. 
Following \citet{gohsen_captions_2023}, our document vectors are computed using an SBERT model.

\begin{equation}
    cos(\theta_{a,b})=sim(v_a,v_b)=\frac{v_a^Tv_b}{\left\| v_a \right\|\left\| v_b \right\|}
    \label{eq:cosine_sim}
\end{equation}


\paragraph{\ac{wmd}}
\ac{wmd} measures the minimal transport cost of aligning word embeddings from one text to another~\citep{gohsen_captions_2023}. 
\citet{kusner_wmd_15} formalize this via a flow matrix $T \in \mathcal{R}^{n \times n}$ where $T_{ij} \geq 0$ denotes how much of word $i$ in a document $d$ must travel to a word $j$ in a document $d'$.
To transform document $d$ to document $d'$, (1) the outgoing flow from word $i$ equals $d_i$, i.e. $\sum_{j}T_{ij}=d_i$, and (2) the incoming flow to word $j$ must match $d'_j$, i.e. $\sum_{i}T_{ij}=d'_j$.
The distance between document $d$ and document $d'$ is the minimum cumulative cost required to move all words from $d$ to $d'$, i.e. $\sum_{i,j}T_{i,j}c(i,j)$, where $c(i,j)$ is the cost of travelling from word $i$ to word $j$~\citep{kusner_wmd_15}.


\subsubsection{Gohsen $\Delta_{sem,syn}$}

\citet{gohsen_captions_2023} introduce $\Delta_{sem,syn}$ to facilitate the interpretation of paraphrasing scores.
First, all syntactic and semantic measures are normalized to a scale from zero to one.
Then, the average syntactic similarity $\diameter_{syn}$ and the average semantic similarity $\diameter_{sem}$ is calculated.
Syntactic metrics include \ac{rouge}-1, \ac{rouge}-L, and \ac{bleu}.
Semantic measures include \ac{wms}, BERT, and cosine similarity of the SBERT embeddings.
Finally, $\Delta_{sem,syn}$ is defined as in \autoref{eq:gohsen_delta}, i.e. the difference of semantic and syntactic average distance~\citep{gohsen_captions_2023}.
\begin{equation}
    \Delta_{sem,syn}=\diameter_{sem}-\diameter_{syn}
    \label{eq:gohsen_delta}
\end{equation}
Hence, high $\Delta_{sem,syn}$ values indicate structurally and lexically diverse and semantically similar text pairs.

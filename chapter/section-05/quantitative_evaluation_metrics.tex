\subsection{Traditional Quantitative Paraphrase Evaluation Measures}
\label{subsec:traditional_quantitative_evaluation_measures}

Evaluating paraphrases can be reduced to summarization or translation evaluation.
The evaluation of paraphrases can be divided into syntactic and semantic approaches. 
% \citet{gohsen_captions_2023} normalized all metrics and averaged the semantic and syntactic scores separately.

\subsubsection{Syntactic Measures}
Syntactic evaluation metrics mainly focus on the n-gram overlaps~\citep{zhou_paraphrase_2021}. 
Common syntactic evaluation metrics include \acs{bleu}, \acs{rouge}-1, and \acs{rouge}-L.

\input{chapter/section-05/metrics/BLEU.tex}
\input{chapter/section-05/metrics/ROUGE.tex}
\input{chapter/section-05/metrics/METEOR.tex}


\subsubsection{Semantic Measures}
Syntactic measures are inadequate when the goal is to evaluate paraphrases that prioritize semantic preservation over lexical similarity. 
To address this limitation, semantic metrics leverage distributed representations of words or sentences.
We compute semantic similarity between transformer based models~\citep{gohsen_captions_2023}.

BERTScore~\citep{hanna_fine_grained_2021} computes similarity between contextual BERT embeddings of candidate and reference texts. 
For reference vectors $r$ and candidate vectors $c$, precision and recall are defined as \autoref{eq:bert_p} and \autoref{eq:bert_r}, respectively.

\begin{equation}
    P_{BERT} = \frac{1}{|c|} \sum_{c_i \in c} \max_{z_j \in r} r_j\top c_i
\label{eq:bert_p}
\end{equation}
\begin{equation}
    R_{BERT} = \frac{1}{|r|} \sum_{r_i \in r} \max_{c_j \in c} r_i\top c_j
\label{eq:bert_r}
\end{equation}

% \begin{equation}
%     F_1 = \frac{2 P_{BERT} R_{BERT}}{P_{BERT} + R_{BERT}} 
% \label{eq:bert_f1}
% \end{equation}
% Since $F_1 \in \left[-1,1\right]$ it can be rescaled to $[0,1]$ by modifying the precision and recall calculation 
% to $\hat{P}_{BERT} = \frac{P_{BERT} - a}{1 - a}$ ($R_{BERT}$ analogous), where $a$ is the empirical lower bound on the BERTScore \citep{hanna_fine_grained_2021}.


BERTScore correlates with human judgment at the semantic level \citep{kurt_pehlivanoglu_comparative_2024}, although it may struggle when lexically overlapping but semantically incorrect candidates are present \citep{hanna_fine_grained_2021}.

\ac{wmd} measures the minimal transport cost of aligning word embeddings from one text to another \citep{gohsen_captions_2023}. 
\textcolor{red}{TODO: Rechnung}

\textcolor{red}{TODO: SBERT cosine similarity}
The cosine similarity between dense vector representations of a SBERT model~\citep{gohsen_captions_2023}.

\subsubsection{Gohsen Delta $\Delta_{sem,syn}$}
First, all syntactic and semantic measures are normalized to a scale from zero to one.
Then, the average syntactic similarity $\diameter_{syn}$ and the average semantic similarity $\diameter_{sem}$ is calculated.
Syntactic metrics include \ac{rouge}-1, \ac{rouge}-L, and \ac{bleu}.
Semantic measures include \ac{wms}, BERT, and cosine similarity of the SBERT embeddings.
Finally, $\Delta_{sem,syn}$ is defined as in \autoref{eq:gohsen_delta}, i.e. the difference of semantic and syntactic average distance~\citep{gohsen_captions_2023}.
\begin{equation}
    \Delta_{sem,syn}=\diameter_{sem}-\diameter_{syn}
    \label{eq:gohsen_delta}
\end{equation}
Hence, high $\Delta_{sem,syn}$ values indicate structurally and lexically diverse and semantically similar text pairs.

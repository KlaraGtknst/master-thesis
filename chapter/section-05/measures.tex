% regardless of experimental design
\section{Evaluation measures}
\label{sec:evaluation_measures}

In this chapter, we introduce state-of-the-art quantitative evaluation metrics for \ac{aa}, \ac{av}, and paraphrase generation. 
Unlike subjective human judgments, these metrics are designed to be comparable and reproducible, providing a more objective basis for evaluation.

\textcolor{red}{
c@1 does not make sense here bc (1) output is "same author" or "i do not know" (standard 0.5 from PAN) and (2) output is 0 or 1, that is why we have threshold -> never 0.5. Maybe we could train two thresholds and could return 0.5 if neither is triggered, but does not make sense bc "different author" class ill-defined (open set -> no representative/exhaustive samples)
}

\input{chapter/section-05/av_evaluation_measures.tex}

\subsection{Paraphrase evaluation}
\label{subsec:paraphrase_evaluation}

Evaluating paraphrases is a central problem in \ac{nlp}, yet it is complicated by the fact that there is no universal definition of what constitutes a paraphrase. 
Definitions vary in degree of semantic equivalence required. 
This conceptual ambiguity makes the task of evaluation especially challenging, since different applications may prioritize different aspects such as fidelity to meaning, stylistic variation, or grammatical well-formedness.

Because of this, paraphrase evaluation cannot be reduced to syntactic variance alone. 
A meaningful assessment must account both for syntactic diversity and for the extent to which semantic content is preserved. 
At the same time, paraphrases must be judged for their fluency and acceptability, as grammatical errors or stylistic awkwardness may render an otherwise accurate reformulation unsuitable in practice.

Existing approaches can broadly be grouped into automatic and human-based methods. 
Automatic measures attempt to quantify the similarity or equivalence between a candidate and a reference paraphrase using algorithmic techniques. 
These methods can be further distinguished by the linguistic level at which they operate. 
Some focus on syntactic structure and word overlap, while others rely on semantic representations to evaluate meaning preservation beyond surface form. 
Human evaluation, in contrast, remains the gold standard, as it incorporates judgments about adequacy, grammaticality, and contextual appropriateness that automatic measures cannot fully capture.

% Syntactic (\ac{bleu}, \ac{rouge}-1, \ac{rouge}-L), semantic (BERTScore, cosine similarity of SBERT vectors, WMS), human evaluation (TODO)

% There is manual (by humans) evaluation and automatic evaluation for paraphrase generation \citep{fu_learning_2024,zhou_paraphrase_2021}.
% According to \citet{zhou_paraphrase_2021}, automatic evaluation metrics mainly focus on the n-gram overlaps instead of meaning, 
% and hence, human evaluation is more accurate and has a higher quality.
% In the following, we focus on automatic evaluation.

In the following, we examine both automatic and human evaluation strategies. 
The automatic measures are divided into syntactic and semantic approaches, reflecting the different dimensions along which paraphrases can be compared. 
This is followed by a discussion of human evaluation, which complements automatic measures by providing a more comprehensive assessment of paraphrase quality.


\input{chapter/section-05/quantitative_evaluation_metrics.tex}

\subsubsection{Bespoke Quantitative Evaluation}
\label{subsec:custom_quantitative_evaluation}

% make subsubsub
\subsubsection{Text Extraction}
\label{subsec:text_extraction}

In order to evaluate the quality of the information extracted by the \pextractor{}, 
we decided to compare the genre, century, and the paraphrase-specifc topic to the 
ground truth available for the \dataBlog{}, \dataGutenberg{} and the \dataCustom{} dataset.

We found that the instructions for the \pextractor{} have to be positioned after the text to be extracted, 
due to the inability of the \pextractor{} to return the extracted information in the specified JSON format 
when the prompt was at the beginning of the input for long texts such as those from the \dataGutenberg{} dataset.

\textcolor{red}{TODO: insert table with results}

Irrespective of the quality of the text extraction, we hypothesize that the quality of the final result of the \pgenerator{} will be good irrespective of the quality of the \pextractor{}.
We motivate this by the fact that both the \pextractor{} and the \pgenerator{} are \acp{llm} and therefore generate text similar.
% Attention: Causal vs. masked language model work different

% make subsubsub
\subsubsection{Paraphrase Generation}
\label{subsec:paraphrase_generation}
To evaluate the quality of the paraphrases generated by the \pgenerator{}, 
we not only computed different paraphrase quality metrics, 
but also compared the text lengths of the generated paraphrases and the original text.

\textcolor{red}{TODO: insert table with results}

% shortcomings of paraphrasing metrics and need for human evaluation
Though easier to reproduce, it is somehow unclear what paraphrase metrics actually measure beyond what their formula states.
While high n-gram overlap might not be the indicator of a good paraphrase in the sense of high syntactic diversity, 
it is not clear if high cosine similarity between the embedding of two texts is a good indicator of a good paraphrase.
Moreover, for all metrics, threshold values for good paraphrases are not well-defined.
It remains to be found whether the worst performing paraphrases are still good enough in terms of human evaluation.
We therefore also employed qualitative evaluation of the paraphrases.

% make subsubsub
\subsubsection{Measures and Findings}
\label{subsec:measures_and_findings}

% shortcoming of traditional quantitative paraphrase metrics
We used state-of-the art measures for the quantitative evaluation of paraphrases. 
Unfortunately, these measures can be misleading since it is unclear what they actually measure.
Generally high scores in BLEU, ROUGE, METEOR mean nearly identical paraphrase (high n-gram overlap).
In this case, we want value syntactic diversity, rendering these measures unintuitive 
for high values do not necessarily correspond to good paraphrases.
Semantic similarity measurements compare the content of the paraphrase to the original text, 
where the interpretation of the cosine of a vector is not clear either.

% findings
Non-naive paraphrasers generally lower syntactic scores than naive paraphrasers,
supposedly because they have a weaker influence on the \pgenerator{} 
(i.e. disclosing extracted content rather than the original text)
leaving more room for variance in texts.
Consequently, \enquote{bad} scored non-naive paraphrases are good in terms of syntactic diversity.

\subsubsection{Qualitative Evaluation}
\label{subsec:qualitative_evaluation}

In addition to the quantitative evaluation, we qualitatively evaluated the paraphrases generated by the \pgenerator{}.
Prior to the evaluation we specified a list of criteria that a good paraphrase should fulfill.
\textcolor{red}{nicht specifc to work here, but general}

\section{Dataset}
\label{sec:dataset}

Since our method extends the original \impAppr{} proposed by \citet{koppel_determining_2014}, we first obtained the datasets used in their study to validate our implementation and reproduce their results. 
The original experiments were conducted on the \dataBlog{} and \dataStudent{} datasets, which are described in detail in \autoref{subsec:original_data}. 
In addition to these, we incorporated two supplementary datasets, \dataPan{} and \dataGutenberg{}, presented in \autoref{subsec:additional_data}. 
Following the general description of all datasets, we outline our preprocessing pipeline in \autoref{subsec:dataset_preprocessing} and conclude with the text-pair selection procedure in \autoref{subsec:dataset_text_pair_selection}.


\subsection{Original Data}
\label{subsec:original_data}

% Blog
The \dataBlog{} corpus~\citep{blog_dataset_2006} consists of blog posts collected from \textit{blogger.com} on or before August 2004, with each blog authored by a single user.
According to the Kaggle repository~\footnote{\href{https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus?resource=download}{Kaggle dataset \texttt{rtatman/blog-authorship-corpus}} (26.07.2025)}, the dataset contains \num{681288} posts from \num{19320} bloggers, averaging approximately 35 posts and \num{7250} words per author.
Users are grouped into three age categories: 13-17, 23-27, and 33-47.
Each record includes the following metadata: \texttt{id}, \texttt{gender}, \texttt{age}, \texttt{topic}, 
\texttt{sign} (referring to the author's astrological/zodiac sign), \texttt{date}, and \texttt{text}.

% student essays
The \dataStudent{} dataset is not publicly available due to the presence of sensitive student information. 
We gratefully acknowledge J. W. Pennebaker for granting access to the original data used by \citet{koppel_determining_2014} in their study. 
The dataset comprises \num{7052} student essays written for five assignments by a cohort of 950 university students in 2006~\citep{koppel_determining_2014}.

The assignments include (1) a stream-of-consciousness task, (2) a reflections on childhood, (3) a self-assessment of personality, (4) a thematic apperception test, and (5) four examples of four different theories.
An inconsistency in the file naming convention is notable: 
Most files are named solely by the author ID, whereas those from the first assignment follow the format \texttt{2006\_authorID}.
Following \citet{koppel_determining_2014}, our \impAppr{} experiments employ only the first four assignments. 
Consistent with \citet{koppel_determining_2014}, disputedâ€“candidate pairs in our setup are drawn from different assignments, irrespective of their class label (\texttt{same-author} or \texttt{different-author}).
In contrast to the original \impAppr{}, which truncated each essay to the first 500 words, we crop texts to the minimum length of the text pair.

Due to privacy restrictions, researchers seeking access to the \dataStudent{} dataset should contact J. W. Pennebaker, the official data custodian. 
For establishing a baseline in our \imp{} experiments, we use both the \dataBlog{} and \dataStudent{} datasets.


\subsection{Additional Data}
\label{subsec:additional_data}
To broaden the evaluation scope of the \impAppr{}, we incorporated additional datasets selected according to two criteria:
(1) control over confounding factors such as genre and topic, and 
(2) verified, undisputed authorship.
Both the \dataPan{} and \dataGutenberg{} datasets satisfy these conditions.

% PAN20: Fanfiction
The \dataPan{} corpus~\citep{bischoff_importance_2020} comprises fanfiction texts sourced from \textit{fanfiction.net}.
Each text belongs exclusively to one fandom (i.e. thematic category), with no crossovers between fandoms.
According to \href{https://pan.webis.de/clef20/pan20-web/author-identification.html}{the official \ac{pan} website}, 
train and test set originate from two different fanfictions and approximate the (long-tail) distribution of the fandoms in the original dataset.
Dataset features include \texttt{id}, \texttt{fandoms}, and \texttt{pair}, where the latter contains the paired texts.
An additional \texttt{jsonl} file provides the ground truth for each pair, specifying \texttt{id}, \texttt{same} (authorship label), and \texttt{authors}.

% Gutenberg
The \dataGutenberg{} dataset~\footnote{\href{https://www.gutenberg.org/}{Project Gutenberg} (26.07.2025)} contains a curated selection of literary works from Project Gutenberg, a digital library dedicated primarily to older works whose U.S. copyrights have expired.
As of this writing, the collection contains over \num{75000} digitized and proofread e-books contributed by volunteers.
For our experiments, we selected 19 works authored by 7 writers from the 16th to 19th centuries, covering genres such as drama, fiction, and poetry.
Metadata for these works was manually extracted from the Project Gutenberg website and Wikipedia.


\subsection{Dataset Preprocessing}
\label{subsec:dataset_preprocessing}

To control confounding factors that influence authorial style, we preprocess each dataset twice:
(1) Prior to generating arrow dataset file and (2) before using the \impAppr{}.
This two-stage approach addresses both experimental scenarios, where all material is prepared in advance, and inference scenarios in which the \impAppr{} model is applied directly to texts.
The preprocessing process was designed to meet the following requirements:
\begin{itemize}
    \item Removal of all formatting and layout information to produce plain text.
    \item Cropping texts to match the length of the shorter text in each pair.
\end{itemize}
It is important to note that text-length adjustment is performed exclusively within the \impAppr{} implementation and is not applied to the Arrow dataset itself.
For a controlled evaluation environment in our \impAppr{}, we opted to work with relatively small, curated datasets rather than scaling to larger collections.  
The effect of individual preprocessing steps on vocabulary size was analysed, with results presented in \autoref{fig:preprocesing_impact_vocab_size}.
In line with \citep{koppel_determining_2014}, the vocabulary consists of space-free character 4-grams.
Removing formatting and layout information includes removing HTML artefacts, play artefacts, newlines, 
converting UTF-8 to ASCII, and stripping leading and trailing whitespace.
Since both \dataBlog{} and \dataPan{} originate from the Internet, we applied HTML-specific preprocessing steps, which had minimal impact on their respective vocabularies.
Upon inspection of the \dataGutenberg{}, we found that certain patterns reappear due to the \textit{play} genre. 
Notably, since we do not consider actor instructions (e.g., character cues) or structural elements (e.g., chapter headings) as part of authorial style, these patterns were removed using regular expressions.
We opted to forgo lowercasing the texts, as our preliminary analysis indicated that lowercasing had no meaningful effect on any dataset while potentially discarding deliberate authorial capitalization choices.

\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/dataset/impact_preprocessing_steps.svg}
    \caption{Effect of preprocessing steps on vocabulary size (space-free character 4-grams).}
    \label{fig:preprocesing_impact_vocab_size}
\end{figure}


\subsection{Selection of Text Pairs}
\label{subsec:dataset_text_pair_selection}

For the \dataBlog{}, \dataStudent{}, and \dataGutenberg{} datasets, we selected pairs of texts according to specific criteria to control potential confounding factors.
Only texts with a minimum length of \num{700} words were considered eligible. 
For the \dataPan{} dataset, we retained the existing pairs in the Arrow dataset to ensure comparability with prior work. 
All datasets include both same-author and different-author pairs. 

For \dataBlog{}, pairs were constructed so that the two texts share the same topic, year, gender, and age (the latter referencing the author). 
The training set comprises 80\% of the data and the test set 20\%, with different topics assigned to each split.

For \dataStudent{}, texts were assigned to either the training (70\%) or test (30\%) set. 
The test set is larger because each author typically contributes only one essay per task and if only a single task were included in the test set, no same-author pairs could be formed. 
Same-author pairs were selected such that authors share the same sex, ethnicity, and political orientation. 
As per \citep{koppel_determining_2014}, all text pairs are drawn from the different tasks.



% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[tbp]
    \centering
    \caption{\ac{aa} scenarios with author $i$ is shortened with $A_i$ \citep{altakrori_topic_2021}.}
    \label{tab:aa_same_topic}
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{} & \textbf{Train} & \textbf{Test} \\ \hline
    \textbf{Topic $T_1$} & $A_1, A_2$ & $A_1, A_2$ \\ \hline
    \textbf{Topic $T_2$} & $A_1, A_2$ & $A_1, A_2$ \\ \hline
    \end{tabular}%
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[tbp]
    \centering
    \caption{\ac{aa} scenarios with author $i$ is shortened with $A_i$ \citep{altakrori_topic_2021}.}
    \label{tab:aa_cross_topic}
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{} & \textbf{Train} & \textbf{Test} \\ \hline
    \textbf{Topic $T_1$} & $A_1, A_2$ &  \\ \hline
    \textbf{Topic $T_2$} &  & $A_1, A_2$ \\ \hline
    \end{tabular}%
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[tbp]
    \centering
    \caption{\ac{aa} scenarios with author $i$ is shortened with $A_i$ \citep{altakrori_topic_2021}.}
    \label{tab:aa_topic_confusion}
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{} & \textbf{Train} & \textbf{Test} \\ \hline
    \textbf{Topic $T_1$} & $A_1$ & $A_2$ \\ \hline
    \textbf{Topic $T_2$} & $A_2$ & $A_1$ \\ \hline
    \end{tabular}%
\end{table}


For the \dataGutenberg{} dataset, pairs were selected such that texts share the same genre and century. 
Authors were split into training (80\%) and test (20\%) sets.

Regardless of the selection criteria, the final datasets contain only three columns: \texttt{authors}, \texttt{pair}, and \texttt{same}.
The \texttt{pair} column contains the texts of the pair as a list of strings,
the \texttt{authors} column contains the authors of the texts as a list of strings,
and the \texttt{same} column indicates whether the texts are from the same author (\texttt{True}) or from different authors (\texttt{False}).
Descriptive statistics for all preprocessed datasets are provided in Table~\ref{tab:data_stats}.

% minimum length necessesary for AV/ AA
The choice of minimum text length was informed by related research in \ac{av} and \ac{aa}.
\citet{bevendorff_generalizing_2019}used text chunks of at least 700 words for an unmasking approach, while \citet{koppel_authorship_2004} set the minimum at 500 words.
Recent work~\citep{llm_detection_av_2025} identifies \num{2500}-\num{4000} characters to be sufficient for effective \ac{llm} detection framed as \ac{av} or \ac{aa}, leading those authors to adopt a \num{3000}-character minimum for their datasets.

% \begin{table}[h]
\begin{sidewaystable}
\centering\small
\caption{Statistics of preprocessed datasets \dataPan{}, \dataBlog{}, \dataGutenberg{}, and \dataStudent{}.}
\label{tab:data_stats}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lrrrrrrrrr@{}}   % numbers should be right aligned, text left aligned
\toprule
dataset & num\_pairs & num\_authors & num\_same\_pairs & num\_different\_pairs & avg\_text\_len & max\_text\_len\_words & std\_text\_len\_words & median\_text\_len\_words \\
\midrule
pan20           & 66905 & 52771 & 35616 & 31289 & 21418.76 (3914.76)   & 55413 & 512.19 & 3889 \\
blog            & 11565 & 5997  & 6204 & 5361  & 6249.94 (1154.25)     & 115365 & 1493.97 & 913 \\
gutenberg       & 12    & 7     & 6     & 6     & 437870.75 (78698.79) & 297704 & 68329.91 & 60282 \\
student\_essays & 224  & 222   & 112   & 112  & 4459.32 (865.90)     & 1634 & 157.41 & 815  \\
\bottomrule
\end{tabular}%
}
% \end{table}
\end{sidewaystable}

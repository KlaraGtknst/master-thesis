\subsection{Exp.\ 3: Assessing the Impact of the Prompt on Paraphrasing}
\label{subsec:prompt_impact}

The goal of this experiment was to investigate how different prompting strategies influence the quality of paraphrases generated by \acp{lm}.  
Since the \impAppr{} relies on paraphrased text as a basis for further computation, it is crucial to ensure that these paraphrases are both faithful to the original content and of comparable length to their references.  
Substantial deviations in length may introduce confounding factors, thereby reducing the reliability of subsequent analyses.  

We designed three prompt variants to instruct models in the paraphrasing task.
Prompt0 directs the model to paraphrase by substituting the main subject, verb, and object with synonyms, keeping the output close in length to the reference.  
Prompt1 requests a paraphrase that varies wording and sentence structure while preserving tone, with length similar to the original.  
Prompt2 instructs the model to produce a paraphrase three times longer than the reference, while preserving meaning and tone. 

We applied these prompts to the four autoregressive \acp{lm}. 
We opted for causal models since their architecture is beneficial for text generation compared to masked models.
The models include \texttt{meta-llama\-3.1-8b-instruct}, \texttt{mistral-large-instruct}, \texttt{openai-gpt-\-oss\-120b}, and \texttt{qwen3-32b}.  
You may find more details about their architecture in the in \autoref{app:language_models}.
For each modelâ€“prompt pair, we generated paraphrases of reference texts drawn from our dataset.  

% During initial exploration, we observed that some models, particularly \texttt{qwen3-32b}, produced reasoning traces, i.e.\ intermediate thought-like text segments separated by \texttt{</think>}.  
% As these traces are not part of the intended paraphrase, they were systematically removed through post-processing prior to evaluation.  

The evaluation focused on two dimensions.
Length fidelity calculates the relative difference in length between reference and paraphrase, used as a proxy for structural comparability.  
Content quality is a manual inspection of paraphrases with lengths longer and close to their references, to verify semantic preservation and readability.  

This experiment was designed to answer two key questions:  
\begin{enumerate}
    \item How strongly does the choice of prompt affect the relative length of generated paraphrases across different \acp{lm}?  
    \item Which prompt formulation best balances structural comparability and semantic fidelity, thereby reducing confounding factors for subsequent \imp{} experiments?  
\end{enumerate}

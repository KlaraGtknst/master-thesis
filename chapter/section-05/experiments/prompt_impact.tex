\subsection{Exp.\ 4: Comparing Prompts}%Assessing the Impact of the Prompt on Paraphrasing}
\label{subsec:prompt_impact}

The goal of this experiment was to investigate how different prompting strategies influence the quality of paraphrases generated by \acp{llm}.  
Since the \impAppr{} relies on paraphrased text as a basis for further computation, it is crucial to ensure that these paraphrases are both on topic and of comparable length as their reference.  
Large variations in length may introduce confounding effects, diminishing the reliability of subsequent analyses.

We designed three prompt variants to instruct models in the paraphrasing task.
Prompt0 directs the model to paraphrase by substituting the main subject, verb, and object with synonyms, keeping the output close in length to the reference.  
Prompt1 requests a paraphrase that varies wording and sentence structure while preserving tone, with length similar to the original.  
Prompt2 instructs the model to produce a paraphrase three times longer than the reference, while preserving meaning and tone. 

We selected reference texts from our datasets and applied the prompts to four autoregressive \acp{llm}, choosing causal models because their architecture better supports text generation than masked models. 
The models include \texttt{meta-llama-3.1-8b-instruct}, \texttt{mistral-large-instruct}, \texttt{qwen3-32b}, and \texttt{openai-gpt-oss-120b}.
Further architectural details are provided in \Cref{app:language_models}. 
Using each modelâ€“prompt pair, we generated paraphrases.

The evaluation focused on two main aspects. 
Since length acts as a confounding variable for \imps{}, we measured the relative difference in length between each reference text and its paraphrase. 
Additionally, we manually inspected long paraphrases and those of similar length to their references to verify semantic preservation and readability.
This experiment was designed to address two key questions:
\begin{enumerate}
\item How strongly does the choice of prompt influence the length of generated paraphrases relative to their reference text across different \acp{llm}?
\item Which prompt most effectively mitigates the confounding effect of paraphrase length?
\end{enumerate}

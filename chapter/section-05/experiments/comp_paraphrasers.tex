\subsection{Exp. 2: Comparison of different Paraphrasers}
\label{subsec:comp_paraphrasers_setup}

Next, we wanted to assess our paraphrasing approaches.
We hence designed two experiments.
The first experiment computes state-of-the-art paraphrasing measures for all paraphrasers on different datasets.
The second experiments aims to evaluate the ability of our two-step models to adhere to instructions.
We tested their proficiency extracting metadata and generating paraphrases of similar length as the reference text.

\paragraph{Exp. 2(a): Quantitative evaluation}
There are two ways to feed the generator metadata.
The extractor can either extract it from the reference text, or the user passes ground truth information when calling the paraphraser function.
In this experiment we use ground truth data if available.

We select one text from the \dataBlog{}, the \dataGutenberg{} and the \dataStudent{} dataset, respectively.
For the \dataGutenberg{} dataset, we load ground truth metadata.
The paraphraser configurations contain two different temperatures for two-step paraphrasers, and two different prompts for one-step paraphrasers.
We create one paraphrase for each text configuration pair.
Evaluation measures include BLEU, ROUGE1, ROUGE2, ROUGEL, ROUGELsum, METEOR, BERTScore Precision, BERTScore Recall, BERTScore F1, SBERT \ac{wms}, SBERT cosine similarity.
Based on these we also compute syntactic and semantic similarity, as well as Gohsen Delta~\citep{gohsen_captions_2023}.
We save the extremest (min, max) paraphrases per metric.
The scores are subsequently visualized via syntactic-semantic scatters, score distributions, and radar plots per paraphraser and per prompt. 

\paragraph{Exp. 2(b): Evaluation of prompt adherence}
For the second experiment, we select five samples from the \dataBlog{}, \dataGutenberg{} and the \dataStudent{} datasets. 
Our extractor extracts the genre, the topic, and the century of each input text.
Extracted and ground truth values are lowercased and stripped from leading and trailing whitespaces. 
For single genre and topic values, we compute the cosine similarity on their respective SBERT embedding.
For the genre extraction, we split the extractors' result by comma and use the maximum similarity.
Since the ground truth topic usually consists of multiple topics separated by comma, we split them into a list and use the maximum similarity.
For century match, we processed the result of the extractor by mapping \textit{present}, \textit{current}, and \textit{now} to 21, then extracting digits and omitting the last two digits from any numbers with at least three digits and finally adding one if the original digit was not divisible by 100.
We then use the ground truth as a baseline $b$ for extracted century $a$ and compute $\frac{a}{b}$.
\textcolor{red}{TODO: examples}
While the \dataBlog{} metadata comes its csv dataset, the \dataStudent{} metadata is derived from existing information about and in the dataset, the \dataGutenberg{} metadata was completely manually curated.
Additional to the previously mentioned processing steps for century values, ground truth century date values were casted to dates.
Moreover, we obtain the relative length difference of the paraphrase and original text for every selected sample. 
This concludes in an evaluation of our paraphrasing (extractors) in terms of genre, topic, century and length similarity.
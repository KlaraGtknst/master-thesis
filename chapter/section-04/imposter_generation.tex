\section{\acs{llm}-based \Imp{} Generation}
\label{sec:impostor_generation}

% good \imps{}: hard negatives
Following the notion introduced by \citet{koppel_determining_2014}, ideal \imp{} texts can be understood as hard negatives: 
Documents not written by the candidate author, yet sufficiently similar in style to be difficult to distinguish from the candidate's own writing. 
The quality of \imps{} directly affects model performance.
If they are too different from the candidate text, models are trained on trivial contrasts, yielding \acp{fp}. 
Conversely, if \imps{} are too similar to the candidate, the risk of \acp{fn} increases.

% obstacles for \imp{} generation in the past
Traditional \imp{} generation techniques struggled to control \imp{} similarity. 
The fixed approach samples from a fixed pool of unrelated texts, while the on-the-fly approach fails to align genre with the candidate text~\citep{koppel_determining_2014}. 
Since authorial style is tightly entangled with such contextual factors, \imps{} produced by these methods differ systematically from the candidate text, weakening their utility. 
Ideally, \imp{} generation should replicate the conditions of original text production, including task, topic, register, target audience and century. 
In practice, however, this information is rarely available.
In \ac{av}, the candidate author may be deceased, unavailable, or unwilling to cooperate.

% heuristics: paraphrase
\acp{llm} make it possible to approximate this ideal. 
By conditioning generation on the candidate text itself, external factors such as topic, genre, and register can be more tightly controlled. 
In particular, paraphrasing offers a heuristic for simulating the text generation process. 
The model produces stylistic variants of the candidate's writing while maintaining semantic alignment.


\subsection{Paraphrasing as a Basis for \Imp{} Generation}

The lack of a universally accepted definition of paraphrase complicates evaluation~\citep{gohsen_task_oriented_2024}. 
For the present work, paraphrases are considered suitable impostors if they satisfy three conditions: 
They remain consistent with the original text's topic, genre, and tone; 
they exhibit sufficient lexical and syntactic divergence to avoid near-duplication; 
and they may diverge semantically, including mild hallucinations, provided the stylistic resemblance is preserved.
Paraphrase quality is largely determined by prompt design and model selection~\citep{Wu_ODD_challenges_2025}.

Two perspectives on paraphrasing are commonly distinguished. 
At the lexical level, paraphrasing involves word-level substitutions such as synonym replacement. 
At the syntactic level, it entails modifications to sentence structure. 
While more fine-grained taxonomies exist~\citep{zhou_paraphrase_2025}, they are not central here, as this study does not involve training or constructing paraphrase models directly.


\subsection{One-step Paraphrasing}

In the one-step approach, an \imp{} is produced directly through prompting an \ac{llm}. 
Effective prompting proved essential.
We observed that specification of what to avoid was vital to obtain valid results. 
Moreover, task instructions placed at the end of the prompt reduced cases where models ignored them in long-context settings, such as the \dataGutenberg{} corpus.

After iterative refinement, the following two prompts yielded consistent results.
The first explicitly decomposes the sentence into subject, verb, and object before reconstruction, encouraging lexical diversity. 
The second instructs the model to preserve meaning while varying wording and structure. 
Both emphasize concise outputs restricted to the paraphrase itself.

\begin{quote}
    \textit{For the text above: Paraphrase the sentence by first identifying the main subject, verb, and object. Then find synonyms for each and construct a new sentence. Only output the final paraphrased sentence.}
\end{quote}

\begin{quote}
    \textit{For the text above: Paraphrase this sentence. Do not change the meaning, but use different words and structure. Output only the paraphrased sentence.}
\end{quote}

We initially experimented with T5-based models from HuggingFace, but these demonstrated poor instruction following and often produced malformed outputs. 
Our final experiments used the models listed in \autoref{tab:base_llms}, all accessed through GWDG/SAIA or IONOS infrastructure.

\begin{table}[]
\centering
\caption{Base \acp{llm} used for paraphrasing.}
\label{tab:base_llms}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{ll}
\toprule
\textbf{Model ID}                    & \textbf{Host} \\
\midrule
qwen3-32b                            & GWDG/ SAIA    \\
mistral-large-instruct               & GWDG/ SAIA    \\
openai-gpt-oss-120b                  & GWDG/ SAIA    \\
meta-llama-3.1-8b-instruct           & GWDG/ SAIA    \\
meta-llama/Llama-3.3-70B-Instruct    & IONOS         \\
mistralai/Mixtral-8x7B-Instruct-v0.1 & IONOS     \\
\bottomrule   
\end{tabular}%
% }
\end{table}


\subsection{Two-step Paraphrasing}

The two-step approaches are inspired by prior literature~\citep{bevendorff_overview_2024, ayele_overview_2024}, and differ in how they extract and utilize auxiliary information from the source text before generating the paraphrase. 
The two steps enforce a clearer disentanglement of content and style at the cost of increased computational overhead.

Two-step paraphrasing decomposes the task into information extraction and text generation. 
In the first stage, the model identifies contextual features of the input text.
These feature include tone, register, time period, target audience, and genre.
Different approaches extract an additional feature specific to their approach.
The additional features are a bullet point summary, the authorial task, topic, or title. 
The bullet point extractor prompt is specified in the following, while all others can be derived from the Appendix.

\begin{quote}
    \textit{Summarize the following text in five to six short bullet points and give an overall description
    of the genre and tone of the text.}
\end{quote}

In the second stage, a paraphrase is generated based on the extracted information. 
Again, all methods other than the bullet point based one slightly differ from the one given below in Listing~\ref{lst:two_step_bullet_point_prompt} and can be found in the Appendix.

\begin{listing}[ht]
\begin{minted}{python}
prompt = "Write a text which covers the following items:\n" 
    + "\n".join(f"- {bp}" for bp in bullet_points)
\end{minted}
\caption{Two-step paraphrase generation prompt.}
\label{lst:two_step_bullet_point_prompt}
\end{listing}

Translation-based paraphrasing is a related two-step variant, where a text is first translated into another language and then back into English. 
Although conceptually simple, this approach has been shown to yield limited stylistic diversity~\citep{zhou_paraphrase_2025}.
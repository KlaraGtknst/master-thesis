\section{\acs{llm}-based \Imp{} Generation}
\label{sec:impostor_generation}

% good \imps{}: hard negatives
Following the notion introduced by \citet{koppel_determining_2014}, ideal \imp{} texts can be understood as hard negatives, i.e. documents not written by the candidate author, yet sufficiently similar in style to be difficult to distinguish from the candidate's own writing. 
The quality of \imps{} directly affects model performance.
If they are too different from the candidate text, models are trained on trivial contrasts, yielding \acp{fp}. 
Conversely, if \imps{} are too similar to the candidate, the risk of \acp{fn} increases.
This behaviour reflects in increasing recall and precision, respectively.

% obstacles for \imp{} generation in the past
Traditional \imp{} generation techniques struggled to control \imp{} similarity. 
The fixed approach samples from a fixed pool of unrelated texts, while the on-the-fly approach fails to align genre with the candidate text~\citep{koppel_determining_2014}. 
Since authorial style is tightly entangled with contextual factors, \imps{} produced by these methods differ systematically from the candidate text, weakening their utility. 
Ideally, \imp{} generation should replicate the conditions of original text production, including task, topic, register, target audience and century. 
In practice, however, this information is rarely available.
In \ac{av}, the candidate author may be deceased, unavailable, or unwilling to cooperate.

% heuristics: paraphrase
\acp{llm} make it possible to approximate this ideal. 
By conditioning generation on the candidate text itself, external factors such as topic, genre, and register can be more tightly controlled. 
In particular, paraphrasing offers a heuristic for simulating the text generation process. 
The model produces stylistic variants of the candidate's writing while maintaining semantic alignment.


\subsection{Paraphrasing as a Basis for \Imp{} Generation}

The lack of a universally accepted definition of paraphrases complicates their generation and evaluation~\citep{gohsen_task_oriented_2024}. 
In this work, paraphrases are considered suitable \imps{} when they preserve the topic, genre, and tone of the original text, introduce sufficient lexical and syntactic variation to prevent near-duplication, and allow limited semantic divergence including mild hallucinations.
We propose two approaches to paraphrasing as displayed in \autoref{fig:paraphrasing_approaches}.
We expect decreasing \impAppr{} recall and accuracy while increasing \impAppr{} precision, i.e. higher confidence in positive labelled text pairs, due to harder negatives leading to fewer positive predictions.

\begin{figure}[h]
    \centering
    \includesvg{images/paraphrasing/idea/Paraphraser_architectures.svg}
    \caption{Different paraphrasing approaches.}
    \label{fig:paraphrasing_approaches}
\end{figure}


Paraphrasing is often analysed along two main dimensions. 
At the lexical level, it involves word-level substitutions such as synonym replacement, typically achieved through rule-based or thesaurus-driven methods. 
At the syntactic level, paraphrasing entails changes to sentence structure, which can be realized through monolingual machine translation~\citep{zhou_paraphrase_2021} or by explicitly prompting \acp{llm}~\citep{kurt_pehlivanoglu_comparative_2024}.
% While more fine-grained taxonomies exist~\citep{zhou_paraphrase_2025}, they are not central here, as this study does not involve training or constructing paraphrase models directly.
In this case, paraphrase quality is largely determined by prompt design and model selection~\citep{Wu_ODD_challenges_2025}.


\subsection{One-step Paraphrasing}

In our one-step approach, an \imp{} is produced directly through prompting an \ac{llm}. 
Effective prompting proved essential.
We observed that specification of what to avoid was vital to obtain valid results. 
Moreover, task instructions placed at the end of the prompt reduced cases where models ignored them in long-context settings, such as the \dataGutenberg{} corpus.

The first prompt explicitly decomposes the sentence into subject, verb, and object before reconstruction, encouraging lexical diversity. 
The second prompt instructs the model to preserve meaning while varying wording and structure. 
Both emphasize concise outputs restricted to the paraphrase itself.

\begin{quote}
    \textit{Paraphrase the given sentence by identifying the main subject, verb, and object. Replace each with synonyms or closely related words, adjusting grammar naturally. Keep the new sentence close in length to the original. Output only the final paraphrased sentence.}
\end{quote}

\begin{quote}
    \textit{Paraphrase the sentence above without changing its meaning. Use different words and vary the sentence structure while keeping the tone consistent. Keep the new sentence similar in length to the original. Output only the paraphrased sentence, with no explanations or extra text.}
\end{quote}

We initially experimented with T5-based models from HuggingFace, but these demonstrated poor instruction following and often produced malformed outputs. 
Our final experiments used the models listed in \autoref{tab:base_llms}, all accessed through the GWDG/SAIA infrastructure.

\begin{table}[]
\centering
\caption{Base \acp{llm} used for paraphrasing.}
\label{tab:base_llms}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{ll}
\toprule
\textbf{Model ID}                    & \textbf{Host} \\
\midrule
qwen3-32b                            & GWDG/ SAIA    \\
mistral-large-instruct               & GWDG/ SAIA    \\
openai-gpt-oss-120b                  & GWDG/ SAIA    \\
meta-llama-3.1-8b-instruct           & GWDG/ SAIA    \\
\bottomrule   
\end{tabular}%
% }
\end{table}


\subsection{Two-step Paraphrasing}

The two-step approaches are inspired by prior literature~\citep{bevendorff_overview_2024, ayele_overview_2024}, and differ in how they extract and utilize auxiliary information from the source text before generating the paraphrase. 
The two steps enforce a clearer disentanglement of content and style at the cost of increased computational overhead.

As displayed in \autoref{fig:two_step_paraphraser}, the two-step paraphrasing decomposes the task into information extraction and text generation. 
In the first stage, the model identifies contextual features of the input text.
These feature include tone, register, time period, target audience, and genre.
Different approaches extract an additional feature specific to their approach.
The additional features are a bullet point summary, the authorial task, topic, or title. 
The extractor prompts can be derived from \autoref{app:extractor_prompts}.
In the second stage, a paraphrase is generated based on the extracted metadata information. 
There are two ways to feed the generator metadata.
The extractor can either extract it from the reference text, or the user passes ground truth information when calling the paraphraser function.
Again, all generator prompts can be found in \autoref{app:generator_prompts}.

\begin{figure}[ht]
  \centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[node distance=2cm]
% Dataset node
\node[dataset] (dataset) {Dataset};

% Anchor for Reference drawing
\node[draw=none, right=of dataset] (refpos) {};

% Draw Reference as an outer rectangle + snakes inside
\node[draw, very thick, rounded corners=6pt, minimum width=1.8cm, minimum height=2cm,
      right=of dataset, inner sep=4pt] (reference) {};

% Add multiple snakes inside reference (smaller, stacked)
\foreach \y in {0.8,0.4, 0.0,-0.4,-0.8}{
  \draw[black, thick, decorate, decoration={snake,amplitude=1pt,segment length=5pt}]
    ($(reference.west)+(0.3,\y)$) -- ($(reference.east)+(-0.3,\y)$);
}

% Caption
\node[below=0.2cm of reference] {Reference};


% Other blocks
\node[above=0.8cm of reference.north] (meta) {Ground Truth Metadata};
\node[block, right=of reference.east] (extractor) {Extractor};
\node[block, right=of extractor] (generator) {Generator};

% paraphrase
\node[draw, very thick, rounded corners=6pt, minimum width=1.8cm, minimum height=2cm,
      right=of generator, inner sep=4pt, teal!80!black] (paraphrase) {};

% Add multiple snakes inside reference (smaller, stacked)
\foreach \y in {0.8,0.4, 0.0,-0.4,-0.8}{
  \draw[teal!80!black, thick, decorate, decoration={snake,amplitude=2pt,segment length=5pt}]
    ($(paraphrase.west)+(0.3,\y)$) -- ($(paraphrase.east)+(-0.3,\y)$);
}
\node[below=0.2cm of paraphrase] {Paraphrase};

% Arrows
\draw[arrow] (dataset) -- (reference.west);
\draw[arrow] (dataset) -- node[midway, above] {$\{0,1\}$} (meta);
\draw[arrow] (reference.east) -- (extractor);
\draw[arrow] (extractor) -- (generator);
\draw[arrow] (generator) -- (paraphrase);

% \draw[arrow] (meta.south) .. controls +(0,-0.8) and +(0,0.8) .. (generator.north);

\draw[arrow] (meta) -- (generator);


\end{tikzpicture}
}
  \caption{Two-step paraphraser.}
  \label{fig:two_step_paraphraser}
\end{figure}


Translation-based paraphrasing is a related two-step variant, where a text is first translated into another language and then back into English. 
Although conceptually simple, this approach has been shown to yield limited stylistic diversity~\citep{zhou_paraphrase_2025}.
\section{\acs{llm}-based \Imp{} Generation}
\label{sec:impostor_generation}

% good \imps{}: hard negatives
Following the notion introduced by \citet{koppel_determining_2014}, ideal \imp{} texts can be understood as hard negatives, i.e.\ documents not written by the candidate author, yet sufficiently similar in style to be difficult to distinguish from the candidate's own writing. 
The quality of \imps{} directly affects model effectiveness.
If they are too different from the candidate text, models are trained on trivial contrasts, yielding \acp{fp}. 
Conversely, if \imps{} are too similar to the candidate, the risk of \acp{fn} increases.
This manifests as increased recall in the former case and increased precision in the latter case.

% obstacles for \imp{} selection in the past
Traditional \imp{} selection techniques struggled to simultaneously control multiple confounders:
The original \texttt{fixed} approach samples from a pool of unrelated texts, while the \texttt{on-the-fly} approach fails to align genre with the candidate text~\citep{koppel_determining_2014}. 
Since authorial style is tightly entangled with domain variables, \imps{} produced by these methods differ systematically from the candidate text, weakening their utility. 
Ideally, \imp{} generation should replicate the conditions of reference text production, including task, topic, register, target audience and century. 
In practice, however, this information is rarely available.
In \ac{av}, the candidate author may be deceased, unavailable, or unwilling to cooperate.

% heuristics: paraphrase
\acp{llm} make it possible to approximate this ideal. 
By conditioning generation on the candidate text, domain variables such as topic, genre, and register can be more tightly controlled. 
In particular, paraphrasing offers a heuristic for simulating the text generation process. 
The model produces stylistic variants of the candidate's writing while maintaining semantic alignment.


\subsection{Hard Negative Mining via Paraphrasing}

The lack of a universally accepted definition of paraphrases complicates their generation and evaluation~\citep{gohsen_task_oriented_2024}. 
In this work, paraphrases qualify as suitable \imps{} if they preserve the topic, genre, tone, and length of the reference text, while introducing enough lexical and syntactic variation to avoid near-duplication. 
Limited semantic divergence is permitted, including mild hallucinations, which we define as added details that remain broadly consistent with the original topic.
We propose two approaches to paraphrasing, namely one-step and two-step paraphrasing, as illustrated in \Cref{fig:paraphrasing_approaches}.
We expect the recall scores of the \ac{llm}-based \impAppr{} to decrease while precision increases, i.e.\ yielding higher confidence in positive labelled text pairs, due to harder negatives leading to fewer positive predictions.

\begin{figure}[h]
    \centering
   \resizebox{0.6\textwidth}{!}{%
      \begin{tikzpicture}[
        box/.style={draw, rounded corners, minimum width=3cm, minimum height=1cm, align=center},
        arrow/.style={-Stealth, thick},
        node distance=1cm
      ]

      % Core nodes
      \node[box, minimum width=8cm] (paraphraser) {Paraphraser\\One-step \hspace{2cm} Two-step};

      \node[box, below=1cm of paraphraser, xshift=-2cm] (rewrite) {Rewrite};
      \node[box, right=1cm of rewrite] (extractor) {Extractor};
      \node[box, below=0.5cm of extractor] (generator) {Generator};
      

      % Dashed box around Extractor, Generator, Rewrite
      \node[draw, inner sep=0.3cm, fit={(rewrite) (extractor) (generator) (paraphraser)}] (solidbox) {};


      % Top and bottom nodes, centered above/below dashed box
      % \node[box, above=0.5cm of dashed.north, minimum width=8cm] (paraphraser) {Paraphraser\\One-step \hspace{2cm} Two-step};
      \node[box, below=0.5cm of solidbox.south, minimum width=8cm] (impostor) {Impostor};

      % Arrows (all vertical)
      \draw[arrow] ($(paraphraser.south west)!0.5!(paraphraser.south)$) -- (rewrite.north);
      \draw[arrow] ($(paraphraser.south east)!0.5!(paraphraser.south)$) -- (extractor.north);
      \draw[arrow] (extractor.south) -- (generator.north);
      \draw[arrow] (rewrite.south) -- ($(impostor.north west)!0.5!(impostor.north)$);
      \draw[arrow] (generator.south) -- ($(impostor.north east)!0.5!(impostor.north)$);

    \end{tikzpicture}
    }
    \caption[Different paraphrasing approaches]{Proposed one-step and two-step paraphrasing approaches.}
    \label{fig:paraphrasing_approaches}
\end{figure}

Paraphrasing is often analysed along two main dimensions. 
At the lexical level, it involves word-level substitutions such as synonym replacement, typically achieved through rule-based or thesaurus-driven methods. 
At the syntactic level, paraphrasing entails changes to sentence structure, which can be realised through monolingual machine translation~\citep{zhou_paraphrase_2021} or by explicitly prompting \acp{llm}~\citep{kurt_pehlivanoglu_comparative_2024}.
% While more fine-grained taxonomies exist~\citep{zhou_paraphrase_2025}, they are not central here, as this study does not involve training or constructing paraphrase models directly.
Paraphrase quality is largely determined by prompt design and model selection~\citep{Wu_ODD_challenges_2025}.


\subsection{One-step Paraphrasing}
\label{subsec:one_step_paraphrasing_prompts}

In our one-step approach, \imps{} are generated directly via \ac{llm} prompting. 
Effective prompting proved crucial. 
We found that specifying constraints was vital for valid outputs. 
Moreover, placing task instructions at the end reduced instruction neglect in long-context settings (e.g. \dataGutenberg{} corpus). 
We evaluated three prompt variants.

The first prompt, hereafter referred to as \texttt{prompt0}, explicitly decomposes the sentence into subject, verb, and object, and instructs the model to replace them with synonyms.
\begin{quote}
    \textit{Paraphrase the given sentence by identifying the main subject, verb, and object. Replace each with synonyms or closely related words, adjusting grammar naturally. Keep the new sentence close in length to the original. Output only the final paraphrased sentence.}
\end{quote}

The second prompt, hereafter referred to as \texttt{prompt1}, instructs the model to preserve meaning while varying wording and structure. 
Both prompts emphasise concise outputs restricted to the paraphrase itself.
\begin{quote}
    \textit{Paraphrase the sentence above without changing its meaning. Use different words and vary the sentence structure while keeping the tone consistent. Keep the new sentence similar in length to the original. Output only the paraphrased sentence, with no explanations or extra text.}
\end{quote}

The third prompt, hereafter referred to as \texttt{prompt2}, extends the second prompt by explicitly instructing the \ac{llm} to produce paraphrases three times as long as the reference text.
\begin{quote}
  \textit{Paraphrase the text above without changing its meaning. Use different words and vary the sentence structure while maintaining a consistent tone. Your paraphrase should be three times as long than the original. Output only the paraphrased sentence, with NO explanations or extra text.}
\end{quote}

We employed autoregressive \acp{llm}, because their architecture better supports text generation than masked models.
Initial experiments with T5-based HuggingFace models showed poor instruction adherence and frequent malformed outputs. 
Our final experiments therefore employed the models listed in \Cref{tab:base_llms}, accessed via the \ac{gwdg} infrastructure\footnote{\url{https://docs.hpc.gwdg.de/} (August 31, 2025)}.
Further details on these models are provided in Appendix \ref{app:language_models}.

\begin{table}[h]
\centering
\caption[\acp{llm} used for paraphrasing]{\acp{llm} used for paraphrasing.}
\label{tab:base_llms}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Model ID}                    & \textbf{Host} \\
\midrule
qwen3-32b                            & \ac{gwdg}    \\
mistral-large-instruct               & \ac{gwdg}    \\
openai-gpt-oss-120b                  & \ac{gwdg}    \\
meta-llama-3.1-8b-instruct           & \ac{gwdg}    \\
\bottomrule   
\end{tabular}%
% }
\end{table}


\subsection{Two-step Paraphrasing}

The two-step approaches are inspired by literature~\citep{bevendorff_overview_2024, ayele_overview_2024}, and differ in how they extract and utilize auxiliary information (i.e.\ metadata) from the source text before generating the paraphrase. 
The two steps enforce a clearer disentanglement of content and style at the cost of increased computational overhead.

As shown in \Cref{fig:two_step_paraphraser}, the two-step paraphrasing decomposes the task into information extraction and text generation. 
In the first stage, the model identifies domain variables of the input text via a \pextractor{}.
These variables include tone, register, time period, target audience, and genre.
Different approaches extract an additional feature specific to their approach.
The additional features are a bullet point summary, the authorial task, topic, or title. 
The extractor prompts can be derived from \Cref{app:extractor_prompts}.
In the second stage, a paraphrase is generated based on metadata. 
Metadata can be derived automatically from the reference text or provided as ground truth when calling the paraphraser function.
All generator prompts can be found in \Cref{app:generator_prompts}.

\begin{figure}[ht]
  \centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[node distance=2cm]
% Dataset node
\node[dataset] (dataset) {Dataset};

% Anchor for Reference drawing
\node[draw=none, right=of dataset] (refpos) {};

% Draw Reference as an outer rectangle + snakes inside
\node[draw, thick, rounded corners=6pt, minimum width=1.8cm, minimum height=2cm,
      right=of dataset, inner sep=4pt] (reference) {};

% Add multiple snakes inside reference (smaller, stacked)
\foreach \y in {0.8,0.4, 0.0,-0.4,-0.8}{
  \draw[black, very thick, decorate, decoration={snake,amplitude=1pt,segment length=5pt}]
    ($(reference.west)+(0.3,\y)$) -- ($(reference.east)+(-0.3,\y)$);
}

% Caption
\node[below=0.2cm of reference] {Reference};


% Other blocks
\node[above=0.8cm of reference.north] (meta) {Ground Truth Metadata};
\node[block, right=of reference.east] (extractor) {Extractor};
\node[block, right=of extractor] (generator) {Generator};

% paraphrase
\node[draw, thick, rounded corners=6pt, minimum width=1.8cm, minimum height=2cm,
      right=of generator, inner sep=4pt, teal!80!black] (paraphrase) {};

% Add multiple snakes inside reference (smaller, stacked)
\foreach \y in {0.8,0.4, 0.0,-0.4,-0.8}{
  \draw[teal!80!black, very thick, decorate, decoration={snake,amplitude=2pt,segment length=5pt}]
    ($(paraphrase.west)+(0.3,\y)$) -- ($(paraphrase.east)+(-0.3,\y)$);
}
\node[below=0.2cm of paraphrase] {Paraphrase};

% Arrows
\draw[arrow] (dataset) -- (reference.west);
\draw[arrow] (dataset) -- node[pos=0.4, above left=2pt and -12.5pt] {$\{0,1\}$} (meta);
\draw[arrow] (reference.east) -- (extractor);
\draw[dashedarrow] (extractor) -- (generator);
\draw[arrow] (generator) -- (paraphrase);
\draw[dashedarrow] (meta) -- (generator);


\end{tikzpicture}
}
  \caption[Two-step paraphraser]{Visual description of the two-step paraphraser.
  The \pgenerator{} is fed metadata information from the \pextractor{} or from ground truth data.}
  \label{fig:two_step_paraphraser}
\end{figure}


The translation-based paraphraser is another two-step approach in which a text is translated into another language and then back into English. 
We use French as second language.
While conceptually simple, it has been shown to produce only limited stylistic variation~\citep{zhou_paraphrase_2025}.
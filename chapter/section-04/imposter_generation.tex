\section{\acs{llm}-based \Imp{} Generation}
\label{sec:impostor_generation}

% good \imps{}: hard negatives
Following the notion introduced by \citet{koppel_determining_2014}, ideal \imp{} texts can be understood as hard negatives, i.e.\ documents not written by the candidate author, yet sufficiently similar in style to be difficult to distinguish from the candidate's own writing. 
The quality of \imps{} directly affects model effectiveness.
If they are too different from the candidate text, models are trained on trivial contrasts, yielding \acp{fp}. 
Conversely, if \imps{} are too similar to the candidate, the risk of \acp{fn} increases.
This manifests as increased recall in the former case and increased precision in the latter case.

% obstacles for \imp{} generation in the past
Traditional \imp{} generation techniques struggled to control the confounder influencing \imp{} generation. 
The fixed approach samples from a fixed pool of unrelated texts, while the on-the-fly approach fails to align genre with the candidate text~\citep{koppel_determining_2014}. 
Since authorial style is tightly entangled with contextual factors, \imps{} produced by these methods differ systematically from the candidate text, weakening their utility. 
Ideally, \imp{} generation should replicate the conditions of original text production, including task, topic, register, target audience and century. 
In practice, however, this information is rarely available.
In \ac{av}, the candidate author may be deceased, unavailable, or unwilling to cooperate.

% heuristics: paraphrase
\acp{llm} make it possible to approximate this ideal. 
By conditioning generation on the candidate text, external factors such as topic, genre, and register can be more tightly controlled. 
In particular, paraphrasing offers a heuristic for simulating the text generation process. 
The model produces stylistic variants of the candidate's writing while maintaining semantic alignment.


\subsection{Hard Negative Mining via Paraphrasing}

The lack of a universally accepted definition of paraphrases complicates their generation and evaluation~\citep{gohsen_task_oriented_2024}. 
In this work, paraphrases are considered suitable \imps{} when they preserve the topic, genre, tone and length of the original text, introduce sufficient lexical and syntactic variation to prevent near-duplication, and allow limited semantic divergence including \textcolor{orange}{mild} hallucinations.
We propose two approaches to paraphrasing as displayed in \Cref{fig:paraphrasing_approaches}.
We expect the \impAppr{} recall scores to decrease while the precision scores increase, i.e.\ higher confidence in positive labelled text pairs, due to harder negatives leading to fewer positive predictions.

\begin{figure}[h]
    \centering
    \includesvg{images/paraphrasing/idea/Paraphraser_architectures.svg}
    \caption{Different paraphrasing approaches.}
    \label{fig:paraphrasing_approaches}
\end{figure}


Paraphrasing is often analysed along two main dimensions. 
At the lexical level, it involves word-level substitutions such as synonym replacement, typically achieved through rule-based or thesaurus-driven methods. 
At the syntactic level, paraphrasing entails changes to sentence structure, which can be realised through monolingual machine translation~\citep{zhou_paraphrase_2021} or by explicitly prompting \acp{llm}~\citep{kurt_pehlivanoglu_comparative_2024}.
% While more fine-grained taxonomies exist~\citep{zhou_paraphrase_2025}, they are not central here, as this study does not involve training or constructing paraphrase models directly.
Paraphrase quality is largely determined by prompt design and model selection~\citep{Wu_ODD_challenges_2025}.


\subsection{One-step Paraphrasing}

In our one-step approach, \imps{} are generated directly via \ac{llm} prompting. 
Effective prompting proved crucial. 
We found that specifying constraints was vital for valid outputs, and placing task instructions at the end reduced instruction neglect in long-context settings (e.g. \dataGutenberg{} corpus). 
We evaluated three prompt variants.
% The first prompt, displayed in \Cref{fig:one_step_paraphrasing_prompt0}, explicitly decomposes the sentence into subject, verb, and object before reconstruction, encouraging lexical diversity. 
% \begin{figure}[h]
%     \centering
%     \includesvg[width=\textwidth]{/Users/klara/Developer/Uni/MA/master-thesis/images/prompt_impact/one_step_prompts/one_step_prompt0.svg}
%     \caption{The first prompt, hereafter denoted \texttt{prompt0}.}
%     \label{fig:one_step_paraphrasing_prompt0}
% \end{figure}

The first prompt explicitly decomposes the sentence into subject, verb, and object before reconstruction, encouraging lexical diversity. 
\begin{quote}
    \textit{Paraphrase the given sentence by identifying the main subject, verb, and object. Replace each with synonyms or closely related words, adjusting grammar naturally. Keep the new sentence close in length to the original. Output only the final paraphrased sentence.}
\end{quote}

The second prompt instructs the model to preserve meaning while varying wording and structure. 
Both prompts emphasise concise outputs restricted to the paraphrase itself.
\begin{quote}
    \textit{Paraphrase the sentence above without changing its meaning. Use different words and vary the sentence structure while keeping the tone consistent. Keep the new sentence similar in length to the original. Output only the paraphrased sentence, with no explanations or extra text.}
\end{quote}

The third prompt extends the second prompt by explicitly instructing the \ac{llm} to produce paraphrases three times as long as the reference text.
\begin{quote}
  \textit{Paraphrase the text above without changing its meaning. Use different words and vary the sentence structure while maintaining a consistent tone. Your paraphrase should be three times as long than the original. Output only the paraphrased sentence, with NO explanations or extra text.}
\end{quote}

Initial experiments with T5-based HuggingFace models showed poor instruction adherence and frequent malformed outputs. 
Our final experiments therefore employed the models listed in \Cref{tab:base_llms}, accessed via the \ac{gwdg}/\ac{saia} infrastructure\footnote{\url{https://docs.hpc.gwdg.de/} (31.08.2025)}.
Further details on these models are provided in \Cref{app:language_models}.

\begin{table}[h]
\centering
\caption{\acp{llm} used for paraphrasing.}
\label{tab:base_llms}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Model ID}                    & \textbf{Host} \\
\midrule
qwen3-32b                            & \ac{gwdg}/ \ac{saia}    \\
mistral-large-instruct               & \ac{gwdg}/ \ac{saia}    \\
openai-gpt-oss-120b                  & \ac{gwdg}/ \ac{saia}    \\
meta-llama-3.1-8b-instruct           & \ac{gwdg}/ \ac{saia}    \\
\bottomrule   
\end{tabular}%
% }
\end{table}


\subsection{Two-step Paraphrasing}

The two-step approaches are inspired by prior literature~\citep{bevendorff_overview_2024, ayele_overview_2024}, and differ in how they extract and utilize auxiliary information (i.e.\ metadata) from the source text before generating the paraphrase. 
The two steps enforce a clearer disentanglement of content and style at the cost of increased computational overhead.

As displayed in \Cref{fig:two_step_paraphraser}, the two-step paraphrasing decomposes the task into information extraction and text generation. 
In the first stage, the model identifies contextual features of the input text via a \pextractor{}.
These feature include tone, register, time period, target audience, and genre.
Different approaches extract an additional feature specific to their approach.
The additional features are a bullet point summary, the authorial task, topic, or title. 
The extractor prompts can be derived from \Cref{app:extractor_prompts}.
In the second stage, a paraphrase is generated based on the extracted metadata information. 
There are two ways to feed the generator metadata.
The extractor can either extract it from the reference text, or the user passes ground truth information when calling the paraphraser function.
Again, all generator prompts can be found in \Cref{app:generator_prompts}.

\begin{figure}[ht]
  \centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[node distance=2cm]
% Dataset node
\node[dataset] (dataset) {Dataset};

% Anchor for Reference drawing
\node[draw=none, right=of dataset] (refpos) {};

% Draw Reference as an outer rectangle + snakes inside
\node[draw, very thick, rounded corners=6pt, minimum width=1.8cm, minimum height=2cm,
      right=of dataset, inner sep=4pt] (reference) {};

% Add multiple snakes inside reference (smaller, stacked)
\foreach \y in {0.8,0.4, 0.0,-0.4,-0.8}{
  \draw[black, thick, decorate, decoration={snake,amplitude=1pt,segment length=5pt}]
    ($(reference.west)+(0.3,\y)$) -- ($(reference.east)+(-0.3,\y)$);
}

% Caption
\node[below=0.2cm of reference] {Reference};


% Other blocks
\node[above=0.8cm of reference.north] (meta) {Ground Truth Metadata};
\node[block, right=of reference.east] (extractor) {Extractor};
\node[block, right=of extractor] (generator) {Generator};

% paraphrase
\node[draw, very thick, rounded corners=6pt, minimum width=1.8cm, minimum height=2cm,
      right=of generator, inner sep=4pt, teal!80!black] (paraphrase) {};

% Add multiple snakes inside reference (smaller, stacked)
\foreach \y in {0.8,0.4, 0.0,-0.4,-0.8}{
  \draw[teal!80!black, thick, decorate, decoration={snake,amplitude=2pt,segment length=5pt}]
    ($(paraphrase.west)+(0.3,\y)$) -- ($(paraphrase.east)+(-0.3,\y)$);
}
\node[below=0.2cm of paraphrase] {Paraphrase};

% Arrows
\draw[arrow] (dataset) -- (reference.west);
\draw[arrow] (dataset) -- node[midway, above] {$\{0,1\}$} (meta);
\draw[arrow] (reference.east) -- (extractor);
\draw[dashedarrow] (extractor) -- (generator);
\draw[arrow] (generator) -- (paraphrase);
\draw[dashedarrow] (meta) -- (generator);


\end{tikzpicture}
}
  \caption[Two-step paraphraser.]{Visual description of the two-step paraphraser.
  The \pgenerator{} is fed metadata information from the \pextractor{} or from ground truth data.}
  \label{fig:two_step_paraphraser}
\end{figure}


The translation-based paraphraser is a related two-step approach in which a text is translated into another language and then back into English. 
While conceptually simple, it has been shown to produce only limited stylistic variation~\citep{zhou_paraphrase_2025}.
\subsection{Exp.\ 4: Assessing the Impact of the Prompt on Paraphrasing}
\label{subsec:prompt_impact_res}


Post-processing was required to remove reasoning traces present in some model outputs, particularly in generations from models such as \texttt{qwen3-32b}. 
These traces, typically delimited by \texttt{</think>}, consist of repeated fragments of the input prompt and do not contribute to the semantic content of the paraphrase. 
We therefore excluded them to retain only the task-relevant text produced by the \ac{lm}.

After post-processing, we computed the relative length difference between each reference and its corresponding paraphrase for all model–prompt combinations. 
The distribution of these differences is presented in \autoref{fig:prompt_impact_post_processed}. 
Because our objective in \imp{} generation is to control for confounding variables, a paraphrase length close to that of the reference is interpreted as an indicator of higher quality. 
We additionally performed a manual assessment of content quality that focused on paraphrases both extremely long and length-balanced relative to the reference.

\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/prompt_impact/paraphraser_length_distribution_post_process_len_perc(qwen)_linear.svg}
    \caption[Impact of different prompts on paraphrases.]{
    Distribution of relative paraphrase lengths across different prompts, after post-processing.    
    }
    \label{fig:prompt_impact_post_processed}
\end{figure}

Our results indicate that the relative length difference depended strongly on the prompt used to instruct the \ac{lm}. 
In particular, the third prompt (hereafter \texttt{prompt2}) explicitly instructed models to generate paraphrases approximately three times longer than the reference. 
Although this instruction may appear extreme, it consistently produced paraphrases of more comparable length across different models. 
Aggregated statistics are provided in \autoref{tab:impact_prompts_paraphrases_lengths}.


\begin{table}[h]
\centering
\caption[Impact of the prompts on paraphrase lengths.]{Impact of the prompts on paraphrase lengths. 
Relative length is defined as $\frac{len(paraphrase)}{len(reference)}\times 100\%$ and denoted $r$. 
Optimal paraphrases are expected to approximate the reference length, i.e.\ $\diameter r \approx 100$. 
Subscript $pp$ indicates post-processed outputs (with reasoning traces removed). 
``Count'' denotes the number of paraphrases considered for each setting. 
For \texttt{prompt2}, only post-processed results are reported.
}
\label{tab:impact_prompts_paraphrases_lengths}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llrrrrr}
\toprule
Paraphraser & Prompt  & \diameter $r$ & $\sigma r$ & \diameter $r_{pp}$ & $\sigma r_{pp}$ & Count \\
\midrule
meta-llama-3.1-8b-instruct & prompt0 & 39.93 & 52.64 & 39.93 & 52.64  & 135   \\
                            & prompt1 & 40.27  & 24.21 & 40.27  & 24.21 & 124 \\
                            & prompt2 & - & - & \textbf{98.24} & 25.65 & 37  \\
mistral-large-instruct & prompt0 & 1.89   & 1.0   & 1.89   & 1.0   & 138 \\
                        & prompt1 & 13.09  & 17.96 & 13.09  & 17.96 & 129 \\
                        & prompt2 & - & - & \textbf{75.28}  & 13.15 & 36  \\
openai-gpt-oss-120b   & prompt0 & 5.53   & 13.47 & 5.53   & 13.47 & 139 \\
                        & prompt1 & 19.21  & 25.0  & 19.21  & 25.0  & 129 \\
                        & prompt2 & - & - & \textbf{150.43} & 55.28 & 38  \\
qwen3-32b           & prompt0 & 88.36  & 70.02 & 18.68  & 24.09 & 134 \\
                        & prompt1 & 95.73  & 47.72 & 38.34  & 15.64 & 123 \\
                        & prompt2 & - & - & \textbf{77.12}  & 19.03 & 39 \\
                                \bottomrule
\end{tabular}%
}
\end{table}

Based on these findings, subsequent experiments adopted the following design choices: 
(i) exclude paraphrases generated with \texttt{prompt0} and \texttt{prompt1}, 
(ii) remove all \texttt{</think>}–delimited reasoning traces via post-processing, and 
(iii) discard paraphrases shorter than $60\%$ of the reference length.

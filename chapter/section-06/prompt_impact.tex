\section{Exp.\ 4: Comparing Prompts}%Assessing the Impact of the Prompt on Paraphrasing}
\label{sec:prompt_impact_res}

In this experiment, we investigate how different prompting strategies influence the quality of paraphrases generated by \acp{llm}. 
To this end, we measured the relative length difference between reference and paraphrase pairs across different \ac{llm}–prompt combinations. 
A subset of pairs was also manually inspected to assess semantic fidelity and readability.

Post-processing was required to remove reasoning traces present in some model outputs, particularly in generations from models such as \texttt{qwen3-32b}. 
These traces, typically delimited by \texttt{</think>}, consist of repeated fragments of the input prompt and do not contribute to the semantic content of the paraphrase. 
We excluded them to retain only the task-relevant text produced by the \ac{llm}.

After post-processing, we computed the relative length difference between each reference and its corresponding paraphrase for all model–prompt combinations. 
The distribution of these differences is presented in \Cref{fig:prompt_impact_post_processed}. 
Because our objective in \imp{} generation is to control for confounding variables, a paraphrase length close to that of the reference is interpreted as an indicator of higher paraphrase quality. 
We additionally performed a manual assessment of content quality that focused on paraphrases both extremely long and length-balanced relative to the reference.

\begin{figure}[H]
    \centering
    \includesvg[width=\textwidth]{images/prompt_impact/paraphraser_length_distribution_post_process_len_perc(qwen)_linear.svg}
    \caption[Impact of different prompts on paraphrases]{
    Distribution of relative paraphrase lengths after post-processing across different prompts.    
    The dotted gray line marks the optimal paraphrase length.
    \texttt{prompt2} consistently generates paraphrases whose lengths are more comparable to the reference than those produced by the other two prompts.
    }
    \label{fig:prompt_impact_post_processed}
\end{figure}

Our results show that the relative length difference of paraphrases strongly depended on the prompt used to instruct the \ac{llm}. 
Notably, the third prompt, i.e.\ \texttt{prompt2}, explicitly instructed models to generate paraphrases three times longer than the reference. 
While this instruction might seem extreme, it consistently generated paraphrases whose lengths were more comparable to the reference than those produced by the other two prompts, across different models, as summarised in \Cref{tab:impact_prompts_paraphrases_lengths}.

\begin{table}[h]
\centering
\caption[Impact of different prompts on paraphrase lengths]{Impact of the prompts on paraphrase lengths. 
Relative length difference is defined as $\frac{\mathrm{len}(paraphrase)}{\mathrm{len}(reference)}\times 100\%$ and denoted $d$. 
Optimal paraphrases are expected to approximate the reference length, i.e.\ $\diameter d \approx 100$. 
Subscript $pp$ indicates post-processed outputs (with reasoning traces removed). 
``Count'' denotes the number of paraphrases considered for each setting. 
For \texttt{prompt2}, only post-processed results are reported.
Bold \diameter $r_{pp}$ values are those closest to the optimal paraphrase length.
}
\label{tab:impact_prompts_paraphrases_lengths}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llrrrrr@{}}
\toprule
Paraphraser & Prompt  & \diameter $d$ & $\sigma d$ & \diameter $d_{pp}$ & $\sigma d_{pp}$ & Count \\
\midrule
meta-llama-3.1-8b-instruct & prompt0 & 39.93 & 52.64 & 39.93 & 52.64  & 135   \\
                            & prompt1 & 40.27  & 24.21 & 40.27  & 24.21 & 124 \\
                            & prompt2 & - & - & \textbf{98.15} & 24.97 & 639  \\
mistral-large-instruct & prompt0 & 1.89   & 1.0   & 1.89   & 1.0   & 138 \\
                        & prompt1 & 13.09  & 17.96 & 13.09  & 17.96 & 129 \\
                        & prompt2 & - & - & \textbf{79.70}  & 11.40 & 449  \\
openai-gpt-oss-120b   & prompt0 & 5.53   & 13.47 & 5.53   & 13.47 & 139 \\
                        & prompt1 & 19.21  & 25.0  & 19.21  & 25.0  & 129 \\
                        & prompt2 & - & - & \textbf{147.54} & 48.45 & 590  \\
qwen3-32b           & prompt0 & 88.36  & 70.02 & 18.68  & 24.09 & 134 \\
                        & prompt1 & 95.73  & 47.72 & 38.34  & 15.64 & 123 \\
                        & prompt2 & - & - & \textbf{81.42}  & 13.46 & 532 \\
                                \bottomrule
\end{tabular}%
}
\end{table}

Manual inspection indicated that paraphrases generated with \texttt{prompt2} exhibited only mild hallucinations and generally remained on topic. 
Moreover, they outperformed paraphrases from other prompts in terms of semantic preservation across all \acp{llm}.

Based on these findings, subsequent experiments adopted the following design choices: 
(i) exclude paraphrases generated with \texttt{prompt0} and \texttt{prompt1}, 
(ii) remove all \texttt{</think>}–delimited reasoning traces via post-processing, and 
(iii) discard paraphrases shorter than $60\%$ of the reference length.

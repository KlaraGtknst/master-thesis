\section{Exp.\ 4: Comparing Prompts}%Assessing the Impact of the Prompt on Paraphrasing}
\label{subsec:prompt_impact_res}


Post-processing was required to remove reasoning traces present in some model outputs, particularly in generations from models such as \texttt{qwen3-32b}. 
These traces, typically delimited by \texttt{</think>}, consist of repeated fragments of the input prompt and do not contribute to the semantic content of the paraphrase. 
We excluded them to retain only the task-relevant text produced by the \ac{llm}.

After post-processing, we computed the relative length difference between each reference and its corresponding paraphrase for all model–prompt combinations. 
The distribution of these differences is presented in \Cref{fig:prompt_impact_post_processed}. 
Because our objective in \imp{} generation is to control for confounding variables, a paraphrase length close to that of the reference is interpreted as an indicator of higher paraphrase quality. 
We additionally performed a manual assessment of content quality that focused on paraphrases both extremely long and length-balanced relative to the reference.

\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/prompt_impact/paraphraser_length_distribution_post_process_len_perc(qwen)_linear.svg}
    \caption[Impact of different prompts on paraphrases.]{
    Distribution of relative paraphrase lengths across different prompts, after post-processing.    
    }
    \label{fig:prompt_impact_post_processed}
\end{figure}

Our results show that the relative length difference of paraphrases strongly depended on the prompt used to instruct the \ac{llm}. 
Notably, the third prompt (hereafter \texttt{prompt2}) explicitly instructed models to generate paraphrases roughly three times longer than the reference. 
While this instruction might seem extreme, it consistently produced paraphrases of more comparable length across different models, as summarised in \Cref{tab:impact_prompts_paraphrases_lengths}.

\begin{table}[h]
\centering
\caption[Impact of the prompts on paraphrase lengths.]{Impact of the prompts on paraphrase lengths. 
Relative length is defined as $\frac{len(paraphrase)}{len(reference)}\times 100\%$ and denoted $r$. 
Optimal paraphrases are expected to approximate the reference length, i.e.\ $\diameter r \approx 100$. 
Subscript $pp$ indicates post-processed outputs (with reasoning traces removed). 
``Count'' denotes the number of paraphrases considered for each setting. 
For \texttt{prompt2}, only post-processed results are reported.
Bold \diameter $r_{pp}$ values are those closest to the optimal paraphrase length.
}
\label{tab:impact_prompts_paraphrases_lengths}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llrrrrr@{}}
\toprule
Paraphraser & Prompt  & \diameter $r$ & $\sigma r$ & \diameter $r_{pp}$ & $\sigma r_{pp}$ & Count \\
\midrule
meta-llama-3.1-8b-instruct & prompt0 & 39.93 & 52.64 & 39.93 & 52.64  & 135   \\
                            & prompt1 & 40.27  & 24.21 & 40.27  & 24.21 & 124 \\
                            & prompt2 & - & - & \textbf{98.24} & 25.65 & 37  \\
mistral-large-instruct & prompt0 & 1.89   & 1.0   & 1.89   & 1.0   & 138 \\
                        & prompt1 & 13.09  & 17.96 & 13.09  & 17.96 & 129 \\
                        & prompt2 & - & - & \textbf{75.28}  & 13.15 & 36  \\
openai-gpt-oss-120b   & prompt0 & 5.53   & 13.47 & 5.53   & 13.47 & 139 \\
                        & prompt1 & 19.21  & 25.0  & 19.21  & 25.0  & 129 \\
                        & prompt2 & - & - & \textbf{150.43} & 55.28 & 38  \\
qwen3-32b           & prompt0 & 88.36  & 70.02 & 18.68  & 24.09 & 134 \\
                        & prompt1 & 95.73  & 47.72 & 38.34  & 15.64 & 123 \\
                        & prompt2 & - & - & \textbf{77.12}  & 19.03 & 39 \\
                                \bottomrule
\end{tabular}%
}
\end{table}

Paraphrases generated with \texttt{prompt2} exhibited only mild hallucinations and generally remained on topic. 
Furthermore, manual inspection indicated that these paraphrases outperformed those from other prompts in terms of semantic preservation.

Based on these findings, subsequent experiments adopted the following design choices: 
(i) exclude paraphrases generated with \texttt{prompt0} and \texttt{prompt1}, 
(ii) remove all \texttt{</think>}–delimited reasoning traces via post-processing, and 
(iii) discard paraphrases shorter than $60\%$ of the reference length.

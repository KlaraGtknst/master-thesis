\section{Exp.\ 2: Comparison of Paraphrasers}
\label{sec:comp_paraphrases}

To evaluate the quality of generated paraphrases, we conducted two experiments. 
In Exp.\ 2(a), we assessed paraphrasing using standard quantitative metrics, while in Exp.\ 2(b), we compared the extracted information and text lengths of the generated paraphrases to the ground truth metadata and original texts, for different paraphrasers respectively.

\paragraph{Exp.\ 2(a): Quantitative evaluation.}

Paraphrasing scores were computed separately for the \dataBlog{}, \dataGutenberg{}, and \dataStudent{} datasets. 
\Cref{fig:sem_syn_blog} presents aggregated semantic and syntactic measurement scores for the \dataBlog{} dataset, while results for \dataGutenberg{} are provided in the Appendix in \Cref{sec:app_paraphrases}.

Syntactic similarity $\diameter_{syn}$ was quantified by averaging \ac{rouge}-1, \ac{rouge}-L, and BLEU scores. 
Semantic similarity $\diameter_{sem}$ was assessed using \ac{bert}Score, cosine similarity of \ac{bert}-based embeddings, and \acs{glove} \ac{wmd}. 
It is important to note that for our purpose, syntactic diversity is desirable.
High syntactic similarity values may reflect near-identical paraphrases due to n-gram overlap. 
Semantic similarity measures the content overlap between the paraphrase and the original text, often via vector-based cosine similarity. 
% The precise interpretation of these metrics remains somewhat unclear.

\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/paraphrasing/experiments/sem_syn_scatter/Blog_sem_syn_scatter_grouped_by_Paraphraser_altered.svg}
    \caption[Comparison of paraphrasers on the \dataBlog{} dataset.]{Average semantic $\diameter_{sem}$ and syntactic similarity $\diameter_{syn}$ for different prompts grouped by paraphraser on the \dataBlog{}.}
    \label{fig:sem_syn_blog}
\end{figure}


Analysis reveals two distinct clusters corresponding to one-step and two-step paraphrasers. 
Most one-step paraphrasers achieve lower syntactic and semantic similarity than two-step paraphrasers. 
The exception is the one-step paraphraser using \texttt{qwen3-32b}, which exhibits higher syntactic similarity than most two-step paraphrasers. 
The translation-based paraphraser emerges as an outlier in terms of both syntactic and semantic similarity. 
Overall, most paraphrases fall within the desired quadrant of high semantic similarity with low syntactic similarity.


\paragraph{Exp.\ 2(b): Evaluation of prompt adherence.}

Our two-step paraphrasing approach relies on extracting valid information from the source text. 
To evaluate the quality of extraction by the \pextractor{}, we compared the topic, genre, and century to the ground truth metadata for the \dataBlog{}, \dataGutenberg{}, and \dataStudent{} datasets. 
Genre and topic were evaluated in terms of semantic similarity, while century was assessed via the percentage deviation from the ground truth.

We observed that instructions for the \pextractor{} must follow the input text, as \acp{llm} tend to focus attention towards the end of the input. 
Otherwise, the \pextractor{} failed to produce the requested JSON format for long texts from the \dataGutenberg{} dataset. 
The prompts are provided in \autoref{app:extractor_prompts}.
Results across datasets are summarised in \Cref{tab:extraction_eval_stats}, with $\diameter$ and $\sigma$ denoting the mean and standard deviation across the five selected samples. 
The \dataBlog{} dataset proved most challenging for genre and topic extraction. 
Although the \dataGutenberg{} dataset exhibited the largest differences in text length between reference and paraphrase, the \pextractor{} achieved its best performance on this dataset.


\begin{table}[h]
\centering
\caption[Extraction effectiveness and length deviation for different datasets.]{Extraction effectiveness and length deviation for different datasets. Five documents per dataset were processed using the \pextractor{} to obtain genre, century, and topic.}
\label{tab:extraction_eval_stats}
\begin{tabular}{@{}lrrrrrrrr@{}} % numbers should be right aligned, text left aligned
\toprule
 &
  \multicolumn{2}{l}{\textbf{Genre}} &
  \multicolumn{2}{l}{\textbf{Century}} &
  \multicolumn{2}{l}{\textbf{Topic}} &
  \multicolumn{2}{l}{\textbf{Length}} \\
  \textbf{Dataset}
 &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} \\
  \midrule
\dataBlog{}            & 0.38 & 0.06  & 0.99 & 0.02 & 0.04  & 0.05  & -0.10 & 0.73 \\
\dataGutenberg{}       & 0.58 & 0.14  & 1.00 & 0.04 & 0.30 & 0.15 & -1.00 & 0.00  \\
\dataStudent{} & 0.53 & 0.26 & 0.60 & 0.55 & 0.25 & 0.05  & 0.34 & 0.20 \\
  \bottomrule
\end{tabular}%
\end{table}

\section{Exp. 2: Comparison of different Paraphrasers}
\label{sec:comp_paraphrases}

To evaluate the quality of the generated paraphrases, we not only compute quantitative paraphrase measurements in Exp. 2(a), but also compare the extracted information and text lengths of the generated paraphrases to the ground truth metadata and original text in Exp. 2(b), respectively.

\paragraph{Exp. 2(a): Quantitative evaluation.}

We evaluated paraphrasing scores on the \dataBlog{}, \dataGutenberg{}, and the \dataStudent{} dataset separably.
In \autoref{fig:sem_syn_blog}, as per \citet{gohsen_captions_2023}, we aggregate semantic and syntactic measurement scores of the \dataBlog{} dataset for better interpretability.
\dataGutenberg{} results are attached in the Appendix in \autoref{sec:app_paraphrases}.

For syntactic measures, we average \ac{rouge}-1, \ac{rouge}-L, and BLEU.
For semantic measure, we average BERTScore, cosine similarity based on BERT-based embeddings, and \ac{wmd}.
Generally high scores in BLEU, \ac{rouge}, METEOR mean nearly identical paraphrases due to high n-gram overlap.
In this case, we want syntactic diversity, rendering these measures unintuitive 
for high values do not necessarily correspond to syntactically diverse paraphrases.
Semantic similarity measurements compare the content of the paraphrase to the original text, 
often under application of the cosine of a vector.
It should be noted that it is unclear what these metrics actually measure.

\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/paraphrasing/experiments/sem_syn_scatter/Blog_sem_syn_scatter_grouped_by_Paraphraser.svg}
    \caption{Average semantic and syntactic similarity for different paraphraser on the \dataBlog{}.}
    \label{fig:sem_syn_blog}
\end{figure}


% findings
There are two clusters of one-step and two-step paraphrasers, respectively.
The majority of one-step paraphrasers achieves lower syntactic and semantic scores than two-step paraphrasers.
The one-step paraphraser based on \texttt{qwen3-32b}, however, obtains higher syntactic similarity scores than most of the two-step paraphrasers.
The only outlier in terms of both syntactic and semantic similarity is the translation-based approach to paraphrasing.
We find that the different prompts used for paraphrasing do not have a big impact on syntactic or semantic similarity (cf. \autoref{sec:app_paraphrases}).
Generally, almost all paraphrases are in the quadrant we consider optimal, i.e. high semantic similarity while keeping low syntactic similarity.


% shortcomings of paraphrasing metrics and need for human evaluation
% While high n-gram overlap might not be the indicator of a good paraphrase in the sense of high syntactic diversity, it is not clear if high cosine similarity between the embedding of two texts is a good indicator of a good paraphrase.
% Moreover, for all metrics, threshold values for good paraphrases are not well-defined.
% It remains to be found whether the worst performing paraphrases are still good enough in terms of human evaluation.
% We therefore also employed qualitative evaluation of the paraphrases.



\paragraph{Exp. 2(b): Evaluation of prompt adherence.}
% Since we developed our paraphrasing approaches with the goal of creating hard negatives for the \impAppr{} by \citet{koppel_determining_2014} in mind, we have specific criteria for good paraphrases.
Our two-step paraphrasing approach builds on the extraction of valid information.
Extracted information is the basis for subsequent generation of paraphrases.
In order to evaluate the quality of the information extracted by the \pextractor{}, we decided to compare the topic, genre, and the century to the ground truth available for the \dataBlog{}, \dataGutenberg{} and the \dataStudent{} dataset.
The first two are compared in terms of semantic similarity, while century is evaluated in terms of percental deviation from the ground truth century.

We found that the instructions for the \pextractor{} have to be positioned after the input text since \acp{llm} seem to focus their attention on the end of an input text.
We found that \pextractor{} were unable to return the requested JSON format for long texts from the \dataGutenberg{} dataset.
The results across the datasets are displayed in \autoref{tab:extraction_eval_stats}.
$\diameter, \sigma$ denotes the average scores and standard deviation across scores of the five selected text samples, respectively.
For genre and topic extraction, the \dataBlog{} seems to be most difficult for the \pextractor{}.
Apart from length difference between reference and paraphrase, the \pextractor{} performs best on the \dataGutenberg{}.

\begin{table}[h]
\centering
\caption{Degree of adherence to prompt for different datasets.
We selected five documents per dataset and used a \pextractor{} to obtain genre, century and topic of the texts.}
\label{tab:extraction_eval_stats}
\begin{tabular}{lllllllll}
\toprule
 &
  \multicolumn{2}{l}{\textbf{Genre}} &
  \multicolumn{2}{l}{\textbf{Century}} &
  \multicolumn{2}{l}{\textbf{Topic}} &
  \multicolumn{2}{l}{\textbf{Length}} \\
 &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} \\
  \midrule
\textbf{\dataBlog{}}            & 0.38 & 0.06  & 0.99 & 0.02 & 0.04  & 0.05  & -0.10 & 0.73 \\
\textbf{\dataGutenberg{}}       & 0.58 & 0.14  & 1.00 & 0.04 & 0.3 & 0.15 & -1.00 & 0.00  \\
\textbf{\dataStudent{}} & 0.53 & 0.26 & 0.60 & 0.55 & 0.25 & 0.05  & 0.34 & 0.20 \\
  \bottomrule
\end{tabular}%
\end{table}

It is noteworthy, that opposed to these results, paraphrases from other experiments tended to be way shorter than the reference text.
In multiple cases, the \pgenerator{} returned something along the line of \texttt{I’m sorry, but I can’t help with that}.


% \begin{figure}[htbp]
%     \centering
%     \includesvg[width=\textwidth]{images/paraphrasing/experiments/extraction/radar_extraction_quality_per_dataset.svg}
%     \caption{.}
%     \label{fig:extraction_eval}
% \end{figure}

\section{Exp. 2: Comparison of different Paraphrasers}



% \subsubsection{\pextractor{}}
\paragraph{Exp. 2(a): Quantitative evaluation}
% make subsubsub
% \subsubsection{Paraphrase Generation}
% \label{subsec:paraphrase_generation}
To evaluate the quality of the paraphrases generated by the \pgenerator{}, 
we not only computed different paraphrase quality metrics, 
but also compared the text lengths of the generated paraphrases and the original text.

\textcolor{red}{TODO: insert table with results}

% shortcomings of paraphrasing metrics and need for human evaluation
Though easier to reproduce, it is somehow unclear what paraphrase metrics actually measure beyond what their formula states.
While high n-gram overlap might not be the indicator of a good paraphrase in the sense of high syntactic diversity, 
it is not clear if high cosine similarity between the embedding of two texts is a good indicator of a good paraphrase.
Moreover, for all metrics, threshold values for good paraphrases are not well-defined.
It remains to be found whether the worst performing paraphrases are still good enough in terms of human evaluation.
We therefore also employed qualitative evaluation of the paraphrases.


\paragraph{Exp. 2(b): Evaluation of prompt adherence}
% Since we developed our paraphrasing approaches with the goal of creating hard negatives for the \impAppr{} by \citet{koppel_determining_2014} in mind, we have specific criteria for good paraphrases.
Our two-step paraphrasing approach builds on the extraction of valid information, since this information is the basis for the subsequent generation approach.
In order to evaluate the quality of the information extracted by the \pextractor{}, we decided to compare the century, genre, and the paraphrase-specific topic to the ground truth available for the \dataBlog{}, \dataGutenberg{} and the \dataStudent{} dataset.
The latter two are compared in terms of semantic similarity, while century is evaluated in terms of percental deviation from the ground truth century.

We found that the instructions for the \pextractor{} have to be positioned after the text to be extracted, due to the inability of the \pextractor{} to return the extracted information in the specified JSON format when the prompt was at the beginning of the input for long texts such as those from the \dataGutenberg{} dataset.
The results are displayed in \autoref{tab:extraction_eval_stats}.
For genre and century extraction, the \dataStudent{} seems to be most difficult for the \pextractor{}.
Apart from length difference between reference and paraphrase, the \pextractor{} performs best on the \dataGutenberg{}.

It is noteworthy, that opposed to these results, paraphrases from other experiments tended to be way shorter than the reference text.


\begin{table}[]
\centering
\caption{Degree of adherence to prompt for different datasets.
We selected five documents per dataset and used a \pextractor{} to obtain genre, century and topic of the texts.}
\label{tab:extraction_eval_stats}
\begin{tabular}{lllllllll}
\toprule
 &
  \multicolumn{2}{l}{\textbf{Genre}} &
  \multicolumn{2}{l}{\textbf{Century}} &
  \multicolumn{2}{l}{\textbf{Topic}} &
  \multicolumn{2}{l}{\textbf{Length}} \\
 &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} \\
  \midrule
\textbf{\dataBlog{}}            & 0.38 & 0.06  & 0.99 & 0.02 & 0.04  & 0.05  & -0.10 & 0.73 \\
\textbf{\dataGutenberg{}}       & 0.58 & 0.14  & 1.00 & 0.04 & 0.3 & 0.15 & -1.00 & 0.00  \\
\textbf{\dataStudent{}} & 0.53 & 0.26 & 0.60 & 0.55 & 0.25 & 0.05  & 0.34 & 0.20 \\
  \bottomrule
\end{tabular}%
\end{table}


Irrespective of the quality of the text extraction, we hypothesize that the quality of the final result of the \pgenerator{} will be good irrespective of the quality of the \pextractor{}.
We motivate this by the fact that both the \pextractor{} and the \pgenerator{} are \acp{llm} and therefore generate text similar.
% Attention: Causal vs. masked language model work different

% \begin{figure}[htbp]
%     \centering
%     \includesvg[width=\textwidth]{images/paraphrasing/experiments/extraction/radar_extraction_quality_per_dataset.svg}
%     \caption{.}
%     \label{fig:extraction_eval}
% \end{figure}

% make subsubsub
\subsubsection{Measures and Findings}
\label{subsec:measures_and_findings}

% shortcoming of traditional quantitative paraphrase metrics
We used state-of-the art measures for the quantitative evaluation of paraphrases. 
Unfortunately, these measures can be misleading since it is unclear what they actually measure.
Generally high scores in BLEU, \ac{rouge}, METEOR mean nearly identical paraphrase (high n-gram overlap).
In this case, we want value syntactic diversity, rendering these measures unintuitive 
for high values do not necessarily correspond to good paraphrases.
Semantic similarity measurements compare the content of the paraphrase to the original text, 
where the interpretation of the cosine of a vector is not clear either.

% findings
Non-naive paraphrasers generally lower syntactic scores than naive paraphrasers,
supposedly because they have a weaker influence on the \pgenerator{} 
(i.e. disclosing extracted content rather than the original text)
leaving more room for variance in texts.
Consequently, \enquote{bad} scored non-naive paraphrases are good in terms of syntactic diversity.
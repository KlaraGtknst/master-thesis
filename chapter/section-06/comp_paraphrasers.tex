\section{Exp.\ 2: Comparison of Different Paraphrasers}
\label{sec:comp_paraphrases}

To evaluate the quality of generated paraphrases, we conducted two experiments. 
In Exp.\ 2(a), we assessed paraphrasing using standard quantitative metrics, while in Exp.\ 2(b), we compared the extracted information and text lengths of the generated paraphrases to the ground truth metadata and original texts, respectively.

\paragraph{Exp.\ 2(a): Quantitative evaluation.}

Paraphrasing scores were computed separately for the \dataBlog{}, \dataGutenberg{}, and \dataStudent{} datasets. 
\autoref{fig:sem_syn_blog} presents aggregated semantic and syntactic measurement scores for the \dataBlog{} dataset, while results for \dataGutenberg{} are provided in the Appendix in \autoref{sec:app_paraphrases}.

Syntactic similarity was quantified by averaging \ac{rouge}-1, \ac{rouge}-L, and BLEU scores. 
Semantic similarity was assessed using BERTScore, cosine similarity of BERT-based embeddings, and SBERT \ac{wmd}. 
It is important to note that for our purposes, syntactic diversity is desirable.
High syntactic similarity values may reflect near-identical paraphrases due to n-gram overlap. 
Semantic similarity measures the content overlap between the paraphrase and the original text, often via vector-based cosine similarity. 
The precise interpretation of these metrics remains somewhat unclear.

\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/paraphrasing/experiments/sem_syn_scatter/Blog_sem_syn_scatter_grouped_by_Paraphraser.svg}
    \caption[Comparison of paraphrasers on the \dataBlog{} dataset.]{Average semantic $\diameter_{sem}$ and syntactic similarity $\diameter_{syn}$ for different paraphraser on the \dataBlog{}.}
    \label{fig:sem_syn_blog}
\end{figure}


Analysis reveals two distinct clusters corresponding to one-step and two-step paraphrasers. 
Most one-step paraphrasers achieve lower syntactic and semantic similarity than two-step paraphrasers. 
The exception is the one-step paraphraser using \texttt{qwen3-32b}, which exhibits higher syntactic similarity than most two-step paraphrasers. 
The translation-based approach emerges as an outlier in terms of both syntactic and semantic similarity. 
Different prompt formulations appear to have minimal impact on these similarity scores (cf. \autoref{sec:app_paraphrases}). 
Overall, most paraphrases fall within the desired quadrant of high semantic similarity with low syntactic similarity.


\paragraph{Exp.\ 2(b): Evaluation of prompt adherence.}

Our two-step paraphrasing approach relies on extracting valid information from the source text. 
To evaluate the quality of extraction by the \pextractor{}, we compared the topic, genre, and century to the ground truth metadata for the \dataBlog{}, \dataGutenberg{}, and \dataStudent{} datasets. 
Genre and topic were evaluated in terms of semantic similarity, while century was assessed via the percentage deviation from the ground truth.

We observed that instructions for the \pextractor{} must follow the input text, as \acp{llm} tend to focus attention toward the end of the input. 
Otherwise, the \pextractor{} failed to produce the requested JSON format for long texts from the \dataGutenberg{} dataset. 
Results across datasets are summarized in \autoref{tab:extraction_eval_stats}, with $\diameter$ and $\sigma$ denoting the mean and standard deviation across the five selected samples. 
The \dataBlog{} dataset proved most challenging for genre and topic extraction. 
While differences in text length between reference and paraphrase were maximal among all datasets, the \pextractor{} performed best on the \dataGutenberg{} dataset.


\begin{table}[h]
\centering
\caption[Extraction performance and length matching for different datasets.]{Extraction performance and length matching for different datasets. Five documents per dataset were processed using the \pextractor{} to obtain genre, century, and topic.}
\label{tab:extraction_eval_stats}
\begin{tabular}{lllllllll}
\toprule
 &
  \multicolumn{2}{l}{\textbf{Genre}} &
  \multicolumn{2}{l}{\textbf{Century}} &
  \multicolumn{2}{l}{\textbf{Topic}} &
  \multicolumn{2}{l}{\textbf{Length}} \\
  \textbf{Dataset}
 &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} \\
  \midrule
\dataBlog{}            & 0.38 & 0.06  & 0.99 & 0.02 & 0.04  & 0.05  & -0.10 & 0.73 \\
\dataGutenberg{}       & 0.58 & 0.14  & 1.00 & 0.04 & 0.3 & 0.15 & -1.00 & 0.00  \\
\dataStudent{} & 0.53 & 0.26 & 0.60 & 0.55 & 0.25 & 0.05  & 0.34 & 0.20 \\
  \bottomrule
\end{tabular}%
\end{table}

Notably, paraphrases generated in other experiments were often substantially shorter than the reference texts. 
In multiple cases, the \pgenerator{} returned placeholders such as \texttt{I’m sorry, but I can’t help with that}.

\section{Exp. 2: Comparison of different Paraphrasers}

To evaluate the quality of the generated paraphrases, we not only computed different paraphrase quality metrics in Exp. 2(a), but also compared the extracted information and text lengths of the generated paraphrases and the original text in Exp. 2(b).

\paragraph{Exp. 2(a): Quantitative evaluation}

We evaluated paraphrasing scores on the \dataBlog{}, \dataGutenberg{}, and the \dataStudent{} dataset separably.
Non-aggregated results are attached in the Appendix in \autoref{sec:app_paraphrases}.
In \autoref{fig:sem_syn_blog}, as per \citet{gohsen_captions_2023}, we aggregate semantic and syntactic measurement scores for better interpretability.
For syntactic measures, we average \ac{rouge}-1, \ac{rouge}-L, and BLEU.
For semantic measure, we average BERTScore, cosine similarity based on BERT-based embeddings, and \ac{wmd}.
Unfortunately, these measures can be misleading since it is unclear what they actually measure.
Generally high scores in BLEU, \ac{rouge}, METEOR mean nearly identical paraphrase (high n-gram overlap).
In this case, we want value syntactic diversity, rendering these measures unintuitive 
for high values do not necessarily correspond to good paraphrases.
Semantic similarity measurements compare the content of the paraphrase to the original text, 
where the interpretation of the cosine of a vector is not clear either.

\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/paraphrasing/experiments/sem_syn_scatter/Blog_sem_syn_scatter_grouped_by_Paraphraser.svg}
    \caption{Average semantic and syntactic similarity for different paraphraser on the \dataBlog{}.}
    \label{fig:sem_syn_blog}
\end{figure}


% findings
The majority of naive paraphrasers achieves lower syntactic and semantic scores than non-naive paraphrasers.
The naive paraphraser based on \texttt{qwen3-32b}, however, obtains higher semantic similarity scores than most of the non-naive paraphrasers.
It is noteworthy, that there are two clusters of naive and non-naive paraphrasers, respectively.
The only outlier in terms of both syntactic and semantic similarity is the translation-based approach to paraphrasing.
We find that the different prompts used for paraphrasing do not have a big impact on syntactic or semantic similarity (cf. \autoref{sec:app_paraphrases}).
Generally, all paraphrases are in the quadrant we consider optimal, i.e. high semantic similarity while keeping low syntactic similarity.


% shortcomings of paraphrasing metrics and need for human evaluation
% While high n-gram overlap might not be the indicator of a good paraphrase in the sense of high syntactic diversity, it is not clear if high cosine similarity between the embedding of two texts is a good indicator of a good paraphrase.
% Moreover, for all metrics, threshold values for good paraphrases are not well-defined.
% It remains to be found whether the worst performing paraphrases are still good enough in terms of human evaluation.
% We therefore also employed qualitative evaluation of the paraphrases.



\paragraph{Exp. 2(b): Evaluation of prompt adherence}
% Since we developed our paraphrasing approaches with the goal of creating hard negatives for the \impAppr{} by \citet{koppel_determining_2014} in mind, we have specific criteria for good paraphrases.
Our two-step paraphrasing approach builds on the extraction of valid information, since this information is the basis for the subsequent generation approach.
In order to evaluate the quality of the information extracted by the \pextractor{}, we decided to compare the century, genre, and the paraphrase-specific topic to the ground truth available for the \dataBlog{}, \dataGutenberg{} and the \dataStudent{} dataset.
The latter two are compared in terms of semantic similarity, while century is evaluated in terms of percental deviation from the ground truth century.

We found that the instructions for the \pextractor{} have to be positioned after the text to be extracted, due to the inability of the \pextractor{} to return the extracted information in the specified JSON format when the prompt was at the beginning of the input for long texts such as those from the \dataGutenberg{} dataset.
The results are displayed in \autoref{tab:extraction_eval_stats}.
For genre and century extraction, the \dataStudent{} seems to be most difficult for the \pextractor{}.
Apart from length difference between reference and paraphrase, the \pextractor{} performs best on the \dataGutenberg{}.

\begin{table}[h]
\centering
\caption{Degree of adherence to prompt for different datasets.
We selected five documents per dataset and used a \pextractor{} to obtain genre, century and topic of the texts.}
\label{tab:extraction_eval_stats}
\begin{tabular}{lllllllll}
\toprule
 &
  \multicolumn{2}{l}{\textbf{Genre}} &
  \multicolumn{2}{l}{\textbf{Century}} &
  \multicolumn{2}{l}{\textbf{Topic}} &
  \multicolumn{2}{l}{\textbf{Length}} \\
 &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} &
  \textbf{\diameter} &
  \textbf{$\sigma$} \\
  \midrule
\textbf{\dataBlog{}}            & 0.38 & 0.06  & 0.99 & 0.02 & 0.04  & 0.05  & -0.10 & 0.73 \\
\textbf{\dataGutenberg{}}       & 0.58 & 0.14  & 1.00 & 0.04 & 0.3 & 0.15 & -1.00 & 0.00  \\
\textbf{\dataStudent{}} & 0.53 & 0.26 & 0.60 & 0.55 & 0.25 & 0.05  & 0.34 & 0.20 \\
  \bottomrule
\end{tabular}%
\end{table}

It is noteworthy, that opposed to these results, paraphrases from other experiments tended to be way shorter than the reference text.
In multiple cases, the \pgenerator{} returned something along the line of \texttt{I’m sorry, but I can’t help with that}.


% \begin{figure}[htbp]
%     \centering
%     \includesvg[width=\textwidth]{images/paraphrasing/experiments/extraction/radar_extraction_quality_per_dataset.svg}
%     \caption{.}
%     \label{fig:extraction_eval}
% \end{figure}

% results
\section{Exp. 6: Comparing \ac{av} Methods in \acs{llm} author scenarios}

This experiment evaluates the performance of the \impAppr{} in comparison to established \ac{av} methods when each input pair contains at least one \ac{llm} generated text.
Baseline Mirror minds performs reasonable well compared to the other approaches due to its high recall.
You may find the recall curves across different threshold attached in the Appendix in \autoref{sec:app_detection_scenarios}.
Mirror minds' paraphrases are single words, which prompt the discriminator to always predict same-author since any candidate text contains more stylistic similarity than the single-word \imps{}.
We find this approach produces many \acp{fp}.


\paragraph{\ac{llm} detection}


\begin{figure}[b]
  \includesvg[width=\linewidth]{images/AV_comparison/detection_scenarios/precision/student_essays_LLM-Detection_threshold_precisions_curves_all_incl_baselines.svg}
\caption{Precision curves for the class same-author. 
The candidate text is \ac{llm} generated.
In this scenarios, ground truth True corresponds to the disputed text being \ac{llm} generated irrespective of the responsible model.
}
\label{fig:llm_detection_prec}
\end{figure}

\paragraph{\ac{llm} \ac{av}}

\begin{figure}[b]
  \centering
  \includesvg[width=\linewidth]{images/AV_comparison/detection_scenarios/precision/student_essays_LLM-AV_threshold_precisions_curves_all_incl_baselines.svg}
  \caption{Precision curves for the class same-author. 
The candidate text is \ac{llm} generated.
In these scenarios, ground truth True corresponds to the disputed text being generated by the same \ac{llm} as the candidate text.
}
  \label{fig:llm_av_prec}
\end{figure}



\paragraph{\ac{llm}-\ac{llm}}
The overall F1 scores for \ac{av} in \ac{llm}-\ac{llm} are better than in the cross human-\ac{llm} scenario.
On first sight, mirror minds seems to perform best.
This misconception is due to trivial \imps{} leading to the high recall of mirror minds.
Even though fixed, content and text length based \imp{} generation achieve precision between $0.4$ and $0.7$, their F1 score is poor due to low recall values indicating most pairs between attributed to different author classes.
The overall best approach is \ac{naive} \acp{llm}, whose precision is roughly around $0.6$ across all thresholds.


  \begin{figure}[b]
    \centering
    \includesvg[width=\linewidth]{images/AV_comparison/detection_scenarios/f1/student_essays_LLM-AV-(only-LLMs)_threshold_f1s_curves_all_incl_baselines.svg}
    \caption{F1 scores for the class same-author.
  All texts are \ac{llm} generated.
  The overall F1 scores for \ac{av} in \ac{llm}-\ac{llm} are good.
  }
    \label{fig:llm-llm_f1}
  \end{figure}


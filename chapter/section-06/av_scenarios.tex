% results
\section{Exp. 6: Comparing \ac{av} Methods in \acs{llm} author scenarios}

This experiment evaluates the performance of the \impAppr{} in comparison to established \ac{av} methods when each input pair contains at least one \ac{llm} generated text.
Baseline Mirror minds performs reasonable well compared to the other approaches due to its high recall.
You may find the recall curves across different threshold attached in the Appendix in \autoref{sec:app_detection_scenarios}.
Mirror minds' paraphrases are single words, which prompt the discriminator to always predict same-author since any candidate text contains more stylistic similarity than the single-word \imps{}.
We find this approach produces many \acp{fp}.


\paragraph{\ac{llm} detection}

In this \ac{llm} detection scenario, the traditional baselines PPMD, Unmasking and supervised \ac{svm} perform best in terms of precision.
Nevertheless, it becomes clear that all methods can either perform well in terms of precision or in terms of recall, but not both at the same time.
Other than that, we find that all random sampling based \imp{} generation approaches content, text length and fixed perform better than naive \ac{llm} \imps{}, even though they are still not good.
In this scenario, mirror minds and naive \ac{llm} \imps{} are too simple leading to low precision and high recall since many input pairs are predicted to belong to the same author class.
Hence, our approaches perform poorly when asked to detect \acp{llm}.


\begin{figure}[b]
  \includesvg[width=\linewidth]{images/AV_comparison/detection_scenarios/precision/student_essays_LLM-Detection_threshold_precisions_curves_all_incl_baselines.svg}
\caption{Precision curves for the class same-author. 
The candidate text is \ac{llm} generated.
In this scenarios, ground truth True corresponds to the disputed text being \ac{llm} generated irrespective of the responsible model.
}
\label{fig:llm_detection_prec}
\end{figure}

\paragraph{\ac{llm} \ac{av}}

Opposed to first impression, mirror minds and naive \acp{llm} is not suitable for this task since both have very high recall values.
This indicates that the generated \imps{} are too simple and thus, there are many \acp{fp}.
While both PPMD and Unmasking have high precision, they have low recall values resulting in an overall poor F1 performance. 
Both approaches predict different author class for most samples.
The best performing approach is the supervised baseline achieving maximum precision of around $0.7$ for a recall of $\le 0.55$.
Overall, all approaches do no perform well especially in terms of precision (slightly above $0.2$).


\begin{figure}[b]
  \centering
  \includesvg[width=\linewidth]{images/AV_comparison/detection_scenarios/f1/student_essays_LLM-AV_threshold_f1s_curves_all_incl_baselines.svg}
  \caption{F1 scores for the class same-author. 
The candidate text is \ac{llm} generated.
In these scenarios, ground truth True corresponds to the disputed text being generated by the same \ac{llm} as the candidate text.
}
  \label{fig:llm_av_prec}
\end{figure}



\paragraph{\ac{llm}-\ac{llm}}
The overall F1 scores for \ac{av} in \ac{llm}-\ac{llm} are better than in the cross human-\ac{llm} scenario.
This indicates that the presence of human authored texts complicates the \ac{av} task.
On first sight, mirror minds seems to perform best.
The misconception is due to trivial \imps{} leading to the high recall of mirror minds.
The Unmasking baseline seems to exhibit the same recall behaviour as mirror minds for a threshold smaller than $0.4$.
Even though fixed, content and text length based \imp{} generation achieve precision between $0.4$ and $0.7$, their F1 score is poor due to low recall values indicating most pairs between attributed to different author classes.
Both PPMD and unsupervised min-max have high precision and low recall, leading to poor F1 scores.
The overall best approach is \ac{naive} \acp{llm}, whose precision is roughly around $0.6$ across all thresholds.
\ac{naive} \acp{llm} decrease in recall indicates that with the increase of the threshold the same number \acp{tp} and \acp{fp} is discarded from the set of samples predicted with a positive label.
This means that the generated \imps{} do not distinguish same author from different author pairs.


  \begin{figure}[b]
    \centering
    \includesvg[width=\linewidth]{images/AV_comparison/detection_scenarios/f1/student_essays_LLM-AV-(only-LLMs)_threshold_f1s_curves_all_incl_baselines.svg}
    \caption{F1 scores for the class same-author.
  All texts are \ac{llm} generated.
  The overall F1 scores for \ac{av} in \ac{llm}-\ac{llm} are good.
  }
    \label{fig:llm-llm_f1}
  \end{figure}


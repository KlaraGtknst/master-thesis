% results
\section{Exp. 6: Comparing \ac{av} Methods in \acs{llm} author scenarios}

This experiment evaluates the performance of the \impAppr{} against established \ac{av} methods when each input pair contains at least one \ac{llm}-generated text.  
The baseline Mirror Minds performs comparatively well due to its consistently high recall.  
The recall curves across different thresholds and scenarios can be found in the Appendix (\autoref{sec:app_detection_scenarios}).  

However, Mirror Minds' strength is a misconception due to its oversimplified paraphrases, which consist of single words.  
This triviality biases the discriminator toward predicting \textit{same-author}, since any candidate text necessarily contains more stylistic information than the single-word \imps{}.  
As a result, this method produces a large number of \acp{fp}.


\paragraph{\ac{llm} detection}

In the \ac{llm} detection setting, the traditional baselines PPMD, Unmasking, and the supervised \ac{svm} perform best in terms of precision.  
Nevertheless, it becomes evident that no method achieves a favourable balance between high precision or high recall since none can attain both simultaneously.  

Among the \imp{}-based approaches, random sampling variants (content, text length, and fixed) outperform naive \ac{llm}-based \imps{}, although their absolute performance remains unsatisfactory.  
Mirror Minds and naive \ac{llm} \imps{} are overly simplistic, leading to consistently low precision and inflated recall, since the majority of input pairs are incorrectly classified as same-author.  
Consequently, these approaches are particularly ill-suited for detecting \acp{llm}.

\begin{figure}[b]
  \includesvg[width=\linewidth]{images/AV_comparison/detection_scenarios/precision/student_essays_LLM-Detection_threshold_precisions_curves_all_incl_baselines.svg}
\caption{Precision curves for the class same-author. 
The candidate text is \ac{llm} generated.
In this scenarios, ground truth True corresponds to the disputed text being \ac{llm} generated irrespective of the responsible model.
}
\label{fig:llm_detection_prec}
\end{figure}

\paragraph{\ac{llm} \ac{av}}

Contrary to initial intuition, neither Mirror Minds nor naive \ac{llm} approaches are suitable for this task, as their high recall results in excessive \acp{fp}.  
Although PPMD and Unmasking achieve relatively high precision, their recall is consistently low, yielding poor overall F1 performance.  
Both methods predominantly predict different-author, thereby underestimating same-author cases.  

The best-performing baseline is the supervised model, which reaches a maximum precision of approximately $0.7$ for recall $\leq 0.55$.  
However, overall performance remains unsatisfactory across all methods, particularly in terms of precision, which hovers slightly above $0.2$.


\begin{figure}[b]
  \centering
  \includesvg[width=\linewidth]{images/AV_comparison/detection_scenarios/f1/student_essays_LLM-AV_threshold_f1s_curves_all_incl_baselines.svg}
  \caption{F1 scores for the class same-author. 
The candidate text is \ac{llm} generated.
In these scenarios, ground truth True corresponds to the disputed text being generated by the same \ac{llm} as the candidate text.
}
  \label{fig:llm_av_prec}
\end{figure}



\paragraph{\ac{llm}-\ac{llm} \ac{av}}

Overall F1 scores for \ac{av} in the \ac{llm}-\ac{llm} scenario are higher than in the humanâ€“\ac{llm} setting, suggesting that the presence of human-authored texts complicates attribution.  

At first glance, Mirror Minds appears to perform best. 
However, this impression is misleading.  
Its trivial \imps{} inflate recall without offering genuine discriminative power.  
Interestingly, the Unmasking baseline exhibits similar recall behaviour for thresholds below $0.4$.  

Fixed, content-based, and text-length-driven \imp{} generation methods achieve precision between $0.4$ and $0.7$, yet their F1 scores remain low due to poor recall, i.e. most pairs are incorrectly assigned to the different-author class.  
Similarly, PPMD and the unsupervised min-max approach yield high precision but equally low recall, again resulting in poor F1 values.  

The overall best-performing approach is naive \ac{llm} generation, which maintains a precision of approximately $0.6$ across thresholds.  
However, with increasing threshold its recall decreases monotonically indicating both \acp{tp} and \acp{fp} are discarded in roughly equal measure, implying that the generated \imps{} fail to meaningfully distinguish between same- and different-author pairs.


  \begin{figure}[b]
    \centering
    \includesvg[width=\linewidth]{images/AV_comparison/detection_scenarios/f1/student_essays_LLM-AV-(only-LLMs)_threshold_f1s_curves_all_incl_baselines.svg}
    \caption{F1 scores for the class same-author.
  All texts are \ac{llm} generated.
  The overall F1 scores for \ac{av} in \ac{llm}-\ac{llm} are good.
  }
    \label{fig:llm-llm_f1}
  \end{figure}


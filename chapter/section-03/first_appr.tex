% \section{First Approaches}

% Approaches of the \nth{19} century were limited to the analysis of word lengths.
The \nth{20} century brought new measures, such as Zipf's law (1932) and Yule's Characteristic (1944).
Zipf's law describes the relationship between rank and frequency of words.
Yule's Characteristic was proposed by George Yule as a first attempt to measure vocabulary richness via word frequencies~\citep{neal_surveying_2018,stamatatos_survey_2009}.

A massive milestone was proposed in 1964 by Mosteller and Wallace with their computer-assisted stylometry on "The Federalist Papers" from the \nth{18} century.
Their discriminations were built on a Bayesian statistical analysis of the frequencies of a small set of common words such as "and" or "to".
This approach instigated stylometry, the research of finding features that quantify writing style.
In the peak years of stylometry between 1964 and the late 1990s, around \num{1000} different measurements were proposed.
Such measures include sentence or words lengths, word or char frequencies, and vocabulary richness.
The main shortcomings of stylometry in \nth{20} century included the lack of method comparison due to the lack of suitable benchmark data and the mostly subjective evaluation of individual measures via visual inspection of scatterplots~\citep{stamatatos_survey_2009}.

The late 1990s initiated a new era of \ai{}.
The presence of more electronic texts and benchmark data marked significant progress towards objective evaluation and comparison of methods.
Moreover, for the first time, the focus shifted from answering disputed literary questions to solving real-world problems and creating practical applications~\citep{stamatatos_survey_2009}.


% % potential introduction to style chapter
% \citet{bischoff_importance_2020} assume that each author has a unique style, unconsciously encoded in their writing.
% This style depends on the author's personal traits, customs an author adopts due to genre, register, type, and topic.
% These concepts are too vague and ill-separable to be efficiently operationalized.
% The goal is to discover a set of style markers more likely to be determined by the author's personality than by domain customs.

% They claim that features frequent function words and word length have a high correlation with topic. \todo{????}
% Hence, traditional author style models are highly susceptible to learning domain-specific features and 
% prone to pick up domain artefacts unless domains are controlled, 
% which imposes severe practical limitations.

% % char 3-gram
% \citet{bischoff_importance_2020} analyse the robustness of character trigrams as a feature for \ac{aa}.
% They find that the character trigrams feature set is not robust in a cross-topic setting, but across two genres.


% \section{Attribution Methods}
% \label{sec:attribution_methods}

% For a comparison (pro/contra) of profile-based and instance-based methods, see \citet{stamatatos_survey_2009}.

% % profile-based
% Methods belonging to the profile-based category concatenate all the available training texts per author in one big file 
% and extract a cumulative representation of that author's style from this concatenated text. % tyo_state_2022: "representation of all author texts"
% Hence, methods in this category are better if only short texts are available for training.
% The difference of texts written by the same author are disregarded \citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014,neal_surveying_2018}.
% The unseen text is compared to each author file and the most similar one based on a distance measure is selected as the predicted author:
% $$ author(x) = argmin_{a \in A} d(x, x_a) $$
% where $x$ is the text to be classified, $A$ is the set of authors, and $d(x, x_a)$ is a distance measure 
% between the text and the author file $x_a$ \citep{stamatatos_survey_2009}.
% % probabilistic models
% Probabilistic models are a special case of profile-based methods.
% They attempt to maximize the probability $P(x|a)$ of the text $x$ belonging to candidate author $a$ \citep{stamatatos_survey_2009,neal_surveying_2018}.
% The attribution model seeks the author that maximizes the similarity metric: 
% $$ author(x) = argmax_{a \in A} \frac{P(x|a)}{P(x|\overline{a})} $$
% where the conditional probabilities are estimated by $x_a$ for author $a$ and the rest of the texts, respectively \citep{stamatatos_survey_2009}.
% Naive Bayes is a variant of this probabilistic  classifier \citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014,neal_surveying_2018}.
% \citet{elmanarelbouanani_authorship_2014} describe a Naive Bayes classifier that takes a feature vector of 365 normalized function word frequencies.
% % compression models, e.g. RAR or GZIP (more info Paper)
% Compression models are based on the idea that the text of one author can be compressed more efficiently than the text of multiple authors.
% The new text is concatenated with the author profile and then compressed.
% The differences between the compressed concatenation with the unseen text and compressed author profiles without the unseen text are computed 
% \citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014,neal_surveying_2018}.
% The author profile with the lowest difference is selected as the predicted author \citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014}.
% Tested compression algorithms include RAR, LZW, GZIP, BZIP2 and 7ZIP. 
% RAR is the most accurate one \citep{elmanarelbouanani_authorship_2014}.
% \citet{elmanarelbouanani_authorship_2014} include the Normalized Compressor Distance (NCD) as a distance measure for compression-based methods. % Chap. 4.2
% \citet{stamatatos_survey_2009} claim that probabilistic approaches are faster in comparison to compression models.
% \citet{neal_surveying_2018} state that LZ77 is a lossless data compression algorithm that is used to compress data by detecting duplicates.
% % Common n-grams
% For the \ac{cng} method, the author profile is composed of the $L$ most common $n$-grams of the training texts.
% The similarity between to texts is estimated by a distance measure based on relative frequencies of the $n$-grams.
% \ac{cng} favours author profiles shorter than $L$ or in imbalanced cases.
% % Simplified Profile Intersection
% \ac{spi} is a simpler distance measure to mitigate the disadvantages of \ac{cng}.
% It is based on the idea that the more common $n$-grams two texts share, the more similar they are.
% It counts the number of common $n$-grams between the two texts and disregards the rest.
% % most similar in terms of distance
% \citep{koppel_authorship_2011} describe the similarity-based paradigm as a profile-based approach 
% where the unseen text is attributed to the author whose profile is closest in terms of a distance metric.
% As a distance metric, \citet{koppel_authorship_2011} suggest the cosine distance in a vector space 
% defined by the space-free character 4-gram frequencies.
% % similarity of vocabularies
% \citet{neal_surveying_2018} define intertextual distances as measures of the similarity between the vocabularies of two texts.
% Some of the most common measures are the following:
% \begin{itemize}
%     \item Delta measures the difference in $z$-scores, or standard scores, of the relative frequencies of the most frequent words in texts, which he termed \textit{Delta}. 
%     Delta has proven one of the most robust intertextual distance measures by computing 
%     $\frac{1}{n}\sum_{i=1}^{n} \left| z(f_i(D)) - z(f_i(D')) \right|$ between two texts $D$ and $D'$.
%     \item Chi-Square Distance $\chi^2$: $\chi^2=\sum_{k=1}^{n}\frac{(O_k-E_k)^2}{E_k}$ is a non-parametric goodness-of-fit statistical measure for determining
%     if a set of frequencies were drawn from the same population.
%     $O$ is the observed frequency and $E$ is the expected frequency.
%     In intertextual distance, the frequencies of lexical features are used, where the population is a collection of candidate author samples.
%     A lower $\chi^2$ value indicates that a sample was drawn from a particular population.
%     \item Kullback-Leibler Divergence $D_{KL}(P||Q)=\sum_{i}P(i) log \frac{P(i)}{Q(i)}$ 
%     is a measure of how one discrete probability distribution $Q$ diverges from a second expected discrete probability distribution $P$.
%     \item Stamatatos Distance is measure based on character $n$-grams.
%     An author profile $P$ is a pair ($n$-gram, normalized frequency) of the $L$ most frequent $n$-grams in a text sample.
%     The first metric measures the distance between an unknown text profile and candidate author profile: 
%     $d_1(P(x),P(T_a))=\sum_{g \in P(x)} (\frac{2(f_x(g)-f_{T_a}(g))}{f_x(g)+ f_{T_a}(g)})^2$, 
%     where $P(x)$ is the profile of the unknown text, $P(T_a)$ is the profile of the text of the candidate author $a$, 
%     $f_x(g)$ is the frequency of $n$-gram $g$ in $P(x)$, and $f_{T_a}(g)$ is the frequency of $n$-gram $g$ in the candidate author text.
%     The second metric concatenates all training samples as a normalization step:
%     $d_2(P(x),P(T_a),P(N)))=\sum_{g \in P(x)} (\frac{2(f_x(g)-f_{T_a}(g))}{f_x(g)+ f_{T_a}(g)})^2 \cdot (\frac{2(f_x(g)-f_N(g))}{f_x(g)+ f_N(g)})^2$, 
%     where $N$ is the concatenated text.

% \end{itemize}

% % instance-based
% The family of instance-based methods, on the other hand, require multiple training text samples per author. 
% Each sample is a separate instance of authorial style \citep{stamatatos_survey_2009,altakrori_topic_2021,elmanarelbouanani_authorship_2014,neal_surveying_2018}.
% If only one training sample is available, the method segments the sample into multiple parts, probably of equal length.
% \citet{stamatatos_survey_2009} state that samples of variable length should be normalized and 
% shorter samples should be discarded.
% % vector space models
% Each text is represented as a vector in a multivariate space.
% \citet{stamatatos_survey_2009} list a number of statistical and machine learning algorithms as classifiers.
% They stress that \acp{svm} are extremely popular in high-dimensional spaces.
% However, they also state that class imbalance is a problem 
% which should be overcome by segmentation, filtering or oversampling.
% \citet{koppel_authorship_2011,koppel_determining_2014} denote this approach belonging to the machine learning paradigm.
% \citet{koppel_determining_2014} claim that machine learning methods are not suitable for large number of candidate authors 
% since they are designed for small number of classes. 
% They also state that the introduction of ensembles of multiple binary classifiers is not a solution to this problem 
% due to the ambiguity of multiple positive answers.
% % similarity-based measures
% Similarity-based measures are used to measure the distance between the unseen text and all other training texts.
% The most likely author is estimated based on a $k$-nearest-neighbour algorithm.
% If $k=1$, the approaches are sensitive to noise.
% However, for $k>1$ and majority vote or weighted vote schemes, the methods are more robust.
% % others in Paper
% Compression-based models can also be considered similarity-based measures which are slow 
% since the compression algorithm is called for each training text \citep{stamatatos_survey_2009,neal_surveying_2018}.
% % Meta-learning models
% Existing classification algorithms can be used as meta-learning models.
% Unmasking is a meta-learning approach which is based on the idea that
% omitting discriminant features and the consequent drop in accuracy of the classifier 
% can be used for inference of the author of the unseen text.
% An unseen text is chunked, such that multiple examples either all belong to the author or to a different author, 
% are generated \citep{koppel_authorship_2004}.
% This gives rise to the idea of two examples sets which are either generated by a single generating process (author) 
% or by two different processes \citep{koppel_authorship_2004}.
% For each unseen text, a \ac{svm} is built to discriminate it, i.e. its segments, 
% from the training texts of each candidate author.
% Hence, for each candidate author, a \ac{svm} is trained.
% After a few iterations, the classifier is no longer able to discriminate between the unseen text and 
% the training texts of the true author, i.e. low accuracy \citep{stamatatos_survey_2009,koppel_authorship_2004}.


% % hybrid
% Hybrid approaches include a combination of profile- and instance based aspects.
% Text samples are represented individually (i.e. instance-based) and 
% the profile vector is built via computing the feature-wise average over the author's sample vectors.
% The similarity between the unseen text and the author profile is used to predict the true author \citep{stamatatos_survey_2009}.

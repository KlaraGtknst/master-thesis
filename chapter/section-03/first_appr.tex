% \section{First Approaches}

% Approaches of the \nth{19} century were limited to the analysis of word lengths.
In the early \nth{20}, statistical measures such as Zipf's law (1932) and Yule's characteristic (1944) were introduced. 
These measures captured word frequency distributions and vocabulary richness, laying the foundation for more formalized analyses of writing style~\citep{neal_surveying_2018,stamatatos_survey_2009}.

A decisive milestone came with the work of Mosteller and Wallace (1964), who applied Bayesian statistical analysis to the "The Federalist Papers".
Their computer-assisted approach, based on the frequencies of a subset of common function words (e.g., "and", "to"), demonstrated the potential of rigorous quantitative methods for \ac{aa}. 
This work is often regarded as the beginning of modern stylometry, the systematic study of quantifiable features of writing style.

Between the 1960s and the late 1990s, stylometric research flourished.
Around \num{1000} features were proposed, including sentence and word lengths, character and word frequencies, and vocabulary richness. 
However, the field faced two major limitations. 
The absence of benchmark datasets prevented systematic comparison of methods, and evaluation often relied on subjective inspection of visualizations (e.g., scatterplots) rather than standardized metrics~\citep{stamatatos_survey_2009}.

The late 1990s marked the transition into a new era. 
The growing availability of electronic text collections enabled the construction of benchmark datasets and more thorough evaluation. 
At the same time, \ac{ml} algorithms facilitated more expressive text representations, moving beyond simple frequency counts toward feature-rich models. 
As a result, the scope of \ac{av} expanded from resolving historical literary questions to addressing practical, real-world applications such as digital forensics~\citep{stamatatos_survey_2009}. 
Nevertheless, as \citet{abbasi_writeprints_2008} note, even by 2008 stylometric methods struggled with scalability across large author sets, diverse genres, and open-world scenarios.

\acp{nn} began to replace simple classifiers in \ac{av} around 2016. 
However, their leading role was soon overtaken by transformer-based models such as BERT, RoBERTa, and T5. 
Despite these advances, traditional stylometric features remained relevant and continued to play an important role, particularly when integrated into ensemble approaches. 
Since late 2022, the advent of \acp{llm} has further reshaped the field, rapidly establishing them as a popular research tool~\citep{schmidt_llm_av_latin_24}.

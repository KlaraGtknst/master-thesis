\section{Canonical Methods}

The following sections introduce \ac{sota} approaches to \ac{av}.
We begin with compression-based methods, continue with traditional and generalized Unmasking and end with the traditional \impAppr{}.

\subsubsection{Compression-based}
% compression models, e.g. RAR or GZIP 
This category of \ac{aa} approaches is based on general-purpose compression models such as RAR or PPMD. %(i.e. a variant of \ac{ppm}~\citep{tyo_state_2022}), LZW, GZIP, BZIP2 and 7ZIP.
Such models capture textual characteristics by exploiting repeated character sequences~\citep{stamatatos_survey_2009,neal_surveying_2018}. 
Natural language, in particular, allows for high compression ratios due to its strong predictability (English has an entropy of at most 1.75 bits per character). 
For example, PPMD employs finite-order Markov language models for compression, which are highly effective in predicting character sequences in natural text but are also sensitive to increased entropy caused by text obfuscation~\citep{bevendorff_divergence_based_2020}.
Accordingly, compression-based \ac{aa} methods are considered character-based approaches.

They are further classified as profile-based methods. In this framework, an author profile is first constructed for each candidate author by concatenating all texts attributed to them and then compressing the resulting sequence. 
The disputed text is subsequently concatenated with each author profile and compressed as well. 
The difference in compression size between (i) the concatenated profile with the disputed text and (ii) the profile alone is then calculated~\citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014,neal_surveying_2018}. 
The author whose profile yields the smallest difference is selected as the most likely author~\citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014}.

The rationale behind this approach is that texts written by the same author can typically be compressed more efficiently than texts produced by different authors~\citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014}.

% RAR is the most accurate one \citep{elmanarelbouanani_authorship_2014}.
% \citet{elmanarelbouanani_authorship_2014} include the Normalized Compressor Distance (NCD) as a distance measure for compression-based methods. % Chap. 4.2
% \citet{stamatatos_survey_2009} claim that probabilistic approaches are faster in comparison to compression models.
% \citet{neal_surveying_2018} state that LZ77 is a lossless data compression algorithm that is used to compress data by detecting duplicates.

% Compression-based models can also be considered similarity-based measures which are slow 
% since the compression algorithm is called for each training text \citep{stamatatos_survey_2009,neal_surveying_2018}.



\subsubsection{Unmasking Method}
\label{subsec:unmasking}

The meta-learning approach Unmasking algorithm was first proposed by \citet{koppel_authorship_2004}.
Meta learning is a technique where the system learns to learn based on learning successes and failures.
It is based on the idea that omitting discriminant features and the consequent drop in accuracy of the classifier can be used for inference of the author of the unseen text.
For Unmasking, (1) an unseen text is chunked, such that the non-overlapping chunks compose multiple samples belonging either to the author or to a different author.
Next, one \ac{svm} is trained for each candidate author to discriminate the disputed texts' chunks from the candidate author's texts.
The \acp{svm}' features are usually frequencies over the $n=250$ highest average frequency words.
(2) The 10-fold cross validation accuracy for the trained model are obtained.
(3) For the next iteration, omit the most discriminating features among those left.
(4) Repeat steps (3) and (4).
(5) Another linear \ac{svm} classifier is trained on the accuracy curve, its central-difference gradients (first- and second order), 
and its gradients sorted by magnitude.
This classifier is used to predict the whether the texts originate from the same author.

After a few iterations, the classifier is no longer able to discriminate between the unseen text and the texts of the true author~\citep{stein_intrinsic_2011,tyo_state_2022,bevendorff_divergence_based_2020,koppel_authorship_2004,stamatatos_survey_2009} 
Two texts are probably written by different authors if the differences between are robust to changes in the underlying feature set used to represent the documents.

To operationalize this idea, differences are measured using classification via cross-validation accuracy~\citep{koppel_authorship_2011,bevendorff_generalizing_2019,bevendorff_divergence_based_2020,potthast_stylometric_2018,koppel_authorship_2004}, 
creating a performance degradation curve~\citep{tyo_state_2022,koppel_authorship_2004}.
An \ac{svm} is trained to classify the degradation curve to determine whether two text originated from the same author~\citep{tyo_state_2022,bevendorff_generalizing_2019,koppel_authorship_2004}.
Steep decrease in the curve indicates that the two texts are similar, and thus, written by the same authors~\citep{potthast_stylometric_2018,koppel_authorship_2004}.
% Provided that the unseen text is very large, this method can handle small open candidate sets \citep{koppel_authorship_2011}.
% koppel_determining_2014, pg. 1 + bevendorff_generalizing_2019 chap. 3.1 incl. algo: based on text chunks of length >= 500 words each
% \citet{koppel_determining_2014,bevendorff_generalizing_2019} claim that effective unmasking requires input documents to be large 
% (i.e. > 10000 words~\citep{koppel_determining_2014}, book-length~\citep{bevendorff_generalizing_2019}, 
% $\geq$ 5000 words (500 words per chunk) \citep{bevendorff_divergence_based_2020}).
% Otherwise the training set becomes too sparse and no descriptive curves can be generated 
% \citep{bevendorff_generalizing_2019,bevendorff_divergence_based_2020}.

% generalized unmasking
\citet{bevendorff_generalizing_2019,bevendorff_divergence_based_2020} propose generalized Unmasking as displayed in \autoref{fig:generalized_unmasking}.
Opposed to the original method, chunks by oversampling words in a bootstrap aggregating manner. 
Each text is a pool of words, from which words are sampled without replacement.
The pool is replenished if it is exhausted before the chunk has sufficiently many words.
Since the random sampling of unmasking features introduces variance, unmasking is performed multiple times and the curves are averaged.
The algorithm is displayed in \autoref{alg:generalized_unmasking}.
The content of the while loop is, except the number of removed features (\citep{koppel_authorship_2004}: 6 total), similar to the original unmasking algorithm \citep{koppel_authorship_2004}.

\begin{algorithm}
    \caption{Generalized Unmasking Algorithm \citep{bevendorff_generalizing_2019,bevendorff_divergence_based_2020}}
    \label{alg:generalized_unmasking}
    \begin{algorithmic}[1]
    \Procedure{Unmasking}{$A$, $B$}
        \Comment{$A$, $B$: input documents}
    
        \State $\mathcal{C}_A \gets \text{RandomChunks}(A, 30, 700)$ \Comment{30 chunks, 700 words each}
        \State $\mathcal{C}_B \gets \text{RandomChunks}(B, 30, 700)$
        \State $\mathcal{F} \gets \text{TopFreqWords}(A, B, 250)$
        \State $\mathcal{C} \gets \mathcal{C}_A \cup \mathcal{C}_B$

        
        \While{$|\mathcal{F}| \geq 0$}
        \State $a \gets \text{CVAcc}(\mathcal{C}_A, \mathcal{C}_B, \mathcal{F}, linSVM)$ \Comment{Append $10$-fold cross-validation accuracy}
        \State $\mathcal{F} \gets \mathcal{F} \setminus \mathcal{F}_{\text{top}}^{\pm}$ \Comment{Remove top $5$ most significant positive and negative features}
    
        \EndWhile
    
        \State \Return List of recorded accuracies $a$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}

% hyperparameters
% The most important hyperparameters are the number of chunks, the number of words per chunk, the size of feature vectors, 
% the number of word removals per round, and the number of averaged unmasking runs.
% More chunks result in generally shallower curves while shorter features vectors or more removals produce steep curves.
% Ideally, curves are not too steep and granular enough to allow distinguishing between different same and different author pairs.
% \citet{bevendorff_bias_2019} recommend 25 to 50 chunks, vector sizes of 250 to 400 features, not fewer than 5, yet not more than 20 removals per round, 
% between 500 and 700 words per chunk and about 10 runs to average for a curve.
% They increase the minimal distance between the \ac{svm} hyperplane and the decision boundary, i.e. their confidence parameter $c$, to increase precision.
% In a medium- to high-assurance scenario (where \acp{fp} should be avoided, but are not entirely critical), they recommend $c \geq 0.6$.
% If \acp{fp} should be avoided at all costs, they recommend $c \geq 0.7$.
% \citet{bevendorff_bias_2019} claim that, for this approach, hyperparameter tuning is simpler than for black box approaches.

% % metric results
% \citet{bevendorff_bias_2019} report that the generalized unmasking approach heavily prioritizes precision 
% opposed to compression-based approaches that balance precision and recall.
    






\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/unmasking/generalized_unmasking.svg}
    \caption{Workflow of Generalized Unmasking~\citep{bevendorff_generalizing_2019}: (1) Create chunks by oversampling words of disputed and candidate texts and represent them using word frequencies. (2) Obtain \ac{svm} accuracy. (3) Eliminate most discriminative features. (4) Repeat from (3)-(4).}
    \label{fig:generalized_unmasking}
\end{figure}



  
\subsubsection{\imp{} Method}
\label{sec:impostor_method_theory}


\begin{definition}
    [\imp{} method]
    This method extends the ngram-unmasking method, i.e. iteratively omitting most influencely features (repeated feature subsampling \citep{koppel_determining_2014})
    from a trained classifier and classifying the accuracy drop.
    It takes score of how often an author is predicted after each feature-elimination step.
    The final prediction is made based on this score \citep{tyo_state_2022}.
\end{definition}


\begin{definition}
    [Hard Negative Mining]
    This method updates the model during training only with the most difficult examples in each batch.
    In the \ac{aa} context, difficult is defined as the most similar two texts from different authors, 
    which makes the decision the most difficult.
    \citet{tyo_state_2022} claim that the \ac{av} setting is strictly easier since 
    it most compare to only a single text.
    Due to the fact, that the most difficult example is model-dependent, \ac{av} problems can be made harder 
    but they can not exist of exactly the hardest negatives.
\end{definition}


\begin{definition}
    [Domain]
    The domain include topic, genre, register, idiolect, time period etc. \citep{bischoff_importance_2020}.
\end{definition}
  
\begin{definition}
    [Domain variables]
    These include topic, genre and language \citep{bischoff_importance_2020}.
\end{definition}

\begin{definition}
    [within-domain]
    Experiments with P=Q.
    Hence, it is necessary to ensure all texts are mutually from the same domain \citep{bischoff_importance_2020}.
    \begin{table}[tbp]
        \centering
        \caption{Typical scheme $S_1$ for \ac{aa} problem instances, where A, B, are authors and P, Q domains and 
        the vertical mapping denotes which author has written in which domain. 
        For training, texts from A and B take turn; for testing, previously unseen texts from A and B are used \citep{bischoff_importance_2020}.}
        \label{tab:within_domain_aa}
        \begin{tabular}{|l|ll|ll|}
        \hline
        \textbf{Scheme $S_1$} & \multicolumn{2}{l|}{\textbf{training}} & \multicolumn{2}{l|}{\textbf{testing}} \\ \hline
        \textbf{authors} & \multicolumn{1}{l|}{A} & B & \multicolumn{1}{l|}{A} & B \\ \hline
        \textbf{domains} & \multicolumn{1}{l|}{P} & Q & \multicolumn{1}{l|}{P} & Q \\ \hline
        \end{tabular}%
    \end{table}
\end{definition}


The \impAppr{} leverages random projections to lower dimensional spaces (i.e. random set of features set to zero is a projection).
\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/imposter/imposter.svg}
    \caption{\imp{}.}
    \label{fig:impostor}
\end{figure}

% AV -> open-set
\ac{av} is an open-set problem, meaning that the author of an anonymous document 
may or may be not be part of the set of candidate authors.

% AA -> closed-set
\ac{aa} is a closed-set problem, meaning that the author of an anonymous document
is part of the set of candidate authors.
For each candidate author, writing samples are available.
The task is to determine the author of the anonymous document from the set of candidate authors.

% reduction: closed-set AA -> open-set AV
\citet{koppel_determining_2014} state that all closed-set \ac{aa} problems are reducible to the \ac{av} problem.
The reverse is not true.
To reduce the \ac{aa} problem to the \ac{av} problem, we solve a \ac{av} problem, i.e. if text was written by a candidate author, 
for each of the respective candidates.
Ideally, we receive one positive answer for the correct candidate author and negative answers for all other candidates.

% complexity
\citet{koppel_determining_2014} explain that the \ac{av} problem is more complex than the \ac{aa} problem.
They claim that the ability to solve a closed-set \ac{aa} problem does not imply the ability to solve an open-set \ac{av} problem.

% open-set identification/ AA = many candidates problem
\citet{koppel_determining_2014} define the many-candidates problem, or the so-called open-set identification problem:
Given a large set of candidate authors, determine which, if any, of them wrote a given anonymous document.
According to \citet{koppel_determining_2014}, the many-candidates problem can be solved reasonably well: \autoref{lst:many_candidate_algo}.
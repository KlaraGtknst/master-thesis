\section{Canonical Methods}

\begin{table}[tbp]
    \centering
    \caption{Building blocks for \ac{av} from \citep{stein_intrinsic_2011}.}
    \label{tab:authorship_verification_blocks}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|lll|l|}
    \hline
    \rowcolor[HTML]{EFEFEF} 
    Pre-analysis & \multicolumn{3}{l|}{\cellcolor[HTML]{EFEFEF}Modeling and classifier methods} & Post-processing \\ \hline
    \rowcolor[HTML]{EFEFEF} 
    Impurity assessment & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}Decomposition strategy} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}Style model construction} & Outlier identification & Outlier post-processing \\ \hline
    Document length analysis & \multicolumn{1}{l|}{Uniform length} & \multicolumn{1}{l|}{Lexical character features} & One-class density estimation & Heuristric voting \\
    Genre Analysis & \multicolumn{1}{l|}{Structural boundaries} & \multicolumn{1}{l|}{Lexical word features} & One-class boundary estimation & Citation analysis \\
    Analysis of issuing institution & \multicolumn{1}{l|}{Text element boundaries} & \multicolumn{1}{l|}{Syntactical features} & One-class reconstruction & Human inspection \\
     & \multicolumn{1}{l|}{Topical boundaries} & \multicolumn{1}{l|}{Structural features} & Two-class discrimination & Unmasking \\
     & \multicolumn{1}{l|}{Stylistic boundaries} & \multicolumn{1}{l|}{Language modeling} &  & Qsum \\
     & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  & Batch means
    \end{tabular}%
    }
\end{table}
Post-processing to avoid false positives, c.f. \citet{stein_intrinsic_2011} for approaches.

\subsubsection{Compression-based???}
\textcolor{red}{TODO}


\subsubsection{Unmasking Method}
\textcolor{red}{TODO}


\begin{definition}
    [n-gram]
    $n$ contiguous words also known as word collocations. \todo{cite, reference?\citep{koppel_authorship_2011}?}
    n-grams are no stylometric features \citep{altakrori_topic_2021}.
    % Quelle Martin Potthast GesprÃ¤ch 19.05.2025:
    Tri-grams are commonly used in stylistic analysis, due to their ability to capture inflections, % Flexion/ Beugung in Deutsch
    morphemes, %  smallest meaningful constituents within a linguistic expression and particularly within a word
    and other syntactic structures for Germanic languages.
    Character-level $n$-gram features capture the frequency of $n$ consecutive characters in a text \citep{neal_surveying_2018}.
    The optimal $n$ is language dependent \citep{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Space free n-gram]
    Removing spaces from the $n$-gram reduces the number of $n$-grams.
    \citet{koppel_authorship_2011} use these definitions:
    \begin{enumerate}
        \item a string of $n$ characters that not include spaces
        \item a string of less than $n$ characters that is surrounded by spaces
    \end{enumerate}
\end{definition}

\begin{definition}
    [Meta learning]
    \todo{Based on learning successes and failures, the system learns to learn. }
    Approaches include:
    \begin{itemize}
        \item Unmasking: Measurement of reconstruction errors starting from a good reconstruction and iteratively impairing the reconstruction. % example on page 9 of Benno's paper
        \item Qsum heuristic: Compares the growth rates of two cumulative sums over a sequence of sentences. The sums are calculated via the deviations from the mean sentence length and the deviations of the function words.
        \item Batch means: \todo{For a series of values the variance development of the sample mean is measured while the sample size is successively increased.}
    \end{itemize}
\end{definition}

\begin{definition}
    [Unmasking]
    The idea of this meta learning approach is that with progressively omitting more and more frequent words/ 
    most discriminating features, 
    topic specific words are excluded and thus, leaving only writing style specific words \citep{stein_intrinsic_2011}.
    After several iterations, remaining features which are not powerful enough to discriminate two documents indicate that 
    these documents originate from the same author \citep{stein_intrinsic_2011,tyo_state_2022,bevendorff_divergence_based_2020,koppel_authorship_2004}.
    In other words: 
    Two texts are probably written by different authors if the differences between are robust to changes in the underlying feature set used to represent the documents.
    Differences can be measured using instance-based (meta) classification via cross-validation accuracy 
    \citep{koppel_authorship_2011,bevendorff_generalizing_2019,bevendorff_divergence_based_2020,potthast_stylometric_2018,koppel_authorship_2004}, 
    creating a performance degradation curve \citep{tyo_state_2022,koppel_authorship_2004}.
    An \ac{svm} is trained to classify the degradation curve to determine whether two text originated from the same author 
    \citep{tyo_state_2022,bevendorff_generalizing_2019,koppel_authorship_2004}.
    Cf. \citep{bevendorff_divergence_based_2020} Chapt. 2 for a detailed algorithm.
    Steep decrease in the curve indicates that the two texts are similar, and thus, 
    written by the same authors \citep{potthast_stylometric_2018,koppel_authorship_2004}.
    Provided that the unseen text is very large, this method can handle small open candidate sets \citep{koppel_authorship_2011}.
    % koppel_determining_2014, pg. 1 + bevendorff_generalizing_2019 chap. 3.1 incl. algo: based on text chunks of length >= 500 words each
    \citet{koppel_determining_2014,bevendorff_generalizing_2019} claim that effective unmasking requires input documents to be large 
    (i.e. > 10000 words~\citep{koppel_determining_2014}, book-length~\citep{bevendorff_generalizing_2019}, 
    $\geq$ 5000 words (500 words per chunk) \citep{bevendorff_divergence_based_2020}).
    Otherwise the training set becomes too sparse and no descriptive curves can be generated 
    \citep{bevendorff_generalizing_2019,bevendorff_divergence_based_2020}.
\end{definition}



\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/unmasking/unmasking.svg}
    \caption{Unmasking.}
    \label{fig:unmasking}
\end{figure}
Our method is an extension of the original \impAppr{} by \citet{koppel_determining_2014}.
By varying the seed and temperature, we can generate as many texts as we want.
  
\subsubsection{\imp{} Method}
\label{sec:impostor_method_theory}


\begin{definition}
    [\imp{} method]
    This method extends the ngram-unmasking method, i.e. iteratively omitting most influencely features (repeated feature subsampling \citep{koppel_determining_2014})
    from a trained classifier and classifying the accuracy drop.
    It takes score of how often an author is predicted after each feature-elimination step.
    The final prediction is made based on this score \citep{tyo_state_2022}.
\end{definition}


\begin{definition}
    [Hard Negative Mining]
    This method updates the model during training only with the most difficult examples in each batch.
    In the \ac{aa} context, difficult is defined as the most similar two texts from different authors, 
    which makes the decision the most difficult.
    \citet{tyo_state_2022} claim that the \ac{av} setting is strictly easier since 
    it most compare to only a single text.
    Due to the fact, that the most difficult example is model-dependent, \ac{av} problems can be made harder 
    but they can not exist of exactly the hardest negatives.
\end{definition}


\begin{definition}
    [Domain]
    The domain include topic, genre, register, idiolect, time period etc. \citep{bischoff_importance_2020}.
\end{definition}
  
\begin{definition}
    [Domain variables]
    These include topic, genre and language \citep{bischoff_importance_2020}.
\end{definition}

\begin{definition}
    [within-domain]
    Experiments with P=Q.
    Hence, it is necessary to ensure all texts are mutually from the same domain \citep{bischoff_importance_2020}.
    \begin{table}[tbp]
        \centering
        \caption{Typical scheme $S_1$ for \ac{aa} problem instances, where A, B, are authors and P, Q domains and 
        the vertical mapping denotes which author has written in which domain. 
        For training, texts from A and B take turn; for testing, previously unseen texts from A and B are used \citep{bischoff_importance_2020}.}
        \label{tab:within_domain_aa}
        \begin{tabular}{|l|ll|ll|}
        \hline
        \textbf{Scheme $S_1$} & \multicolumn{2}{l|}{\textbf{training}} & \multicolumn{2}{l|}{\textbf{testing}} \\ \hline
        \textbf{authors} & \multicolumn{1}{l|}{A} & B & \multicolumn{1}{l|}{A} & B \\ \hline
        \textbf{domains} & \multicolumn{1}{l|}{P} & Q & \multicolumn{1}{l|}{P} & Q \\ \hline
        \end{tabular}%
    \end{table}
\end{definition}


The \impAppr{} leverages random projections to lower dimensional spaces (i.e. random set of features set to zero is a projection).
\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/imposter/imposter.svg}
    \caption{\imp{}.}
    \label{fig:impostor}
\end{figure}

% AV -> open-set
\ac{av} is an open-set problem, meaning that the author of an anonymous document 
may or may be not be part of the set of candidate authors.

% AA -> closed-set
\ac{aa} is a closed-set problem, meaning that the author of an anonymous document
is part of the set of candidate authors.
For each candidate author, writing samples are available.
The task is to determine the author of the anonymous document from the set of candidate authors.

% reduction: closed-set AA -> open-set AV
\citet{koppel_determining_2014} state that all closed-set \ac{aa} problems are reducible to the \ac{av} problem.
The reverse is not true.
To reduce the \ac{aa} problem to the \ac{av} problem, we solve a \ac{av} problem, i.e. if text was written by a candidate author, 
for each of the respective candidates.
Ideally, we receive one positive answer for the correct candidate author and negative answers for all other candidates.

% complexity
\citet{koppel_determining_2014} explain that the \ac{av} problem is more complex than the \ac{aa} problem.
They claim that the ability to solve a closed-set \ac{aa} problem does not imply the ability to solve an open-set \ac{av} problem.

% open-set identification/ AA = many candidates problem
\citet{koppel_determining_2014} define the many-candidates problem, or the so-called open-set identification problem:
Given a large set of candidate authors, determine which, if any, of them wrote a given anonymous document.
According to \citet{koppel_determining_2014}, the many-candidates problem can be solved reasonably well: \autoref{lst:many_candidate_algo}.
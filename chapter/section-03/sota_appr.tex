\section{Canonical Methods}

The following sections introduce \ac{sota} approaches to \ac{av}.
We begin with compression-based methods, continue with traditional and generalized Unmasking and end with the traditional \impAppr{}.

\subsubsection{Compression-based}

% compression models, e.g. RAR or GZIP (more info Paper)
Compression models are based on the idea that the text of one author can be compressed more efficiently than the text of multiple authors.
The new text is concatenated with the author profile and then compressed.
The differences between the compressed concatenation with the unseen text and compressed author profiles without the unseen text are computed 
\citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014,neal_surveying_2018}.
The author profile with the lowest difference is selected as the predicted author \citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014}.
Tested compression algorithms include RAR, LZW, GZIP, BZIP2 and 7ZIP. 
RAR is the most accurate one \citep{elmanarelbouanani_authorship_2014}.
\citet{elmanarelbouanani_authorship_2014} include the Normalized Compressor Distance (NCD) as a distance measure for compression-based methods. % Chap. 4.2
\citet{stamatatos_survey_2009} claim that probabilistic approaches are faster in comparison to compression models.
\citet{neal_surveying_2018} state that LZ77 is a lossless data compression algorithm that is used to compress data by detecting duplicates.


Compression-based models can also be considered similarity-based measures which are slow 
since the compression algorithm is called for each training text \citep{stamatatos_survey_2009,neal_surveying_2018}.

% compression-based features
The idea of compression based approaches is to use the compression model acquired from one text to compress another text. 
If both texts are from the same author, the bit-wise size of the compressed file will be relatively low \citep{stamatatos_survey_2009,neal_surveying_2018}.
Since compression models usually describe characteristics of the texts based on the repetition of character sequences, 
they can be considered as character-based features. \todo{Is this still the case? Paper is from 2009}


\citet{tyo_state_2022} first present character embeddings.
Starting with RNN models shared across multiple authors with individual heads, 
further presenting CNNs and % CNN for AA: char better than word embeddings.
compression-based methods such as ZIP, RAR (i.e. a variant of \ac{ppm}), \ac{ppm}, 
to build representations comparable via distance metrics.
For \ac{aa}, \citet{tyo_state_2022} claim, that \ac{ppm} is a low performer and scales poorly to large datasets.
\citet{bevendorff_divergence_based_2020} quote someone else, who recommend compression-based cosine (CBC) calculated on text pairs after compression with the PPMD algorithm.
They explain that natural language allows very good for compression ratios due to its predictability (English has an entropy of at most 1.75 bits per character).
PPMD uses finite-order Markov language models for compression, which work well for predicting characters in a sentence but are sensitive to increased entropy stemming from obfuscation.

\todo{Partial Matching (PPM)= text compression; byte-pair encoding (modern \ac{aa} method)}
page 3 in \citep{tyo_state_2022}: table about \ac{aa} and \ac{av} datasets with their respective characteristics



\subsubsection{Unmasking Method}
\textcolor{red}{TODO}

% Unmasking is a meta-learning approach which is based on the idea that
% omitting discriminant features and the consequent drop in accuracy of the classifier 
% can be used for inference of the author of the unseen text.
% An unseen text is chunked, such that multiple examples either all belong to the author or to a different author, 
% are generated \citep{koppel_authorship_2004}.
% This gives rise to the idea of two examples sets which are either generated by a single generating process (author) 
% or by two different processes \citep{koppel_authorship_2004}.
% For each unseen text, a \ac{svm} is built to discriminate it, i.e. its segments, 
% from the training texts of each candidate author.
% Hence, for each candidate author, a \ac{svm} is trained.
% After a few iterations, the classifier is no longer able to discriminate between the unseen text and 
% the training texts of the true author, i.e. low accuracy \citep{stamatatos_survey_2009,koppel_authorship_2004}.


\begin{definition}
    [n-gram]
    $n$ contiguous words also known as word collocations. \todo{cite, reference?\citep{koppel_authorship_2011}?}
    n-grams are no stylometric features \citep{altakrori_topic_2021}.
    % Quelle Martin Potthast GesprÃ¤ch 19.05.2025:
    Tri-grams are commonly used in stylistic analysis, due to their ability to capture inflections, % Flexion/ Beugung in Deutsch
    morphemes, %  smallest meaningful constituents within a linguistic expression and particularly within a word
    and other syntactic structures for Germanic languages.
    Character-level $n$-gram features capture the frequency of $n$ consecutive characters in a text \citep{neal_surveying_2018}.
    The optimal $n$ is language dependent \citep{neal_surveying_2018}.
\end{definition}

\begin{definition}
    [Space free n-gram]
    Removing spaces from the $n$-gram reduces the number of $n$-grams.
    \citet{koppel_authorship_2011} use these definitions:
    \begin{enumerate}
        \item a string of $n$ characters that not include spaces
        \item a string of less than $n$ characters that is surrounded by spaces
    \end{enumerate}
\end{definition}

\begin{definition}
    [Meta learning]
    \todo{Based on learning successes and failures, the system learns to learn. }
    Approaches include:
    \begin{itemize}
        \item Unmasking: Measurement of reconstruction errors starting from a good reconstruction and iteratively impairing the reconstruction. % example on page 9 of Benno's paper
        \item Qsum heuristic: Compares the growth rates of two cumulative sums over a sequence of sentences. The sums are calculated via the deviations from the mean sentence length and the deviations of the function words.
        \item Batch means: \todo{For a series of values the variance development of the sample mean is measured while the sample size is successively increased.}
    \end{itemize}
\end{definition}

\begin{definition}
    [Unmasking]
    The idea of this meta learning approach is that with progressively omitting more and more frequent words/ 
    most discriminating features, 
    topic specific words are excluded and thus, leaving only writing style specific words \citep{stein_intrinsic_2011}.
    After several iterations, remaining features which are not powerful enough to discriminate two documents indicate that 
    these documents originate from the same author \citep{stein_intrinsic_2011,tyo_state_2022,bevendorff_divergence_based_2020,koppel_authorship_2004}.
    In other words: 
    Two texts are probably written by different authors if the differences between are robust to changes in the underlying feature set used to represent the documents.
    Differences can be measured using instance-based (meta) classification via cross-validation accuracy 
    \citep{koppel_authorship_2011,bevendorff_generalizing_2019,bevendorff_divergence_based_2020,potthast_stylometric_2018,koppel_authorship_2004}, 
    creating a performance degradation curve \citep{tyo_state_2022,koppel_authorship_2004}.
    An \ac{svm} is trained to classify the degradation curve to determine whether two text originated from the same author 
    \citep{tyo_state_2022,bevendorff_generalizing_2019,koppel_authorship_2004}.
    Cf. \citep{bevendorff_divergence_based_2020} Chapt. 2 for a detailed algorithm.
    Steep decrease in the curve indicates that the two texts are similar, and thus, 
    written by the same authors \citep{potthast_stylometric_2018,koppel_authorship_2004}.
    Provided that the unseen text is very large, this method can handle small open candidate sets \citep{koppel_authorship_2011}.
    % koppel_determining_2014, pg. 1 + bevendorff_generalizing_2019 chap. 3.1 incl. algo: based on text chunks of length >= 500 words each
    \citet{koppel_determining_2014,bevendorff_generalizing_2019} claim that effective unmasking requires input documents to be large 
    (i.e. > 10000 words~\citep{koppel_determining_2014}, book-length~\citep{bevendorff_generalizing_2019}, 
    $\geq$ 5000 words (500 words per chunk) \citep{bevendorff_divergence_based_2020}).
    Otherwise the training set becomes too sparse and no descriptive curves can be generated 
    \citep{bevendorff_generalizing_2019,bevendorff_divergence_based_2020}.
\end{definition}



\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/unmasking/unmasking.svg}
    \caption{Unmasking.}
    \label{fig:unmasking}
\end{figure}
Our method is an extension of the original \impAppr{} by \citet{koppel_determining_2014}.
By varying the seed and temperature, we can generate as many texts as we want.
  
\subsubsection{\imp{} Method}
\label{sec:impostor_method_theory}


\begin{definition}
    [\imp{} method]
    This method extends the ngram-unmasking method, i.e. iteratively omitting most influencely features (repeated feature subsampling \citep{koppel_determining_2014})
    from a trained classifier and classifying the accuracy drop.
    It takes score of how often an author is predicted after each feature-elimination step.
    The final prediction is made based on this score \citep{tyo_state_2022}.
\end{definition}


\begin{definition}
    [Hard Negative Mining]
    This method updates the model during training only with the most difficult examples in each batch.
    In the \ac{aa} context, difficult is defined as the most similar two texts from different authors, 
    which makes the decision the most difficult.
    \citet{tyo_state_2022} claim that the \ac{av} setting is strictly easier since 
    it most compare to only a single text.
    Due to the fact, that the most difficult example is model-dependent, \ac{av} problems can be made harder 
    but they can not exist of exactly the hardest negatives.
\end{definition}


\begin{definition}
    [Domain]
    The domain include topic, genre, register, idiolect, time period etc. \citep{bischoff_importance_2020}.
\end{definition}
  
\begin{definition}
    [Domain variables]
    These include topic, genre and language \citep{bischoff_importance_2020}.
\end{definition}

\begin{definition}
    [within-domain]
    Experiments with P=Q.
    Hence, it is necessary to ensure all texts are mutually from the same domain \citep{bischoff_importance_2020}.
    \begin{table}[tbp]
        \centering
        \caption{Typical scheme $S_1$ for \ac{aa} problem instances, where A, B, are authors and P, Q domains and 
        the vertical mapping denotes which author has written in which domain. 
        For training, texts from A and B take turn; for testing, previously unseen texts from A and B are used \citep{bischoff_importance_2020}.}
        \label{tab:within_domain_aa}
        \begin{tabular}{|l|ll|ll|}
        \hline
        \textbf{Scheme $S_1$} & \multicolumn{2}{l|}{\textbf{training}} & \multicolumn{2}{l|}{\textbf{testing}} \\ \hline
        \textbf{authors} & \multicolumn{1}{l|}{A} & B & \multicolumn{1}{l|}{A} & B \\ \hline
        \textbf{domains} & \multicolumn{1}{l|}{P} & Q & \multicolumn{1}{l|}{P} & Q \\ \hline
        \end{tabular}%
    \end{table}
\end{definition}


The \impAppr{} leverages random projections to lower dimensional spaces (i.e. random set of features set to zero is a projection).
\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/imposter/imposter.svg}
    \caption{\imp{}.}
    \label{fig:impostor}
\end{figure}

% AV -> open-set
\ac{av} is an open-set problem, meaning that the author of an anonymous document 
may or may be not be part of the set of candidate authors.

% AA -> closed-set
\ac{aa} is a closed-set problem, meaning that the author of an anonymous document
is part of the set of candidate authors.
For each candidate author, writing samples are available.
The task is to determine the author of the anonymous document from the set of candidate authors.

% reduction: closed-set AA -> open-set AV
\citet{koppel_determining_2014} state that all closed-set \ac{aa} problems are reducible to the \ac{av} problem.
The reverse is not true.
To reduce the \ac{aa} problem to the \ac{av} problem, we solve a \ac{av} problem, i.e. if text was written by a candidate author, 
for each of the respective candidates.
Ideally, we receive one positive answer for the correct candidate author and negative answers for all other candidates.

% complexity
\citet{koppel_determining_2014} explain that the \ac{av} problem is more complex than the \ac{aa} problem.
They claim that the ability to solve a closed-set \ac{aa} problem does not imply the ability to solve an open-set \ac{av} problem.

% open-set identification/ AA = many candidates problem
\citet{koppel_determining_2014} define the many-candidates problem, or the so-called open-set identification problem:
Given a large set of candidate authors, determine which, if any, of them wrote a given anonymous document.
According to \citet{koppel_determining_2014}, the many-candidates problem can be solved reasonably well: \autoref{lst:many_candidate_algo}.
\subsection{Compression-based Methods}
\label{subsec:compression_based}

% compression models, e.g.\ \ac{\ac{rar}} or GZIP 
This category of approaches relies on general-purpose compression models, such as \ac{ppmd}~\citep{ppm_2001} or \ac{rar}~\citep{rar_2004}. %(i.e.\ a variant of \ac{ppm}~\citep{tyo_state_2022}), LZW, GZIP, BZIP2 and 7ZIP.
Such models capture textual characteristics by exploiting repeated character sequences~\citep{stamatatos_survey_2009,neal_surveying_2018}. 
Because natural language exhibits strong predictability, compression methods that model character sequences can effectively capture author-specific patterns.
Focusing on character-level patterns, compression-based approaches are typically classified as character-based and are sensitive to text obfuscation~\citep{bevendorff_divergence_based_2020}.

Compression-based methods are further classified as profile-based methods, where an author profile is formed by concatenating and compressing all texts $k_a \in K_a$ attributed to a candidate $a \in A$. 
To infer authorship of a disputed text $d \in U$, it is concatenated with each profile and compressed. 
Next, the compression difference between the combined and original profile is computed. 
The author whose profile yields the smallest compression difference is selected as the most likely author~\citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014,neal_surveying_2018}.

The rationale behind this approach is that texts written by the same author can typically be compressed more efficiently than texts produced by different authors~\citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014}.

% \ac{rar} is the most accurate one \citep{elmanarelbouanani_authorship_2014}.
% \citet{elmanarelbouanani_authorship_2014} include the Normalized Compressor Distance (NCD) as a distance measure for compression-based methods. % Chap. 4.2
% \citet{stamatatos_survey_2009} claim that probabilistic approaches are faster in comparison to compression models.
% \citet{neal_surveying_2018} state that LZ77 is a lossless data compression algorithm that is used to compress data by detecting duplicates.

% Compression-based models can also be considered similarity-based measures which are slow 
% since the compression algorithm is called for each training text \citep{stamatatos_survey_2009,neal_surveying_2018}.
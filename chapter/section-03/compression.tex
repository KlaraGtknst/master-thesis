\subsection{Compression-based Methods}
% compression models, e.g. \ac{\ac{rar}} or GZIP 
This category of approaches relies on general-purpose compression models, such as \ac{rar} or \ac{ppmd}. %(i.e.\ a variant of \ac{ppm}~\citep{tyo_state_2022}), LZW, GZIP, BZIP2 and 7ZIP.
Such models capture textual characteristics by exploiting repeated character sequences~\citep{stamatatos_survey_2009,neal_surveying_2018}. 
Because natural language exhibits strong predictability, compression methods that model character sequences can effectively capture author-specific patterns.
This focus on character-level regularities is the reason compression-based approaches are typically classified as character-based and the reason why they are also sensitive to text obfuscation~\citep{bevendorff_divergence_based_2020}.

They are further classified as profile-based methods. In this framework, an author profile is first constructed for each candidate author by concatenating all texts attributed to them and then compressing the resulting sequence. 
The disputed text is subsequently concatenated with each author profile and compressed as well. 
The compression difference is computed between (i) the profile concatenated with the disputed text and (ii) the profile alone.  
The author whose profile yields the smallest difference is selected as the most likely author~\citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014,neal_surveying_2018}.

The rationale behind this approach is that texts written by the same author can typically be compressed more efficiently than texts produced by different authors~\citep{stamatatos_survey_2009,elmanarelbouanani_authorship_2014}.

% \ac{rar} is the most accurate one \citep{elmanarelbouanani_authorship_2014}.
% \citet{elmanarelbouanani_authorship_2014} include the Normalized Compressor Distance (NCD) as a distance measure for compression-based methods. % Chap. 4.2
% \citet{stamatatos_survey_2009} claim that probabilistic approaches are faster in comparison to compression models.
% \citet{neal_surveying_2018} state that LZ77 is a lossless data compression algorithm that is used to compress data by detecting duplicates.

% Compression-based models can also be considered similarity-based measures which are slow 
% since the compression algorithm is called for each training text \citep{stamatatos_survey_2009,neal_surveying_2018}.
\section{Technical Background}

In the following, we provide some technical background on general \ac{ml} principles.
We start with different nuances of classification, and conclude with some train-test scenarios.

% classification
In the context of \ac{ml}, we distinguish regression and classification.
For regression, the model predicts a continuous numerical value, while in a classification scenario, the model predicts a discrete class.
%% closed
Ideally, the set of all possible author classes is known a priori.
This situation is called closed-set classification where the true author is definitely one of the candidate authors~\citep{stamatatos_survey_2009,koppel_authorship_2011,barlas_cross_domain_2020,boenninghoff_o2d2_2021,neal_surveying_2018}.
% Hence, closed-set problems can use supervised or unsupervised classification techniques \citep{abbasi_writeprints_2008}.
%% open
In an open-set classification the true author is might not be included in the set of candidate authors~\citep{stamatatos_survey_2009,barlas_cross_domain_2020,neal_surveying_2018}.
It is a generalization of the closed-set classification problem.
%  allowing for an unknown author using a threshold for similarity \citep{neal_surveying_2018}.

%%% 1-class
One-class classification denote problems where the classifier is trained on samples of a single class.
If counterexamples, i.e. so-called outliers, are available, they are usually not considered to be representative of non-target class.
Hence, the classifier has to learn the concept of the target class in the absence of discriminating features~\citep{stein_intrinsic_2011,koppel_authorship_2004}.
% Examples of one-class classification are intrinsic plagiarism analysis and \ac{av}.
% Approaches to one-class classification fall into the following categories \citep{stein_intrinsic_2011}:
% \begin{itemize}
%     \item One-class density estimation, e.g., Naive Bayes
%     \item One-class boundary estimation
%     \item One-class reconstruction
% \end{itemize}

%%% multi-class


%% train-test
In \ac{ml}, we generally train models on a selection of the dataset (i.e. training set), optimize the models' hyperparameters on a second disjoint selection of the dataset (i.e. validation set), and evaluate the models on a third disjoint selection of the dataset (i.e. test set).
There should be no data leakage between the training, validation and test sets to ensure the validity of the results~\citep{bischoff_importance_2020,altakrori_topic_2021,boenninghoff_o2d2_2021}.
%% covariate shift
In the domain of stylometry, we denote covariate shift the change of distribution of neural stylometric features between training and test set due to, for instance, topic variability \citep{boenninghoff_o2d2_2021}.
%% cross-topic + cross-domain
This may include cross-topic scenarios, where new, unseen topics are used in the testing phase~\citep{altakrori_topic_2021}, or more generally, cross-domain scenarios there is a topic or genre change between training and test set~\citep{barlas_cross_domain_2020}.

%% un-/supervised
In terms of training, there are two concepts:
Supervised techniques for stylometric analysis require class labels for the training phase.
Examples include \acp{svm}, \acp{nn}, decision trees, and linear discriminant analysis.
\acp{svm} are very common in authorship analysis due to their robustness~\citep{abbasi_writeprints_2008}.
Unsupervised techniques train with no prior knowledge of classes.
Examples include \ac{pca} and cluster analysis.
\ac{pca} has been used in previous authorship studies due to its ability to capture essential variance across large number of features in a reduced dimensionality~\citep{abbasi_writeprints_2008}.

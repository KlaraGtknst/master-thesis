\subsection{Dataset}
\label{subsec:dataset}

\subsubsection{Original Data}
Due to this approach extending the original imposter approach by \citet{koppel_determining_2014}, 
we first use original data to establish the feasibility and reproducibility of the original approach. 
\citet{koppel_determining_2014} used \dataBlog{} and the \dataStudent{} dataset.

% Blog
The \dataBlog{} dataset~\citep{blog_dataset_2006} is a collection of blog posts from \textit{blogger.com} on or before August 2004.
Each blog is the work of a single user.
According to kaggle~\footnote{\href{https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus?resource=download}{Kaggle dataset \texttt{rtatman/blog-authorship-corpus}} (26.07.2025)},
the corpus contains \num{681288} posts from \num{19320} bloggers with approximately 35 posts and \num{7250} words per person.
Users are binned by age into the following categories: 13-17, 23-27, and 33-47.
Dataset features include \texttt{id}, \texttt{gender}, \texttt{age}, \texttt{topic}, 
\texttt{sign} (referring to the author's astrological/zodiac sign), \texttt{date}, and \texttt{text}.

% student essays
The  \dataStudent{} dataset is not publicly available, due to sensible information about the students.
J. W. Pennebaker was kind enough to provide us with the original data M. Koppel used in his paper.
The dataset contains \num{7052} student essays total across five assignments from a class in 2006.
The assignments include stream of consciousness, talk about your childhood, describe your personality, 
Thematic Apperception Test, and Give four examples of four different theories.
For reproducing our work, due to privacy concerns, 
we refer to J. W. Pennebaker as the data holder and authority of the \dataStudent{} dataset.
For establishing a baseline for the imposter approach, we use the \dataBlog{} and the \dataStudent{} dataset.

\subsubsection{Additional Data}
To extend our test scenario for the imposter approach, we opted to find additional datasets that 
(1) control confounders (e.g. genre, topic) and (2) provide undisputed authorship.
We found that both the \dataPan{} and \dataGutenberg{} datasets are suitable candidates.
The statistical properties of the datasets are shown in \autoref{tab:data_stats}.

% PAN20: Fanfiction
The \dataPan{} dataset~\citep{bischoff_importance_2020} is a collection of fanfiction texts from the \textit{fanfiction.net} website.
Texts belong to one fandom (i.e. thematic category), and there are not fanfiction crossovers.
According to \href{https://pan.webis.de/clef20/pan20-web/author-identification.html}{PAN's official website}, 
train and test set originate from two different fanfictions and approximate the (long-tail) distribution of the fandoms in the original dataset.
Dataset features include \texttt{id}, \texttt{fandoms}, and \texttt{pair} containing the text pairs.
They provide an additional \texttt{jsonl} file containing the ground truth for the pairs, 
i.e. \texttt{id}, \texttt{same}, and \texttt{authors} features.

% Gutenberg
The \dataGutenberg{} dataset~\footnote{\href{https://www.gutenberg.org/}{Project Gutenberg} (26.07.2025)} 
is a selection of literary works from the Project Gutenberg.
Project Gutenberg is an online library focusing on older works for which U.S. copyright has expired.
Volunteers have already digitized and proofread more than \num{75000} e-books.
In this case, we selected 19 works from 7 authors from the 16th and 19th century.
Genres include drama, fiction and poetry.
Metadata was manually extracted from the Project Gutenberg website.


\subsubsection{Dataset Preprocessing}
\label{subsubsec:dataset_preprocessing}

To further control confounders influencing authorial style, we preprocess the dataset 
(once before creating the arrow dataset file and once before using the detector).
The requirements for the preprocessing are:
\begin{itemize}
    \item The texts are stripped of all format/ layout information to obtain plain text before saving them as arrow files.
    \item The texts should be of similar length (detector crops texts to the length of the shorter text).
\end{itemize}
In order to produce a controlled testing environment for our imposter approach, 
we opted to curate small datasets rather than scaling up to larger datasets.
Removing layout information includes removing HTML artefacts, play artefacts, newlines, 
converting utf-8 to ASCII, and stripping leading and trailing whitespace.
We opted to forgo lowercasing the texts in order to preserve authorial capitalization.


\subsubsection{Selection of Text Pairs}
\label{subsubsec:dataset_text_pair_selection}

We had to select pairs of texts for the \dataBlog{}, \dataStudent{} and the \dataGutenberg{} dataset.
Eligible texts have a minimum length of \num{3000} words for all datasets but the \dataStudent{} dataset, 
where the minimum length requirement is \num{500} words.
We select a lower limit of \num{500} words for the \dataStudent{} dataset, 
since the longest essay only contains \num{1136} words.
We decided to keep the existing pairs in the \dataPan{} arrow dataset for better comparability.
All datasets consist of same- and different-author pairs. 
As mentioned before, we aimed to control confounders when selecting pairs.
Find descriptive statistics for the preprocessed datasets in Table~\ref{tab:data_stats}.

\begin{table}[h]
\centering\small
\caption{Statistics of preprocessed datasets \dataPan{}, \dataBlog{}, \dataGutenberg{}, and \dataStudent{}.}
\label{tab:data_stats}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lrrrrrrrrr@{}}   % numbers should be right aligned, text left aligned
\toprule
dataset & num\_pairs & num\_authors & num\_same\_pairs & num\_different\_pairs & avg\_text\_len & max\_text\_len\_words & std\_text\_len\_words & median\_text\_len\_words \\
\midrule
pan20           & 66906 & 52773 & 35616 & 31290 & 21418.64 (3914.74)   & 55413  & 512.28   & 3889  \\
blog            & 20547 & 9300  & 10889 & 9658  & 4603.75 (853.32)     & 70374  & 937.24   & 673   \\
gutenberg       & 12    & 7     & 6     & 6     & 437870.75 (78698.79) & 297704 & 68329.91 & 60282 \\
student\_essays & 3952  & 588   & 221   & 3731  & 3404.76 (651.63)     & 1362   & 129.22   & 620  \\
\bottomrule
\end{tabular}%
}
\end{table}
\textcolor{red}{Nicht aktuell}

For the \dataBlog{} dataset, 
two texts of a pair are selected such that they share the same topic, year, gender and age, where the last to reference the text's author.
Train (80\%) and test split (20\%) have different topics.

For the \dataStudent{} dataset,
tasks are either in the train (70\%) or test (30\%) set.
The test set is bigger, since an author typically only writes one essay per task and if only one task is selected for the test set we can not create any same author pairs.
The pairs are selected such that their authors share the same sex, ethnicity, and political orientation.
Additionally, for different author pairs, the texts are selected such that they share the same task.

For the \dataGutenberg{} dataset,
we selected pairs of texts that share the same genre and century.
Authors can either be in the train (80\%) or test (20\%) set.

Irrespective of the information used to select pairs, the final dataset contains only the columns \texttt{authors}, \texttt{pair}, and \texttt{same}.
The \texttt{pair} column contains the texts of the pair as a list of strings,
the \texttt{authors} column contains the authors of the texts as a list of strings,
and the \texttt{same} column indicates whether the texts are from the same author (\texttt{True}) or from different authors (\texttt{False}).

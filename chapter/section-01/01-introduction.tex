\chapter{Introduction}
\label{chap:introduction}



\input{chapter/section-01/problem}

\section{Research Questions}
\label{sec:research_questions}
To guide this objective, we formulate the following research questions:
\begin{questions}
    \item \textbf{How suitable are \acp{llm} for \imp{} generation?} \label{enum:rq1} \hfill \\
    Optimal \imps{} function as hard negatives in \ac{av}.
    By generating paraphrases with \acp{llm} that control genre, topic, and other contextual factors, similarity measures in the \imp{} method primarily emphasize differences in authorial style rather than content.
    However, the threshold of syntactic similarity at which paraphrases are attributed to the candidate author remains unclear, potentially reducing detection accuracy.
    We design an experiment comparing simple one-step and advanced two-step \ac{llm} paraphrasing techniques against traditional baselines using standard classification metrics.
    Additionally, we analyse how syntactic similarity between the candidate reference and \ac{llm}-generated paraphrases affects detection performance.
    % We obtain this controlled environment by utilizing \acp{llm} to paraphrase the original text.
    % There are different approaches to paraphrasing text using \acp{llm}.
    % They include (a) directly asking the \ac{llm} to paraphrase the text, 
    % (b) first extracting specific information from the original text and subsequently generating a paraphrase based on the information.
    % This thesis compares both approaches on \dataStudent{}, \dataBlog{}, \dataGutenberg{} and \dataPan{}.

    \item \textbf{Which metrics are appropriate to evaluate the quality of generated \imps{}?} \label{enum:rq2} \hfill \\
    Optimal \imps{} are generated under the same constraints as the candidate text.
    We use \ac{llm}-generated paraphrases as a heuristic of the original generation process.
    Thus, evaluating \imps{} equates to evaluating paraphrases, a task complicated by the lack of a universal definition of paraphrasing.
    Previous studies often adapt metrics from related \ac{nlp} tasks such as machine translation and summarization.
    Two primary evaluation dimensions are semantic similarity and syntactic similarity.
    Contrary to initial assumptions, high syntactic similarity is not always desirable, as it may indicate minimal modification of the original text.
    Our emphasis is on achieving high semantic similarity while preserving syntactic diversity to ensure genuine rephrasing.
    Moreover, relatively low scores can be acceptable if qualitative human assessments confirm paraphrase adequacy.
    We construct an experiment to evaluate our paraphrases in terms of \acl{sota} paraphrasing measures. 

    % \item \textbf{Which features are used for the \ac{av} problem?} \label{enum:rq3} \hfill \\
    % Traditional features include character tri-gram features, while newer research has proposed using \ac{llm} such as BERT.

    \item \textbf{How does the \ac{llm}-based \impAppr{} perform compared to \acl{sota} \ac{av} methods?} \label{enum:rq3} \hfill \\
    We use \unmasking{} and compression-based \ac{ppmd} as baselines to evaluate our \ac{llm}-based \impAppr{}.
    Performance is assessed across three scenarios: (a) human-human input pairs,
    (b) pairs containing at least one \ac{llm}, and (c) \ac{llm}-\ac{llm} pairs.
    We test \ac{av} and \ac{llm} detection scenarios.
    Comparisons are conducted on the \dataStudent{} dataset using standard classification metrics.
    
\end{questions}

% \section*{Idea}
% \label{sec:idea}

% Given a text of unknown authorship (i.e., human or \ac{llm}), 
% construct a set of \imp{} texts using state-of-the-art \acp{llm} based on the original text.
% Obtain the author by \ac{aa}/ \ac{av} methods, such as \unmasking{}, to \textit{confidently}, i.e.\ high precision, identify \ac{llm} generated texts
% (and possibly which \ac{llm}).



\section{Contributions}
\label{sec:contributions}
The main contributions of this thesis are as follows:
\begin{itemize}
    \item We reimplemented the traditional \impAppr{}~\citep{koppel_determining_2014} (cf. \autoref{sec:impostor_method_theory}).
    \item We developed an implementation of \ac{llm}-based \imp{} generation (cf. \autoref{sec:impostor_generation}). 
    \item To evaluate the use of \acp{llm} for generating hard negative \imps{}, we conducted an empirical assessment focusing on (a) the \textcolor{red}{influence of syntactic similarity between the reference candidate and the paraphrase}, and (b) performance relative to traditional \imp{} generation techniques proposed by \citet{koppel_determining_2014} (cf. \autoref{sec:res_syn_sim_impact} and \autoref{sec:results_trad_av}/ \ref{enum:rq1})
    \item We compared paraphrasing approaches quantitatively using standard summarization and translation-based techniques (cf. \autoref{sec:comp_paraphrases}/ \ref{enum:rq2})
    \item The proposed \ac{llm}-based \impAppr{} was evaluated against established \ac{av} techniques, namely \unmasking{} and \ac{ppmd} across different scenarios (cf. \autoref{sec:results_trad_av} and \autoref{sec:results_llm_av}/ \ref{enum:rq3})
    \item We analysed how different number of chunks affect paraphrasing scores (cf. \autoref{sec:results_chunks}).
    \item We collected and preprocessed the original datasets, including a deliberate candidate-pair selection procedure aligned with the methodology of the original study by \citet{koppel_determining_2014}, informed through direct consultation with the original authors (cf. \autoref{sec:dataset}).
\end{itemize}



\section{Thesis Structure}
\label{sec:thesis_structure}
The remainder of this thesis is organized as follows. 
In \autoref{chap:authorship_identification}, we provide background on \acf{av}, covering early approaches, \acl{sota} methods, stylometry, and practical applications. 
\autoref{chap:related_work} reviews related research in greater detail. 
Building on this foundation, \autoref{chap:llm_impostor_method} introduces our extension to the \impAppr{}. 
The experimental setup is described in \autoref{chap:experimental_setup}, and the results are presented in \autoref{chap:experimental_results}. 
Finally, \autoref{chap:discussion} discusses the findings, while \autoref{chap:conclusion} concludes the thesis and outlines directions for future work.

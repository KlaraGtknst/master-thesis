\chapter{Conclusion}
\label{chap:conclusion}

This thesis set out to investigate the potential of extending the traditional \impAppr{} with \ac{llm}-based \imps{} in the context of \ac{av}. 
Across three research questions, we examined whether \acp{llm} can generate more effective hard negatives, measures of paraphrase evaluation, and how the extended approach compares to traditional baselines in the traditional \ac{av} scenario. 

Our findings indicate that although \acp{llm} are capable of generating paraphrases serving as strong hard negatives, their ability of improving \ac{av} effectiveness is highly dependent on their prompt.
Unfortunately, constructing a good prompt is not trivial making this approach neither off-the-shelf nor general purpose. 
In particular, bad prompt generated supposely good paraphrases in terms of low syntactic similarity which turned out to be too short, undermining their role as hard negatives and leading to excessive \acp{fp}. 
Moreover, the approach is computationally expensive. 

We also found that aggregated syntactic and semantic similarity measures facilitate interpretable evaluation, and that low syntactic similarity is not an intuitive predictor of paraphrase quality. 
Overall, our results suggest that \ac{llm}-based \imp{} generation is not yet a practical or reliable extension for general-purpose \ac{av}, though it may hold potential for specific, carefully constructed scenarios of constrained applications.


\section{Future Work}

The limitations of this work suggest several directions for future research. 
A central priority is the improvement of paraphrase quality. 
Future efforts should focus on designing more effective and possibly generally applicable prompting strategies to ensure that generated paraphrases balance syntactic variation, controlled paraphrase length and semantic similarity. 

Another important direction concerns reproducibility and data collection. 
Our difficulties in reproducing prior results underscore the need for standardized datasets and transparent preprocessing pipelines. 

Addressing practical obstacles, such as bot-prevention mechanisms and limitations of external APIs, will be critical for enabling consistent experimentation. 

In addition, future studies should extend beyond the present focus on long texts of at least 700 words. 
Investigating shorter texts and exploring a broader range of domains, e.g.\ the \dataPan{} dataset, would provide more comparable results of the extended \impAppr{}.

Although we evaluated two-step paraphrases as an option for artificial \imp{} generation we could perform any tests assessing its \impAppr{} results due to time constraints.
Future work should include exploration in this direction.

Moreover, once the method works reasonably well with more test samples on existing experiment scenarios, one should develop finer scenarios, where we record the prediction for texts generated by one or two \acp{llm} rather than humans.
One should also evaluate the approaches capabilities for \ac{llm} detection.

A further methodological enhancement lies in the implementation of abstention mechanisms. 
Allowing the system to abstain from predictions in cases of high uncertainty could significantly improve the reliability of \ac{av}. 

By pursuing these directions, future research can address the identified weaknesses and move towards successful application of \acp{llm} in \ac{av}.

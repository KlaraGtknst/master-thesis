\chapter{Conclusion}
\label{chap:conclusion}

We explored extending the traditional \impAppr{} with \ac{llm}-generated \imps{}, addressing (1) whether \acp{llm} can produce more effective hard negatives, (2) which measures best evaluate paraphrases, and (3) how the extended approach compares to traditional baselines in the standard \ac{av} setting.

Our findings indicate that \acp{llm} are capable of generating paraphrases that can serve as strong hard negatives. 
However, the effectiveness of \ac{av} critically depends on prompt design. 
Constructing effective prompts is non-trivial, making the approach neither off-the-shelf nor general-purpose. 
Poorly designed prompts may produce paraphrases that appear favourable according to low syntactic similarity metrics but are overly short, undermining their utility as hard negatives and increasing \acp{fp}. 
In addition, the approach is computationally intensive, limiting its practical applicability.  

We further found that aggregated syntactic and semantic similarity measures facilitate interpretable evaluation, while low syntactic similarity alone is not a reliable indicator of paraphrase quality. 
Overall, our results suggest that \ac{llm}-based \imp{} generation is not yet a practical or robust extension for general-purpose \ac{av}, though it may hold promise in carefully constrained, task-specific scenarios.  



\section{Future Work}

Several directions emerge from the limitations identified in this study. 
Improving paraphrase quality remains a central priority. 
Future work should focus on designing more effective and potentially generalizable prompting strategies to ensure that paraphrases maintain semantic fidelity, controlled length, and sufficient syntactic variation.  

Reproducibility and data collection also require attention. 
Challenges in reproducing prior results highlight the need for standardized datasets and transparent preprocessing pipelines. 
Addressing practical obstacles, such as bot-prevention mechanisms and API limitations, will be critical to enable consistent experimentation.  

Expanding the scope of text types is another important direction. 
Evaluating texts shorter than 700 words and a broader range of domains, such as the \dataPan{} dataset, would provide a more comprehensive assessment of the \ac{llm}-based \impAppr{}.  

The potential of two-step paraphrase generation should be explored further. 
Although implemented in this study, time constraints prevented a full evaluation of its effect on \impAppr{} performance. 
Future research should investigate whether this strategy can improve results.  

\ac{llm} generated text pairs present additional opportunities. Finer-grained experiments could examine predictions for text pairs generated by one or more \acp{llm}, as well as the approach's potential for \ac{llm} detection.  

Finally, incorporating abstention mechanisms which allow the system to withhold predictions under high uncertainty, could improve the reliability of \ac{av} results.

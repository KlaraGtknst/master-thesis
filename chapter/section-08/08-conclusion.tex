\chapter{Conclusion}
\label{chap:conclusion}

We explored extending the traditional \impAppr{} with \ac{llm}-generated \imps{}, addressing (1) whether \acp{llm} can produce more effective hard negatives, (2) which measures best evaluate paraphrases, and (3) how the extended approach compares to traditional baselines in the standard \ac{av} setting.

Our findings indicate that \acp{llm} are capable of generating paraphrases that can serve as strong hard negatives. 
However, the effectiveness of \ac{av} critically depends on prompt design. 
Constructing effective prompts is non-trivial, making the approach neither off-the-shelf nor general-purpose. 
Poorly designed prompts may produce paraphrases that appear favourable according to low syntactic similarity metrics but are overly short, undermining their utility as hard negatives and increasing \acp{fp}. 
In addition, the approach is computationally intensive, limiting its practical applicability.  

We further found that aggregated syntactic and semantic similarity measures facilitate interpretable evaluation. Moreover, low syntactic similarity alone is not a reliable indicator of paraphrase quality. 
% Overall, our results suggest that \ac{llm}-based \imp{} generation is not yet a practical or robust extension for general-purpose \ac{av}, though it may hold promise in carefully constrained, task-specific scenarios.  



% \section{Future Work}

Several directions emerge from the limitations identified in this study. 
Improving paraphrase quality remains a central priority. 
Future work should focus on designing more effective and potentially generalisable prompting strategies to ensure that paraphrases maintain semantic fidelity, controlled length, and sufficient syntactic variation.  
%
Furthermore, the potential of two-step paraphrase generation should be explored further. 
Although implemented in this study, time constraints prevented an assessment of its impact on \impAppr{} predictions. 

Reproducibility and data collection also require attention. Standardised datasets, transparent preprocessing pipelines, and mitigation of practical obstacles such as bot-prevention mechanisms and API limitations are essential for consistent experimentation.

Expanding the range of text types to shorter texts and more diverse domains such as \dataPan{}, and \ac{llm}-generated pairs would provide a more comprehensive assessment and demonstrate potential for \ac{llm} detection.

Finally, incorporating abstention mechanisms to withhold predictions under high uncertainty could enhance the reliability of \ac{av} results.
